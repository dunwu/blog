<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/uploads/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/uploads/favicon.ico">
  <link rel="mask-icon" href="/blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blog/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous" defer></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"dunwu.github.io","root":"/blog/","images":"/blog/images","scheme":"Pisces","darkmode":true,"version":"8.25.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"atom-one-light","dark":"atom-one-dark"},"prism":{"light":"atom-one-light","dark":"atom-one-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":true,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/blog/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false,"trigger":"auto"}}</script><script src="/blog/js/config.js" defer></script>

    <meta name="description" content="钝悟的个人博客">
<meta property="og:type" content="website">
<meta property="og:title" content="Dunwu Blog">
<meta property="og:url" content="https://dunwu.github.io/blog/page/33/index.html">
<meta property="og:site_name" content="Dunwu Blog">
<meta property="og:description" content="钝悟的个人博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="钝悟 ◾ Dunwu">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://dunwu.github.io/blog/page/33/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/33/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Dunwu Blog</title>
  








  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/blog/js/utils.js" defer></script><script src="/blog/js/motion.js" defer></script><script src="/blog/js/sidebar.js" defer></script><script src="/blog/js/next-boot.js" defer></script>

  <script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.5.0/dist/search.js" integrity="sha256-xFC6PJ82SL9b3WkGjFavNiA9gm5z6UBxWPiu4CYjptg=" crossorigin="anonymous" defer></script>
<script src="/blog/js/third-party/search/local-search.js" defer></script>





  <script src="/blog/js/third-party/pace.js" defer></script>


  





  <noscript>
    <link rel="stylesheet" href="/blog/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Dunwu Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">大道至简，知易行难</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/blog/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/blog/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/blog/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">428</span></a></li><li class="menu-item menu-item-categories"><a href="/blog/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">124</span></a></li><li class="menu-item menu-item-archives"><a href="/blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">508</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="钝悟 ◾ Dunwu"
      src="/blog/uploads/avatar.gif">
  <p class="site-author-name" itemprop="name">钝悟 ◾ Dunwu</p>
  <div class="site-description" itemprop="description">钝悟的个人博客</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/blog/archives/">
          <span class="site-state-item-count">508</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/blog/categories/">
        <span class="site-state-item-count">124</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/blog/tags/">
        <span class="site-state-item-count">428</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/dunwu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;dunwu" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:forbreak@163.com" title="E-Mail → mailto:forbreak@163.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdn.jsdelivr.net/npm/@creativecommons/vocabulary@2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/626b2d15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/626b2d15/" class="post-title-link" itemprop="url">Spring AOP</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-02-26 23:47:47" itemprop="dateCreated datePublished" datetime="2020-02-26T23:47:47+08:00">2020-02-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/Java/" itemprop="url" rel="index"><span itemprop="name">Java</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/Java/%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">框架</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/Java/%E6%A1%86%E6%9E%B6/Spring/" itemprop="url" rel="index"><span itemprop="name">Spring</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/Java/%E6%A1%86%E6%9E%B6/Spring/Spring%E6%A0%B8%E5%BF%83/" itemprop="url" rel="index"><span itemprop="name">Spring核心</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Spring-AOP"><a href="#Spring-AOP" class="headerlink" title="Spring AOP"></a>Spring AOP</h1><h2 id="AOP-概念"><a href="#AOP-概念" class="headerlink" title="AOP 概念"></a>AOP 概念</h2><h3 id="什么是-AOP"><a href="#什么是-AOP" class="headerlink" title="什么是 AOP"></a>什么是 AOP</h3><p>AOP(Aspect-Oriented Programming，即 <strong>面向切面编程</strong>)与 OOP( Object-Oriented Programming，面向对象编程) 相辅相成，提供了与 OOP 不同的抽象软件结构的视角。</p>
<p>在 OOP 中，我们以类(class)作为我们的基本单元，而 AOP 中的基本单元是 <strong>Aspect(切面)</strong></p>
<h3 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h3><h4 id="Aspect-切面"><a href="#Aspect-切面" class="headerlink" title="Aspect(切面)"></a>Aspect(切面)</h4><p><code>aspect</code> 由 <code>pointcount</code> 和 <code>advice</code> 组成, 它既包含了横切逻辑的定义, 也包括了连接点的定义. Spring AOP 就是负责实施切面的框架, 它将切面所定义的横切逻辑织入到切面所指定的连接点中.<br>AOP 的工作重心在于如何将增强织入目标对象的连接点上, 这里包含两个工作:</p>
<ol>
<li>如何通过 pointcut 和 advice 定位到特定的 joinpoint 上</li>
<li>如何在 advice 中编写切面代码.</li>
</ol>
<p><strong>可以简单地认为, 使用 @Aspect 注解的类就是切面.</strong></p>
<h4 id="advice-增强"><a href="#advice-增强" class="headerlink" title="advice(增强)"></a>advice(增强)</h4><p>由 aspect 添加到特定的 join point(即满足 point cut 规则的 join point) 的一段代码.<br>许多 AOP 框架, 包括 Spring AOP, 会将 advice 模拟为一个拦截器(interceptor), 并且在 join point 上维护多个 advice, 进行层层拦截.<br>例如 HTTP 鉴权的实现, 我们可以为每个使用 RequestMapping 标注的方法织入 advice, 当 HTTP 请求到来时, 首先进入到 advice 代码中, 在这里我们可以分析这个 HTTP 请求是否有相应的权限, 如果有, 则执行 Controller, 如果没有, 则抛出异常. 这里的 advice 就扮演着鉴权拦截器的角色了.</p>
<h4 id="连接点-join-point"><a href="#连接点-join-point" class="headerlink" title="连接点(join point)"></a>连接点(join point)</h4><blockquote>
<p>a point during the execution of a program, such as the execution of a method or the handling of an exception. In Spring AOP, a join point always represents a method execution.</p>
</blockquote>
<p>程序运行中的一些时间点, 例如一个方法的执行, 或者是一个异常的处理.<br><code>在 Spring AOP 中, join point 总是方法的执行点, 即只有方法连接点.</code></p>
<h4 id="切点-point-cut"><a href="#切点-point-cut" class="headerlink" title="切点(point cut)"></a>切点(point cut)</h4><p>匹配 join point 的谓词(a predicate that matches join points).<br>Advice 是和特定的 point cut 关联的, 并且在 point cut 相匹配的 join point 中执行.<br><code>在 Spring 中, 所有的方法都可以认为是 joinpoint, 但是我们并不希望在所有的方法上都添加 Advice, 而 pointcut 的作用就是提供一组规则(使用 AspectJ pointcut expression language 来描述) 来匹配joinpoint, 给满足规则的 joinpoint 添加 Advice.</code></p>
<h4 id="关于-join-point-和-point-cut-的区别"><a href="#关于-join-point-和-point-cut-的区别" class="headerlink" title="关于 join point 和 point cut 的区别"></a>关于 join point 和 point cut 的区别</h4><p>在 Spring AOP 中, 所有的方法执行都是 join point. 而 point cut 是一个描述信息, 它修饰的是 join point, 通过 point cut, 我们就可以确定哪些 join point 可以被织入 Advice. 因此 join point 和 point cut 本质上就是两个不同纬度上的东西.<br><code>advice 是在 join point 上执行的, 而 point cut 规定了哪些 join point 可以执行哪些 advice</code></p>
<h4 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h4><p>为一个类型添加额外的方法或字段. Spring AOP 允许我们为 <code>目标对象</code> 引入新的接口(和对应的实现). 例如我们可以使用 introduction 来为一个 bean 实现 IsModified 接口, 并以此来简化 caching 的实现.</p>
<h4 id="目标对象-Target"><a href="#目标对象-Target" class="headerlink" title="目标对象(Target)"></a>目标对象(Target)</h4><p>织入 advice 的目标对象. 目标对象也被称为 <code>advised object</code>.<br><code>因为 Spring AOP 使用运行时代理的方式来实现 aspect, 因此 adviced object 总是一个代理对象(proxied object)</code><br><code>注意, adviced object 指的不是原来的类, 而是织入 advice 后所产生的代理类.</code></p>
<h4 id="AOP-proxy"><a href="#AOP-proxy" class="headerlink" title="AOP proxy"></a>AOP proxy</h4><p>一个类被 AOP 织入 advice, 就会产生一个结果类, 它是融合了原类和增强逻辑的代理类.<br>在 Spring AOP 中, 一个 AOP 代理是一个 JDK 动态代理对象或 CGLIB 代理对象.</p>
<h4 id="织入-Weaving"><a href="#织入-Weaving" class="headerlink" title="织入(Weaving)"></a>织入(Weaving)</h4><p>将 aspect 和其他对象连接起来, 并创建 adviced object 的过程.<br>根据不同的实现技术, AOP 织入有三种方式:</p>
<ul>
<li>编译器织入, 这要求有特殊的 Java 编译器.</li>
<li>类装载期织入, 这需要有特殊的类装载器.</li>
<li>动态代理织入, 在运行期为目标类添加增强(Advice)生成子类的方式.<br>Spring 采用动态代理织入, 而 AspectJ 采用编译器织入和类装载期织入.</li>
</ul>
<h3 id="advice-的类型"><a href="#advice-的类型" class="headerlink" title="advice 的类型"></a>advice 的类型</h3><ul>
<li>before advice, 在 join point 前被执行的 advice. 虽然 before advice 是在 join point 前被执行, 但是它并不能够阻止 join point 的执行, 除非发生了异常(即我们在 before advice 代码中, 不能人为地决定是否继续执行 join point 中的代码)</li>
<li>after return advice, 在一个 join point 正常返回后执行的 advice</li>
<li>after throwing advice, 当一个 join point 抛出异常后执行的 advice</li>
<li>after(final) advice, 无论一个 join point 是正常退出还是发生了异常, 都会被执行的 advice.</li>
<li>around advice, 在 join point 前和 joint point 退出后都执行的 advice. 这个是最常用的 advice.</li>
</ul>
<h3 id="关于-AOP-Proxy"><a href="#关于-AOP-Proxy" class="headerlink" title="关于 AOP Proxy"></a>关于 AOP Proxy</h3><p>Spring AOP 默认使用标准的 JDK 动态代理(dynamic proxy)技术来实现 AOP 代理, 通过它, 我们可以为任意的接口实现代理.<br><code>如果需要为一个类实现代理, 那么可以使用 CGLIB 代理.</code> 当一个业务逻辑对象没有实现接口时, 那么 Spring AOP 就默认使用 CGLIB 来作为 AOP 代理了. 即如果我们需要为一个方法织入 advice, 但是这个方法不是一个接口所提供的方法, 则此时 Spring AOP 会使用 CGLIB 来实现动态代理. 鉴于此, Spring AOP 建议基于接口编程, 对接口进行 AOP 而不是类.</p>
<h3 id="彻底理解-aspect-join-point-point-cut-advice"><a href="#彻底理解-aspect-join-point-point-cut-advice" class="headerlink" title="彻底理解 aspect, join point, point cut, advice"></a>彻底理解 aspect, join point, point cut, advice</h3><p>看完了上面的理论部分知识, 我相信还是会有不少朋友感觉到 AOP 的概念还是很模糊, 对 AOP 中的各种概念理解的还不是很透彻. 其实这很正常, 因为 AOP 中的概念是在是太多了, 我当时也是花了老大劲才梳理清楚的.<br>下面我以一个简单的例子来比喻一下 AOP 中 aspect, jointpoint, pointcut 与 advice 之间的关系.</p>
<p>让我们来假设一下, 从前有一个叫爪哇的小县城, 在一个月黑风高的晚上, 这个县城中发生了命案. 作案的凶手十分狡猾, 现场没有留下什么有价值的线索. 不过万幸的是, 刚从隔壁回来的老王恰好在这时候无意中发现了凶手行凶的过程, 但是由于天色已晚, 加上凶手蒙着面, 老王并没有看清凶手的面目, 只知道凶手是个男性, 身高约七尺五寸. 爪哇县的县令根据老王的描述, 对守门的士兵下命令说: 凡是发现有身高七尺五寸的男性, 都要抓过来审问. 士兵当然不敢违背县令的命令, 只好把进出城的所有符合条件的人都抓了起来.</p>
<p>来让我们看一下上面的一个小故事和 AOP 到底有什么对应关系.<br>首先我们知道, 在 Spring AOP 中 join point 指代的是所有方法的执行点, 而 point cut 是一个描述信息, 它修饰的是 join point, 通过 point cut, 我们就可以确定哪些 join point 可以被织入 Advice. 对应到我们在上面举的例子, 我们可以做一个简单的类比, join point 就相当于 <strong>爪哇的小县城里的百姓</strong>, point cut 就相当于 <strong>老王所做的指控, 即凶手是个男性, 身高约七尺五寸</strong>, 而 advice 则是施加在符合老王所描述的嫌疑人的动作: <strong>抓过来审问</strong>.<br>为什么可以这样类比呢?</p>
<ul>
<li>join point –&gt; 爪哇的小县城里的百姓: 因为根据定义, join point 是所有可能被织入 advice 的候选的点, 在 Spring AOP 中, 则可以认为所有方法执行点都是 join point. 而在我们上面的例子中, 命案发生在小县城中, 按理说在此县城中的所有人都有可能是嫌疑人.</li>
<li>point cut –&gt; 男性, 身高约七尺五寸: 我们知道, 所有的方法(joint point) 都可以织入 advice, 但是我们并不希望在所有方法上都织入 advice, 而 pointcut 的作用就是提供一组规则来匹配 joinpoint, 给满足规则的 joinpoint 添加 advice. 同理, 对于县令来说, 他再昏庸, 也知道不能把县城中的所有百姓都抓起来审问, 而是根据<code>凶手是个男性, 身高约七尺五寸</code>, 把符合条件的人抓起来. 在这里<code>凶手是个男性, 身高约七尺五寸</code> 就是一个修饰谓语, 它限定了凶手的范围, 满足此修饰规则的百姓都是嫌疑人, 都需要抓起来审问.</li>
<li>advice –&gt; 抓过来审问, advice 是一个动作, 即一段 Java 代码, 这段 Java 代码是作用于 point cut 所限定的那些 join point 上的. 同理, 对比到我们的例子中, <code>抓过来审问</code> 这个动作就是对作用于那些满足 <code>男性, 身高约七尺五寸</code> 的<code>爪哇的小县城里的百姓</code>.</li>
<li>aspect: aspect 是 point cut 与 advice 的组合, 因此在这里我们就可以类比: <strong>“根据老王的线索, 凡是发现有身高七尺五寸的男性, 都要抓过来审问”</strong> 这一整个动作可以被认为是一个 aspect.</li>
</ul>
<p>或则我们也可以从语法的角度来简单类比一下. 我们在学英语时, 经常会接触什么 <code>定语</code>, <code>被动句</code> 之类的概念, 那么可以做一个不严谨的类比, 即 <code>joinpoint</code> 可以认为是一个 <code>宾语</code>, 而 <code>pointcut</code> 则可以类比为修饰 <code>joinpoint</code> 的定语, 那么整个 <code>aspect</code> 就可以描述为: <code>满足 pointcut 规则的 joinpoint 会被添加相应的 advice 操作.</code></p>
<h2 id="AspectJ-支持"><a href="#AspectJ-支持" class="headerlink" title="@AspectJ 支持"></a>@AspectJ 支持</h2><p><strong><code>@AspectJ</code></strong> 是一种使用 Java 注解来实现 AOP 的编码风格。</p>
<p>@AspectJ 风格的 AOP 是 AspectJ Project 在 AspectJ 5 中引入的, 并且 Spring 也支持 @AspectJ 的 AOP 风格.</p>
<h3 id="使能-AspectJ-支持"><a href="#使能-AspectJ-支持" class="headerlink" title="使能 @AspectJ 支持"></a>使能 @AspectJ 支持</h3><p>@AspectJ 可以以 XML 的方式或以注解的方式来使能, 并且不论以哪种方式使能@ASpectJ, 我们都必须保证 aspectjweaver.jar 在 classpath 中.</p>
<h4 id="使用-Java-Configuration-方式使能-AspectJ"><a href="#使用-Java-Configuration-方式使能-AspectJ" class="headerlink" title="使用 Java Configuration 方式使能@AspectJ"></a>使用 Java Configuration 方式使能@AspectJ</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="meta">@EnableAspectJAutoProxy</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">AppConfig</span> &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="使用-XML-方式使能-AspectJ"><a href="#使用-XML-方式使能-AspectJ" class="headerlink" title="使用 XML 方式使能@AspectJ"></a>使用 XML 方式使能@AspectJ</h4><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;aop:aspectj-<span class="built_in">auto</span>proxy/&gt;</span><br></pre></td></tr></table></figure>

<h3 id="定义-aspect-切面"><a href="#定义-aspect-切面" class="headerlink" title="定义 aspect(切面)"></a>定义 aspect(切面)</h3><p>当使用注解 <strong>@Aspect</strong> 标注一个 Bean 后, 那么 Spring 框架会自动收集这些 Bean, 并添加到 Spring AOP 中, 例如:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="meta">@Aspect</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyTest</span> &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>注意, 仅仅使用@Aspect 注解, 并不能将一个 Java 对象转换为 Bean, 因此我们还需要使用类似 @Component 之类的注解.</code><br><code>注意, 如果一个 类被@Aspect 标注, 则这个类就不能是其他 aspect 的 **advised object** 了, 因为使用 @Aspect 后, 这个类就会被排除在 auto-proxying 机制之外.</code></p>
<h3 id="声明-pointcut"><a href="#声明-pointcut" class="headerlink" title="声明 pointcut"></a>声明 pointcut</h3><p>一个 pointcut 的声明由两部分组成:</p>
<ul>
<li>一个方法签名, 包括方法名和相关参数</li>
<li>一个 pointcut 表达式, 用来指定哪些方法执行是我们感兴趣的(即因此可以织入 advice).</li>
</ul>
<p>在@AspectJ 风格的 AOP 中, 我们使用一个方法来描述 pointcut, 即:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Pointcut(&quot;execution(* com.xys.service.UserService.*(..))&quot;)</span> <span class="comment">// 切点表达式</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">dataAccessOperation</span><span class="params">()</span> &#123;&#125; <span class="comment">// 切点前面</span></span><br></pre></td></tr></table></figure>

<p><code>这个方法必须无返回值.</code><br><code>这个方法本身就是 pointcut signature, pointcut 表达式使用@Pointcut 注解指定.</code><br>上面我们简单地定义了一个 pointcut, 这个 pointcut 所描述的是: 匹配所有在包 <strong>com.xys.service.UserService</strong> 下的所有方法的执行.</p>
<h4 id="切点标志符-designator"><a href="#切点标志符-designator" class="headerlink" title="切点标志符(designator)"></a>切点标志符(designator)</h4><p>AspectJ5 的切点表达式由标志符(designator)和操作参数组成. 如 “execution(* greetTo(..))” 的切点表达式, **execution** 就是 标志符, 而圆括号里的 *****greetTo(..) 就是操作参数</p>
<h5 id="execution"><a href="#execution" class="headerlink" title="execution"></a>execution</h5><p>匹配 join point 的执行, 例如 “execution(* hello(..))” 表示匹配所有目标类中的 hello() 方法. 这个是最基本的 pointcut 标志符.</p>
<h5 id="within"><a href="#within" class="headerlink" title="within"></a>within</h5><p>匹配特定包下的所有 join point, 例如 <code>within(com.xys.*)</code> 表示 com.xys 包中的所有连接点, 即包中的所有类的所有方法. 而<code>within(com.xys.service.*Service)</code> 表示在 com.xys.service 包中所有以 Service 结尾的类的所有的连接点.</p>
<h5 id="this-与-target"><a href="#this-与-target" class="headerlink" title="this 与 target"></a>this 与 target</h5><p>this 的作用是匹配一个 bean, 这个 bean(Spring AOP proxy) 是一个给定类型的实例(instance of). 而 target 匹配的是一个目标对象(target object, 即需要织入 advice 的原始的类), 此对象是一个给定类型的实例(instance of).</p>
<h5 id="bean"><a href="#bean" class="headerlink" title="bean"></a>bean</h5><p>匹配 bean 名字为指定值的 bean 下的所有方法, 例如:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">bean</span><span class="params">(*Service)</span></span> <span class="comment">// 匹配名字后缀为 Service 的 bean 下的所有方法</span></span><br><span class="line"><span class="function"><span class="title">bean</span><span class="params">(myService)</span></span> <span class="comment">// 匹配名字为 myService 的 bean 下的所有方法</span></span><br></pre></td></tr></table></figure>

<h5 id="args"><a href="#args" class="headerlink" title="args"></a>args</h5><p>匹配参数满足要求的的方法.<br>例如:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Pointcut(&quot;within(com.xys.demo2.*)&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">pointcut2</span><span class="params">()</span> &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Before(value = &quot;pointcut2()  &amp;&amp;  args(name)&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">doSomething</span><span class="params">(String name)</span> &#123;</span><br><span class="line">    logger.info(<span class="string">&quot;---page: &#123;&#125;---&quot;</span>, name);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Service</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">NormalService</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Logger</span> <span class="variable">logger</span> <span class="operator">=</span> LoggerFactory.getLogger(getClass());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">someMethod</span><span class="params">()</span> &#123;</span><br><span class="line">        logger.info(<span class="string">&quot;---NormalService: someMethod invoked---&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">test</span><span class="params">(String name)</span> &#123;</span><br><span class="line">        logger.info(<span class="string">&quot;---NormalService: test invoked---&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;服务一切正常&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>当 NormalService.test 执行时, 则 advice <code>doSomething</code> 就会执行, test 方法的参数 name 就会传递到 <code>doSomething</code> 中.</p>
<p>常用例子:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 匹配只有一个参数 name 的方法</span></span><br><span class="line"><span class="meta">@Before(value = &quot;aspectMethod()  &amp;&amp;  args(name)&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">doSomething</span><span class="params">(String name)</span> &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 匹配第一个参数为 name 的方法</span></span><br><span class="line"><span class="meta">@Before(value = &quot;aspectMethod()  &amp;&amp;  args(name, ..)&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">doSomething</span><span class="params">(String name)</span> &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 匹配第二个参数为 name 的方法</span></span><br><span class="line">Before(value = <span class="string">&quot;aspectMethod()  &amp;&amp;  args(*, name, ..)&quot;</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">doSomething</span><span class="params">(String name)</span> &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="annotation"><a href="#annotation" class="headerlink" title="@annotation"></a>@annotation</h5><p>匹配由指定注解所标注的方法, 例如:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Pointcut(&quot;@annotation(com.xys.demo1.AuthChecker)&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">pointcut</span><span class="params">()</span> &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>则匹配由注解 <code>AuthChecker</code> 所标注的方法.</p>
<h4 id="常见的切点表达式"><a href="#常见的切点表达式" class="headerlink" title="常见的切点表达式"></a>常见的切点表达式</h4><h5 id="匹配方法签名"><a href="#匹配方法签名" class="headerlink" title="匹配方法签名"></a>匹配方法签名</h5><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 匹配指定包中的所有的方法</span></span><br><span class="line"><span class="function"><span class="title">execution</span><span class="params">(* com.xys.service.*(..)</span></span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 匹配当前包中的指定类的所有方法</span></span><br><span class="line"><span class="function"><span class="title">execution</span><span class="params">(* UserService.*(..)</span></span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 匹配指定包中的所有 public 方法</span></span><br><span class="line"><span class="function"><span class="title">execution</span><span class="params">(public * com.xys.service.*(..)</span></span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 匹配指定包中的所有 public 方法, 并且返回值是 int 类型的方法</span></span><br><span class="line"><span class="function"><span class="title">execution</span><span class="params">(public int com.xys.service.*(..)</span></span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 匹配指定包中的所有 public 方法, 并且第一个参数是 String, 返回值是 int 类型的方法</span></span><br><span class="line"><span class="function"><span class="title">execution</span><span class="params">(public int com.xys.service.*(String name, ..)</span></span>)</span><br></pre></td></tr></table></figure>

<h5 id="匹配类型签名"><a href="#匹配类型签名" class="headerlink" title="匹配类型签名"></a>匹配类型签名</h5><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 匹配指定包中的所有的方法, 但不包括子包</span></span><br><span class="line"><span class="function"><span class="title">within</span><span class="params">(com.xys.service.*)</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 匹配指定包中的所有的方法, 包括子包</span></span><br><span class="line"><span class="function"><span class="title">within</span><span class="params">(com.xys.service..*)</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 匹配当前包中的指定类中的方法</span></span><br><span class="line"><span class="function"><span class="title">within</span><span class="params">(UserService)</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 匹配一个接口的所有实现类中的实现的方法</span></span><br><span class="line"><span class="function"><span class="title">within</span><span class="params">(UserDao+)</span></span></span><br></pre></td></tr></table></figure>

<h5 id="匹配-Bean-名字"><a href="#匹配-Bean-名字" class="headerlink" title="匹配 Bean 名字"></a>匹配 Bean 名字</h5><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 匹配以指定名字结尾的 Bean 中的所有方法</span></span><br><span class="line"><span class="function"><span class="title">bean</span><span class="params">(*Service)</span></span></span><br></pre></td></tr></table></figure>

<h5 id="切点表达式组合"><a href="#切点表达式组合" class="headerlink" title="切点表达式组合"></a>切点表达式组合</h5><figure class="highlight delphi"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 匹配以 Service 或 ServiceImpl 结尾的 bean</span></span><br><span class="line">bean<span class="comment">(*Service || *ServiceImpl)</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">// 匹配名字以 Service 结尾, 并且在包 com.xys.service 中的 bean</span></span><br><span class="line"><span class="comment">bean(*Service) &amp;&amp; within(com.xys.service.*)</span></span><br></pre></td></tr></table></figure>

<h3 id="声明-advice"><a href="#声明-advice" class="headerlink" title="声明 advice"></a>声明 advice</h3><p>advice 是和一个 pointcut 表达式关联在一起的, 并且会在匹配的 join point 的方法执行的前&#x2F;后&#x2F;周围 运行. <code>pointcut 表达式可以是简单的一个 pointcut 名字的引用, 或者是完整的 pointcut 表达式</code>.<br>下面我们以几个简单的 advice 为例子, 来看一下一个 advice 是如何声明的.</p>
<h4 id="Before-advice"><a href="#Before-advice" class="headerlink" title="Before advice"></a>Before advice</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> xiongyongshun</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@version</span> 1.0</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@created</span> 16/9/9 13:13</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="meta">@Aspect</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">BeforeAspectTest</span> &#123;</span><br><span class="line">    <span class="comment">// 定义一个 Pointcut, 使用 切点表达式函数 来描述对哪些 Join point 使用 advise.</span></span><br><span class="line">    <span class="meta">@Pointcut(&quot;execution(* com.xys.service.UserService.*(..))&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">dataAccessOperation</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="meta">@Aspect</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">AdviseDefine</span> &#123;</span><br><span class="line">    <span class="comment">// 定义 advise</span></span><br><span class="line">    <span class="meta">@Before(&quot;com.xys.aspect.PointcutDefine.dataAccessOperation()&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">doBeforeAccessCheck</span><span class="params">(JoinPoint joinPoint)</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;*****Before advise, method: &quot;</span> + joinPoint.getSignature().toShortString() + <span class="string">&quot; *****&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里, <strong>@Before</strong> 引用了一个 pointcut, 即 “com.xys.aspect.PointcutDefine.dataAccessOperation()” 是一个 pointcut 的名字.<br>如果我们在 advice 在内置 pointcut, 则可以:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="meta">@Aspect</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">AdviseDefine</span> &#123;</span><br><span class="line">    <span class="comment">// 将 pointcut 和 advice 同时定义</span></span><br><span class="line">    <span class="meta">@Before(&quot;within(com.xys.service..*)&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">doAccessCheck</span><span class="params">(JoinPoint joinPoint)</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;*****doAccessCheck, Before advise, method: &quot;</span> + joinPoint.getSignature().toShortString() + <span class="string">&quot; *****&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="around-advice"><a href="#around-advice" class="headerlink" title="around advice"></a>around advice</h4><p>around advice 比较特别, 它可以在一个方法的之前之前和之后添加不同的操作, 并且甚至可以决定何时, 如何, 是否调用匹配到的方法.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="meta">@Aspect</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">AdviseDefine</span> &#123;</span><br><span class="line">    <span class="comment">// 定义 advise</span></span><br><span class="line">    <span class="meta">@Around(&quot;com.xys.aspect.PointcutDefine.dataAccessOperation()&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> Object <span class="title function_">doAroundAccessCheck</span><span class="params">(ProceedingJoinPoint pjp)</span> <span class="keyword">throws</span> Throwable &#123;</span><br><span class="line">        <span class="type">StopWatch</span> <span class="variable">stopWatch</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StopWatch</span>();</span><br><span class="line">        stopWatch.start();</span><br><span class="line">        <span class="comment">// 开始</span></span><br><span class="line">        <span class="type">Object</span> <span class="variable">retVal</span> <span class="operator">=</span> pjp.proceed();</span><br><span class="line">        stopWatch.stop();</span><br><span class="line">        <span class="comment">// 结束</span></span><br><span class="line">        System.out.println(<span class="string">&quot;invoke method: &quot;</span> + pjp.getSignature().getName() + <span class="string">&quot;, elapsed time: &quot;</span> + stopWatch.getTotalTimeMillis());</span><br><span class="line">        <span class="keyword">return</span> retVal;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>around advice 和前面的 before advice 差不多, 只是我们把注解 <strong>@Before</strong> 改为了 <strong>@Around</strong> 了.</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://item.jd.com/11899370.html">《 Spring 实战（第 4 版）》</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/493f6a7c/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/493f6a7c/" class="post-title-link" itemprop="url">HDFS 应用</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-02-24 21:14:47" itemprop="dateCreated datePublished" datetime="2020-02-24T21:14:47+08:00">2020-02-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/" itemprop="url" rel="index"><span itemprop="name">hadoop</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="HDFS-应用"><a href="#HDFS-应用" class="headerlink" title="HDFS 应用"></a>HDFS 应用</h1><p><strong>HDFS</strong> 是 <strong>Hadoop Distributed File System</strong> 的缩写，即 Hadoop 的分布式文件系统。</p>
<p>HDFS 是一种用于<strong>存储具有流数据访问模式的超大文件的文件系统</strong>，它<strong>运行在廉价的机器集群</strong>上。</p>
<p>HDFS 的设计目标是管理数以千计的服务器、数以万计的磁盘，将这么大规模的服务器计算资源当作一个单一的存储系统进行管理，对应用程序提供 <strong>PB 级</strong>的存储容量，让应用程序像使用普通文件系统一样存储大规模的文件数据。</p>
<p>HDFS 是在一个大规模分布式服务器集群上，对数据分片后进行并行读写及冗余存储。因为 HDFS 可以部署在一个比较大的服务器集群上，集群中所有服务器的磁盘都可供 HDFS 使用，所以整个 HDFS 的存储空间可以达到 PB 级容量。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202502192251433.png"></p>
<h2 id="HDFS-命令"><a href="#HDFS-命令" class="headerlink" title="HDFS 命令"></a>HDFS 命令</h2><h3 id="显示当前目录结构"><a href="#显示当前目录结构" class="headerlink" title="显示当前目录结构"></a>显示当前目录结构</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">显示当前目录结构</span></span><br><span class="line">hdfs dfs -ls &lt;path&gt;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">递归显示当前目录结构</span></span><br><span class="line">hdfs dfs -ls -R &lt;path&gt;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">显示根目录下内容</span></span><br><span class="line">hdfs dfs -ls /</span><br></pre></td></tr></table></figure>

<h3 id="创建目录"><a href="#创建目录" class="headerlink" title="创建目录"></a>创建目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建目录</span></span><br><span class="line">hdfs dfs -mkdir &lt;path&gt;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">递归创建目录</span></span><br><span class="line">hdfs dfs -mkdir -p &lt;path&gt;</span><br></pre></td></tr></table></figure>

<h3 id="删除操作"><a href="#删除操作" class="headerlink" title="删除操作"></a>删除操作</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">删除文件</span></span><br><span class="line">hdfs dfs -rm &lt;path&gt;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">递归删除目录和文件</span></span><br><span class="line">hdfs dfs -rm -R &lt;path&gt;</span><br></pre></td></tr></table></figure>

<h3 id="导入文件到-HDFS"><a href="#导入文件到-HDFS" class="headerlink" title="导入文件到 HDFS"></a>导入文件到 HDFS</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">二选一执行即可</span></span><br><span class="line">hdfs dfs -put [localsrc] [dst]</span><br><span class="line">hdfs dfs -copyFromLocal [localsrc] [dst]</span><br></pre></td></tr></table></figure>

<h3 id="从-HDFS-导出文件"><a href="#从-HDFS-导出文件" class="headerlink" title="从 HDFS 导出文件"></a>从 HDFS 导出文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">二选一执行即可</span></span><br><span class="line">hdfs dfs -get [dst] [localsrc]</span><br><span class="line">hdfs dfs -copyToLocal [dst] [localsrc]</span><br></pre></td></tr></table></figure>

<h3 id="查看文件内容"><a href="#查看文件内容" class="headerlink" title="查看文件内容"></a>查看文件内容</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">二选一执行即可</span></span><br><span class="line">hdfs dfs -text &lt;path&gt;</span><br><span class="line">hdfs dfs -cat &lt;path&gt;</span><br></pre></td></tr></table></figure>

<h3 id="显示文件的最后一千字节"><a href="#显示文件的最后一千字节" class="headerlink" title="显示文件的最后一千字节"></a>显示文件的最后一千字节</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -tail &lt;path&gt;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">和 Linux 下一样，会持续监听文件内容变化 并显示文件的最后一千字节</span></span><br><span class="line">hdfs dfs -tail -f &lt;path&gt;</span><br></pre></td></tr></table></figure>

<h3 id="拷贝文件"><a href="#拷贝文件" class="headerlink" title="拷贝文件"></a>拷贝文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -cp [src] [dst]</span><br></pre></td></tr></table></figure>

<h3 id="移动文件"><a href="#移动文件" class="headerlink" title="移动文件"></a>移动文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mv [src] [dst]</span><br></pre></td></tr></table></figure>

<h3 id="统计当前目录下各文件大小"><a href="#统计当前目录下各文件大小" class="headerlink" title="统计当前目录下各文件大小"></a>统计当前目录下各文件大小</h3><ul>
<li>默认单位字节</li>
<li>-s : 显示所有文件大小总和，</li>
<li>-h : 将以更友好的方式显示文件大小（例如 64.0m 而不是 67108864）</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -du &lt;path&gt;</span><br></pre></td></tr></table></figure>

<h3 id="合并下载多个文件"><a href="#合并下载多个文件" class="headerlink" title="合并下载多个文件"></a>合并下载多个文件</h3><ul>
<li>-nl 在每个文件的末尾添加换行符（LF）</li>
<li>-skip-empty-file 跳过空文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -getmerge</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例 将 HDFS 上的 hbase-policy.xml 和 hbase-site.xml 文件合并后下载到本地的/usr/test.xml</span></span><br><span class="line">hdfs dfs -getmerge -nl  /test/hbase-policy.xml /test/hbase-site.xml /usr/test.xml</span><br></pre></td></tr></table></figure>

<h3 id="统计文件系统的可用空间信息"><a href="#统计文件系统的可用空间信息" class="headerlink" title="统计文件系统的可用空间信息"></a>统计文件系统的可用空间信息</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -df -h /</span><br></pre></td></tr></table></figure>

<h3 id="更改文件复制因子"><a href="#更改文件复制因子" class="headerlink" title="更改文件复制因子"></a>更改文件复制因子</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -setrep [-R] [-w] &lt;numReplicas&gt; &lt;path&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>更改文件的复制因子。如果 path 是目录，则更改其下所有文件的复制因子</li>
<li>-w : 请求命令是否等待复制完成</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例</span></span><br><span class="line">hdfs dfs -setrep -w 3 /user/hadoop/dir1</span><br></pre></td></tr></table></figure>

<h3 id="权限控制"><a href="#权限控制" class="headerlink" title="权限控制"></a>权限控制</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">权限控制和 Linux 上使用方式一致</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">变更文件或目录的所属群组。 用户必须是文件的所有者或超级用户。</span></span><br><span class="line">hdfs dfs -chgrp [-R] GROUP URI [URI ...]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改文件或目录的访问权限  用户必须是文件的所有者或超级用户。</span></span><br><span class="line">hdfs dfs -chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; URI [URI ...]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改文件的拥有者  用户必须是超级用户。</span></span><br><span class="line">hdfs dfs -chown [-R] [OWNER][:[GROUP]] URI [URI ]</span><br></pre></td></tr></table></figure>

<h3 id="文件检测"><a href="#文件检测" class="headerlink" title="文件检测"></a>文件检测</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -test - [defsz]  URI</span><br></pre></td></tr></table></figure>

<p>可选选项：</p>
<ul>
<li>-d：如果路径是目录，返回 0。</li>
<li>-e：如果路径存在，则返回 0。</li>
<li>-f：如果路径是文件，则返回 0。</li>
<li>-s：如果路径不为空，则返回 0。</li>
<li>-r：如果路径存在且授予读权限，则返回 0。</li>
<li>-w：如果路径存在且授予写入权限，则返回 0。</li>
<li>-z：如果文件长度为零，则返回 0。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例</span></span><br><span class="line">hdfs dfs -<span class="built_in">test</span> -e filename</span><br></pre></td></tr></table></figure>

<h2 id="HDFS-API"><a href="#HDFS-API" class="headerlink" title="HDFS API"></a>HDFS API</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>想要使用 HDFS API，需要导入依赖 <code>hadoop-client</code>。如果是 CDH 版本的 Hadoop，还需要额外指明其仓库地址：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0</span></span></span><br><span class="line"><span class="string"><span class="tag">         http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.heibaiying<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hdfs-java-api<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>2.6.0-cdh5.15.2<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!---配置 CDH 仓库地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--Hadoop-client--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="FileSystem"><a href="#FileSystem" class="headerlink" title="FileSystem"></a>FileSystem</h3><p><code>FileSystem</code> 是所有 HDFS 操作的主入口。由于之后的每个单元测试都需要用到它，这里使用 <code>@Before</code> 注解进行标注。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">HDFS_PATH</span> <span class="operator">=</span> <span class="string">&quot;hdfs://192.168.0.106:8020&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">HDFS_USER</span> <span class="operator">=</span> <span class="string">&quot;root&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> FileSystem fileSystem;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">prepare</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="comment">// 这里我启动的是单节点的 Hadoop, 所以副本系数设置为 1, 默认值为 3</span></span><br><span class="line">        configuration.set(<span class="string">&quot;dfs.replication&quot;</span>, <span class="string">&quot;1&quot;</span>);</span><br><span class="line">        fileSystem = FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(HDFS_PATH), configuration, HDFS_USER);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (URISyntaxException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@After</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">destroy</span><span class="params">()</span> &#123;</span><br><span class="line">    fileSystem = <span class="literal">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p><a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html">FileSystem 官方 Java API 文档</a></p>
</blockquote>
<h3 id="创建目录-1"><a href="#创建目录-1" class="headerlink" title="创建目录"></a>创建目录</h3><p>支持递归创建目录：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">mkDir</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    fileSystem.mkdirs(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test0/&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="创建指定权限的目录"><a href="#创建指定权限的目录" class="headerlink" title="创建指定权限的目录"></a>创建指定权限的目录</h3><p><code>FsPermission(FsAction u, FsAction g, FsAction o)</code> 的三个参数分别对应：创建者权限，同组其他用户权限，其他用户权限，权限值定义在 <code>FsAction</code> 枚举类中。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">mkDirWithPermission</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    fileSystem.mkdirs(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test1/&quot;</span>),</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">FsPermission</span>(FsAction.READ_WRITE, FsAction.READ, FsAction.READ));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="创建文件，并写入内容"><a href="#创建文件，并写入内容" class="headerlink" title="创建文件，并写入内容"></a>创建文件，并写入内容</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">create</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="comment">// 如果文件存在，默认会覆盖，可以通过第二个参数进行控制。第三个参数可以控制使用缓冲区的大小</span></span><br><span class="line">    <span class="type">FSDataOutputStream</span> <span class="variable">out</span> <span class="operator">=</span> fileSystem.create(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/a.txt&quot;</span>),</span><br><span class="line">                                               <span class="literal">true</span>, <span class="number">4096</span>);</span><br><span class="line">    out.write(<span class="string">&quot;hello hadoop!&quot;</span>.getBytes());</span><br><span class="line">    out.write(<span class="string">&quot;hello spark!&quot;</span>.getBytes());</span><br><span class="line">    out.write(<span class="string">&quot;hello flink!&quot;</span>.getBytes());</span><br><span class="line">    <span class="comment">// 强制将缓冲区中内容刷出</span></span><br><span class="line">    out.flush();</span><br><span class="line">    out.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="判断文件是否存在"><a href="#判断文件是否存在" class="headerlink" title="判断文件是否存在"></a>判断文件是否存在</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">exist</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">boolean</span> <span class="variable">exists</span> <span class="operator">=</span> fileSystem.exists(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/a.txt&quot;</span>));</span><br><span class="line">    System.out.println(exists);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="查看文件内容-1"><a href="#查看文件内容-1" class="headerlink" title="查看文件内容"></a>查看文件内容</h3><p>查看小文本文件的内容，直接转换成字符串后输出：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readToString</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">FSDataInputStream</span> <span class="variable">inputStream</span> <span class="operator">=</span> fileSystem.open(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/a.txt&quot;</span>));</span><br><span class="line">    <span class="type">String</span> <span class="variable">context</span> <span class="operator">=</span> inputStreamToString(inputStream, <span class="string">&quot;utf-8&quot;</span>);</span><br><span class="line">    System.out.println(context);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>inputStreamToString</code> 是一个自定义方法，代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 把输入流转换为指定编码的字符</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> inputStream 输入流</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> encode      指定编码类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String <span class="title function_">inputStreamToString</span><span class="params">(InputStream inputStream, String encode)</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (encode == <span class="literal">null</span> || (<span class="string">&quot;&quot;</span>.equals(encode))) &#123;</span><br><span class="line">            encode = <span class="string">&quot;utf-8&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">BufferedReader</span> <span class="variable">reader</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedReader</span>(<span class="keyword">new</span> <span class="title class_">InputStreamReader</span>(inputStream, encode));</span><br><span class="line">        <span class="type">StringBuilder</span> <span class="variable">builder</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuilder</span>();</span><br><span class="line">        <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="keyword">while</span> ((str = reader.readLine()) != <span class="literal">null</span>) &#123;</span><br><span class="line">            builder.append(str).append(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> builder.toString();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="文件重命名"><a href="#文件重命名" class="headerlink" title="文件重命名"></a>文件重命名</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">rename</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">Path</span> <span class="variable">oldPath</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/a.txt&quot;</span>);</span><br><span class="line">    <span class="type">Path</span> <span class="variable">newPath</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/b.txt&quot;</span>);</span><br><span class="line">    <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> fileSystem.rename(oldPath, newPath);</span><br><span class="line">    System.out.println(result);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="删除目录或文件"><a href="#删除目录或文件" class="headerlink" title="删除目录或文件"></a>删除目录或文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">delete</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     *  第二个参数代表是否递归删除</span></span><br><span class="line"><span class="comment">     *    +  如果 path 是一个目录且递归删除为 true, 则删除该目录及其中所有文件；</span></span><br><span class="line"><span class="comment">     *    +  如果 path 是一个目录但递归删除为 false, 则会则抛出异常。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> fileSystem.delete(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/b.txt&quot;</span>), <span class="literal">true</span>);</span><br><span class="line">    System.out.println(result);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="上传文件到-HDFS"><a href="#上传文件到-HDFS" class="headerlink" title="上传文件到 HDFS"></a>上传文件到 HDFS</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">copyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="comment">// 如果指定的是目录，则会把目录及其中的文件都复制到指定目录下</span></span><br><span class="line">    <span class="type">Path</span> <span class="variable">src</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\BigData-Notes\\notes\\installation&quot;</span>);</span><br><span class="line">    <span class="type">Path</span> <span class="variable">dst</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/&quot;</span>);</span><br><span class="line">    fileSystem.copyFromLocalFile(src, dst);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="上传大文件并显示上传进度"><a href="#上传大文件并显示上传进度" class="headerlink" title="上传大文件并显示上传进度"></a>上传大文件并显示上传进度</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">copyFromLocalBigFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">File</span> <span class="variable">file</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">File</span>(<span class="string">&quot;D:\\kafka.tgz&quot;</span>);</span><br><span class="line">        <span class="keyword">final</span> <span class="type">float</span> <span class="variable">fileSize</span> <span class="operator">=</span> file.length();</span><br><span class="line">        <span class="type">InputStream</span> <span class="variable">in</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedInputStream</span>(<span class="keyword">new</span> <span class="title class_">FileInputStream</span>(file));</span><br><span class="line"></span><br><span class="line">        <span class="type">FSDataOutputStream</span> <span class="variable">out</span> <span class="operator">=</span> fileSystem.create(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/kafka5.tgz&quot;</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Progressable</span>() &#123;</span><br><span class="line">                  <span class="type">long</span> <span class="variable">fileCount</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">                  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">progress</span><span class="params">()</span> &#123;</span><br><span class="line">                     fileCount++;</span><br><span class="line">                     <span class="comment">// progress 方法每上传大约 64KB 的数据后就会被调用一次</span></span><br><span class="line">                     System.out.println(<span class="string">&quot;上传进度：&quot;</span> + (fileCount * <span class="number">64</span> * <span class="number">1024</span> / fileSize) * <span class="number">100</span> + <span class="string">&quot; %&quot;</span>);</span><br><span class="line">                   &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        IOUtils.copyBytes(in, out, <span class="number">4096</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h3 id="从-HDFS-上下载文件"><a href="#从-HDFS-上下载文件" class="headerlink" title="从 HDFS 上下载文件"></a>从 HDFS 上下载文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">copyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">Path</span> <span class="variable">src</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/kafka.tgz&quot;</span>);</span><br><span class="line">    <span class="type">Path</span> <span class="variable">dst</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\app\\&quot;</span>);</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 第一个参数控制下载完成后是否删除源文件，默认是 true, 即删除；</span></span><br><span class="line"><span class="comment">     * 最后一个参数表示是否将 RawLocalFileSystem 用作本地文件系统；</span></span><br><span class="line"><span class="comment">     * RawLocalFileSystem 默认为 false, 通常情况下可以不设置，</span></span><br><span class="line"><span class="comment">     * 但如果你在执行时候抛出 NullPointerException 异常，则代表你的文件系统与程序可能存在不兼容的情况 (window 下常见）,</span></span><br><span class="line"><span class="comment">     * 此时可以将 RawLocalFileSystem 设置为 true</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    fileSystem.copyToLocalFile(<span class="literal">false</span>, src, dst, <span class="literal">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="查看指定目录下所有文件的信息"><a href="#查看指定目录下所有文件的信息" class="headerlink" title="查看指定目录下所有文件的信息"></a>查看指定目录下所有文件的信息</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">listFiles</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    FileStatus[] statuses = fileSystem.listStatus(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api&quot;</span>));</span><br><span class="line">    <span class="keyword">for</span> (FileStatus fileStatus : statuses) &#123;</span><br><span class="line">        <span class="comment">//fileStatus 的 toString 方法被重写过，直接打印可以看到所有信息</span></span><br><span class="line">        System.out.println(fileStatus.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>FileStatus</code> 中包含了文件的基本信息，比如文件路径，是否是文件夹，修改时间，访问时间，所有者，所属组，文件权限，是否是符号链接等，输出内容示例如下：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">FileStatus&#123;</span></span><br><span class="line"><span class="attr">path</span>=<span class="string">hdfs://192.168.0.106:8020/hdfs-api/test;</span></span><br><span class="line"><span class="attr">isDirectory</span>=<span class="string">true;</span></span><br><span class="line"><span class="attr">modification_time</span>=<span class="string">1556680796191;</span></span><br><span class="line"><span class="attr">access_time</span>=<span class="string">0;</span></span><br><span class="line"><span class="attr">owner</span>=<span class="string">root;</span></span><br><span class="line"><span class="attr">group</span>=<span class="string">supergroup;</span></span><br><span class="line"><span class="attr">permission</span>=<span class="string">rwxr-xr-x;</span></span><br><span class="line"><span class="attr">isSymlink</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="递归查看指定目录下所有文件的信息"><a href="#递归查看指定目录下所有文件的信息" class="headerlink" title="递归查看指定目录下所有文件的信息"></a>递归查看指定目录下所有文件的信息</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">listFilesRecursive</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; files = fileSystem.listFiles(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hbase&quot;</span>), <span class="literal">true</span>);</span><br><span class="line">    <span class="keyword">while</span> (files.hasNext()) &#123;</span><br><span class="line">        System.out.println(files.next());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>和上面输出类似，只是多了文本大小，副本系数，块大小信息。</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">LocatedFileStatus&#123;</span></span><br><span class="line"><span class="attr">path</span>=<span class="string">hdfs://192.168.0.106:8020/hbase/hbase.version;</span></span><br><span class="line"><span class="attr">isDirectory</span>=<span class="string">false;</span></span><br><span class="line"><span class="attr">length</span>=<span class="string">7;</span></span><br><span class="line"><span class="attr">replication</span>=<span class="string">1;</span></span><br><span class="line"><span class="attr">blocksize</span>=<span class="string">134217728;</span></span><br><span class="line"><span class="attr">modification_time</span>=<span class="string">1554129052916;</span></span><br><span class="line"><span class="attr">access_time</span>=<span class="string">1554902661455;</span></span><br><span class="line"><span class="attr">owner</span>=<span class="string">root; group=supergroup;</span></span><br><span class="line"><span class="attr">permission</span>=<span class="string">rw-r--r--;</span></span><br><span class="line"><span class="attr">isSymlink</span>=<span class="string">false&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="查看文件的块信息"><a href="#查看文件的块信息" class="headerlink" title="查看文件的块信息"></a>查看文件的块信息</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">getFileBlockLocations</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">    <span class="type">FileStatus</span> <span class="variable">fileStatus</span> <span class="operator">=</span> fileSystem.getFileStatus(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/kafka.tgz&quot;</span>));</span><br><span class="line">    BlockLocation[] blocks = fileSystem.getFileBlockLocations(fileStatus, <span class="number">0</span>, fileStatus.getLen());</span><br><span class="line">    <span class="keyword">for</span> (BlockLocation block : blocks) &#123;</span><br><span class="line">        System.out.println(block);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>块输出信息有三个值，分别是文件的起始偏移量 (offset)，文件大小 (length)，块所在的主机名 (hosts)。</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">0</span>,<span class="number">57028557</span>,hadoop001</span><br></pre></td></tr></table></figure>

<p>这里我上传的文件只有 57M（小于 128M)，且程序中设置了副本系数为 1，所有只有一个块信息。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/HDFS%E5%B8%B8%E7%94%A8Shell%E5%91%BD%E4%BB%A4.md">https://github.com/heibaiying/BigData-Notes/blob/master/notes/HDFS%E5%B8%B8%E7%94%A8Shell%E5%91%BD%E4%BB%A4.md</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/HDFS-Java-API.md">https://github.com/heibaiying/BigData-Notes/blob/master/notes/HDFS-Java-API.md</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/9da66d60/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/9da66d60/" class="post-title-link" itemprop="url">Hadoop 面试</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-02-24 21:14:47" itemprop="dateCreated datePublished" datetime="2020-02-24T21:14:47+08:00">2020-02-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/" itemprop="url" rel="index"><span itemprop="name">hadoop</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>20k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>18 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Hadoop-面试"><a href="#Hadoop-面试" class="headerlink" title="Hadoop 面试"></a>Hadoop 面试</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="【初级】简介一下大数据技术生态？"><a href="#【初级】简介一下大数据技术生态？" class="headerlink" title="【初级】简介一下大数据技术生态？"></a>【初级】简介一下大数据技术生态？</h3><p>:::details 要点</p>
<ul>
<li><strong>数据采集</strong>：Flume、Sqoop、Logstash、Filebeat</li>
<li><strong>分布式文件存储</strong>：Hadoop HDFS</li>
<li><strong>NoSql</strong><ul>
<li><strong>文档数据库</strong>：Mongodb</li>
<li><strong>列式数据库</strong>：HBase</li>
<li><strong>搜索引擎</strong>：Solr、Elasticsearch</li>
</ul>
</li>
<li><strong>分布式计算</strong><ul>
<li><strong>批处理</strong>：Hadoop MapReduce</li>
<li><strong>流处理</strong>：Storm、Kafka</li>
<li><strong>混合处理</strong>：Spark、Flink</li>
</ul>
</li>
<li><strong>查询分析</strong>：Hive、Spark SQL、Flink SQL、Pig、Phoenix</li>
<li><strong>集群资源管理</strong>：Hadoop YARN</li>
<li><strong>分布式协调</strong>：Zookeeper</li>
<li><strong>任务调度</strong>：Azkaban、Oozie</li>
<li><strong>集群部署和监控</strong>：Ambari、Cloudera Manager</li>
</ul>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202502192251433.png"></p>
<p>:::</p>
<h3 id="【初级】什么是-HDFS？"><a href="#【初级】什么是-HDFS？" class="headerlink" title="【初级】什么是 HDFS？"></a>【初级】什么是 HDFS？</h3><p>:::details 要点</p>
<p><strong>HDFS</strong> 是 <strong>Hadoop Distributed File System</strong> 的缩写，即 Hadoop 的分布式文件系统。</p>
<p>HDFS 是一种用于<strong>存储具有流数据访问模式的超大文件的文件系统</strong>，它<strong>运行在廉价的机器集群</strong>上。</p>
<p>HDFS 的设计目标是管理数以千计的服务器、数以万计的磁盘，将这么大规模的服务器计算资源当作一个单一的存储系统进行管理，对应用程序提供 <strong>PB 级</strong>的存储容量，让应用程序像使用普通文件系统一样存储大规模的文件数据。</p>
<p>HDFS 是在一个大规模分布式服务器集群上，对数据分片后进行并行读写及冗余存储。因为 HDFS 可以部署在一个比较大的服务器集群上，集群中所有服务器的磁盘都可供 HDFS 使用，所以整个 HDFS 的存储空间可以达到 PB 级容量。</p>
<p>HDFS 的常见使用场景：</p>
<ul>
<li><strong>大数据存储</strong> - HDFS 能够存储 PB 级甚至 EB 级的数据，适合存储日志数据、传感器数据、社交媒体数据等。</li>
<li><strong>批处理与分析</strong> - HDFS 是 Hadoop MapReduce 的默认存储系统，MapReduce 作业直接从 HDFS 读取数据并进行分布式计算。</li>
<li><strong>数据仓库</strong> - HDFS 可以作为数据仓库的底层存储，支持大规模数据的离线分析。</li>
<li><strong>数据冷备</strong> - 由于 HDFS 的高可靠和低成本，适用于存储访问频率较低的冷数据（如历史数据、备份数据）。</li>
<li><strong>多媒体数据存储</strong>：HDFS 适合存储大规模的多媒体数据（如图像、视频、音频）。</li>
</ul>
<p>:::</p>
<h3 id="【初级】HDFS-有什么特性（优缺点）？"><a href="#【初级】HDFS-有什么特性（优缺点）？" class="headerlink" title="【初级】HDFS 有什么特性（优缺点）？"></a>【初级】HDFS 有什么特性（优缺点）？</h3><p>:::details 要点</p>
<p><strong>HDFS 的优点</strong>：</p>
<ul>
<li><strong>高可用</strong> - 冗余数据副本，支持自动故障恢复；支持 NameNode HA、安全模式</li>
<li><strong>易扩展</strong> - 能够处理 10K 节点的规模；处理数据达到 GB、TB、甚至 PB 级别的数据；能够处理百万规模以上的文件数量，数量相当之大。</li>
<li><strong>批处理</strong> - 流式数据访问；数据位置暴露给计算框架</li>
<li><strong>低成本</strong> - HDFS 构建在廉价的商用机器上。</li>
</ul>
<p><strong>HDFS 的缺点</strong>：</p>
<ul>
<li><strong>不适合低延迟数据访问</strong> - 适合高吞吐率的场景，就是在某一时间内写入大量的数据。但是它在低延时的情况下是不行的，比如毫秒级以内读取数据，它是很难做到的。</li>
<li><strong>不适合大量小文件存储</strong><ul>
<li>存储大量小文件（这里的小文件是指小于 HDFS 系统的 Block 大小的文件（默认 64M）) 的话，它会占用 NameNode 大量的内存来存储文件、目录和块信息。这样是不可取的，因为 NameNode 的内存总是有限的。</li>
<li>磁盘寻道时间超过读取时间</li>
</ul>
</li>
<li><strong>不支持并发写入</strong> - 一个文件同时只能有一个写入者</li>
<li><strong>不支持文件随机修改</strong> - 仅支持追加写入</li>
</ul>
<p>:::</p>
<h3 id="【初级】什么是-YARN？"><a href="#【初级】什么是-YARN？" class="headerlink" title="【初级】什么是 YARN？"></a>【初级】什么是 YARN？</h3><p>:::details 要点</p>
<p><strong>YARN</strong>（Yet Another Resource Negotiator，即另一种资源调度器） 是 Hadoop 的<strong>集群资源管理系统</strong>。YARN 负责资源管理和调度。用户可以将各种服务框架部署在 YARN 上，由 YARN 进行统一地管理和资源分配。</p>
<p>在 Hadoop 1.x 版本，MapReduce 中的 jobTracker 担负了太多的责任，接收任务是它，资源调度是它，监控 TaskTracker 运行情况还是它。这样实现的好处是比较简单，但相对的，就容易出现一些问题，比如常见的单点故障问题。要解决这些问题，只能将 jobTracker 进行拆分，将其中部分功能拆解出来。沿着这个思路，于是有了 YARN。</p>
<p>:::</p>
<h3 id="【初级】什么是-MapReduce？"><a href="#【初级】什么是-MapReduce？" class="headerlink" title="【初级】什么是 MapReduce？"></a>【初级】什么是 MapReduce？</h3><p>:::details 要点</p>
<p>MapReduce 是 Hadoop 项目中的分布式计算框架。它降低了分布式计算的门槛，可以让用户轻松编写程序，让其以可靠、容错的方式运行在大型集群上并行处理海量数据（TB 级）。</p>
<p>MapReduce 的设计思路是：</p>
<ul>
<li>分而治之，并行计算</li>
<li>移动计算，而非移动数据</li>
</ul>
<p>MapReduce 作业通过将输入的数据集拆分为独立的块，这些块由 <code>map</code> 任务以并行的方式处理。框架对 <code>map</code> 的输出进行排序，然后将其输入到 <code>reduce</code> 任务中。作业的输入和输出都存储在文件系统中。该框架负责调度任务、监控任务并重新执行失败的任务。</p>
<p>通常，计算节点和存储节点是相同的，即 MapReduce 框架和 HDFS 在同一组节点上运行。此配置允许框架在已存在数据的节点上有效地调度任务，从而在整个集群中实现非常高的聚合带宽。</p>
<p>MapReduce 框架由一个主 <code>ResourceManager</code>、每个集群节点一个工作程序 <code>NodeManager</code> 和每个应用程序的 <code>MRAppMaster</code> （YARN 组件） 组成。</p>
<p>MapReduce 框架仅对 <code>&lt;key、value&gt;</code> 对进行作，也就是说，框架将作业的输入视为一组 <code>&lt;key、value&gt;</code> 对，并生成一组 <code>&lt;key、value&gt;</code> 对作为作业的输出，可以想象是不同的类型。<code>键</code>和<code>值</code>类必须可由框架序列化，因此需要实现 <a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Writable.html">Writable</a> 接口。此外，关键类必须实现 <a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/WritableComparable.html">WritableComparable</a> 接口，以便于按框架进行排序。</p>
<p>MapReduce 作业的 Input 和 Output 类型：</p>
<figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="function"><span class="title">input</span>) &lt;k1, v1&gt; -&gt;</span> <span class="function"><span class="title">map</span> -&gt;</span> &lt;<span class="function"><span class="title">k2</span>, v2&gt; -&gt;</span> <span class="function"><span class="title">combine</span> -&gt;</span> &lt;<span class="function"><span class="title">k2</span>, v2&gt; -&gt;</span> <span class="function"><span class="title">reduce</span> -&gt;</span> &lt;k3, v3&gt; (output)</span><br></pre></td></tr></table></figure>

<p>MapReduce 适用场景：</p>
<ul>
<li>数据统计，如：网站的 PV、UV 统计</li>
<li>搜索引擎构建索引</li>
<li>海量数据查询</li>
</ul>
<p>MapReduce 不适用场景：</p>
<ul>
<li>OLAP - 要求毫秒或秒级返回结果</li>
<li>流计算 - 流计算的输入数据集是动态的，而 MapReduce 是静态的</li>
<li>DAG 计算<ul>
<li>多个作业存在依赖关系，后一个的输入是前一个的输出，构成有向无环图 DAG</li>
<li>每个 MapReduce 作业的输出结果都会落盘，造成大量磁盘 IO，导致性能非常低下</li>
</ul>
</li>
</ul>
<p>:::</p>
<h3 id="【初级】MapReduce-有什么特性（优缺点）？"><a href="#【初级】MapReduce-有什么特性（优缺点）？" class="headerlink" title="【初级】MapReduce 有什么特性（优缺点）？"></a>【初级】MapReduce 有什么特性（优缺点）？</h3><p>:::details 要点</p>
<p>MapReduce 有以下特性：</p>
<ul>
<li>移动计算，而非移动数据</li>
<li>良好的扩展性：计算能力随着节点数增加，近似线性递增</li>
<li>高可用</li>
<li>适合海量数据的离线批处理</li>
<li>降低了分布式编程的门槛</li>
</ul>
<p>:::</p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><h3 id="【高级】HDFS-的架构是怎样设计的？"><a href="#【高级】HDFS-的架构是怎样设计的？" class="headerlink" title="【高级】HDFS 的架构是怎样设计的？"></a>【高级】HDFS 的架构是怎样设计的？</h3><p>:::details 要点</p>
<p>HDFS 架构有以下几个核心要点：</p>
<ul>
<li><strong>主从架构</strong></li>
<li><strong>按块分区</strong></li>
<li><strong>数据副本</strong></li>
<li><strong>命名空间</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/cs/bigdata/hdfs/hdfs-architecture.png"></p>
<p><strong>（1）HDFS 主从架构</strong></p>
<p>HDFS 采用 master&#x2F;slave 架构。一个 HDFS 集群是由一个 NameNode 和一定数目的 DataNode 组成。NameNode 是一个中心服务器，负责管理文件系统的命名空间 (namespace) 以及客户端对文件的访问。集群中的 DataNode 一般是一个节点一个，负责管理它所在节点上的存储。HDFS 暴露了文件系统的命名空间，用户能够以文件的形式在上面存储数据。从内部看，一个文件其实被分成一个或多个数据块，这些块存储在一组 DataNode 上。NameNode 执行文件系统的命名空间操作，比如打开、关闭、重命名文件或目录。它也负责确定数据块到具体 DataNode 节点的映射。DataNode 负责处理文件系统客户端的读写请求。在 NameNode 的统一调度下进行数据块的创建、删除和复制。</p>
<ul>
<li><strong>NameNode</strong> - 负责 HDFS 集群的管理、协调。具体来说，主要有以下职责：<ul>
<li><strong>管理命名空间</strong> - 执行有关命名空间的操作，例如打开，关闭、重命名文件和目录等。</li>
<li><strong>管理元数据</strong> - 维护文件的位置、所有者、权限、数据块等。</li>
<li><strong>管理 Block 副本策略</strong> - 默认 3 个副本</li>
<li><strong>客户端读写请求寻址</strong></li>
</ul>
</li>
<li><strong>DataNode</strong>：负责提供来自文件系统客户端的读写请求，执行块的创建，删除等操作。具体来说，主要有以下职责：<ul>
<li>执行客户端发送的读写操作</li>
<li>存储 Block 和数据校验和</li>
<li>定期向 NameNode 发送心跳以续活</li>
<li>定期向 NameNode 上报 Block 信息</li>
</ul>
</li>
</ul>
<p><strong>（2）按块分区</strong></p>
<p><strong>HDFS 将文件数据分割成若干数据块（Block），每个 DataNode 存储一部分数据块</strong>，这样文件就分布存储在整个 HDFS 服务器集群中。</p>
<p>将大文件分割成 Block 的主要目的是为了优化网络传输和数据处理的效率。这种分割机制使得文件的不同部分可以并行处理，大大提高了数据处理的速度。</p>
<p>HDFS Block 有以下要点：</p>
<ul>
<li>Block 是 HDFS 最小存储单元</li>
<li>文件写入 HDFS 会被切分成若干个 Block</li>
<li>Block 大小固定，默认为 128MB，可通过 <code>dfs.blocksize</code> 参数修改</li>
<li>若一个 Block 的大小小于设定值，不会占用整个块空间</li>
<li>默认情况下每个 Block 有 3 个副本</li>
</ul>
<p>这实际上是典型的分布式分区思想，使得 HDFS 具备了扩展能力。</p>
<p><strong>（3）数据复制</strong></p>
<p>HDFS 被设计成能够在一个大集群中跨机器可靠地存储超大文件。它将每个文件存储成一系列的数据块，除了最后一个，所有的数据块都是同样大小的。为了容错，文件的所有数据块都会有副本。每个文件的数据块大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。HDFS 中的文件都是一次性写入的，并且严格要求在任何时候只能有一个写入者。</p>
<p>NameNode 全权管理数据块的复制，它周期性地从集群中的每个 DataNode 接收心跳信号和块状态报告 (Blockreport)。接收到心跳信号意味着该 DataNode 节点工作正常。块状态报告包含了一个该 DataNode 上所有数据块的列表。</p>
<p><strong>（4）命名空间</strong></p>
<p>HDFS 支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统命名空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。HDFS 不支持用户磁盘配额和访问权限控制，也不支持硬链接和软链接。但是 HDFS 架构并不妨碍实现这些特性。</p>
<p>NameNode 负责维护文件系统的命名空间，任何对文件系统命名空间或属性的修改都将被 NameNode 记录下来。应用程序可以设置 HDFS 保存的文件的副本数目。文件副本的数目称为文件的副本系数，这个信息也是由 NameNode 保存的。</p>
<p>:::</p>
<h3 id="【中级】HDFS-使用-NameNode-的好处-？"><a href="#【中级】HDFS-使用-NameNode-的好处-？" class="headerlink" title="【中级】HDFS 使用 NameNode 的好处 ？"></a>【中级】HDFS 使用 NameNode 的好处 ？</h3><p>:::details 要点</p>
<p>HDFS 使用 NameNode 的好处主要体现在以下几个方面：</p>
<ul>
<li><strong>中心化的元数据管理</strong> - NameNode 在 HDFS 中负责存储整个文件系统的元数据，包括文件和目录的结构、每个文件的数据块信息及其在 DataNode 上的位置等。这种中心化的管理，使得文件系统的组织和管理变得更加简洁高效，并且可以确保整个文件系统的一致性。</li>
<li><strong>易扩展</strong> - 由于实际的数据存储在 DataNode 上，而 NameNode 只存储元数据，这样的架构设计使得 HDFS 可以轻松扩展到处理 PB 级别甚至更大规模的数据集。</li>
<li><strong>快速的文件访问</strong>：用户或应用程序在访问文件时，首先与 NameNode 交互以获得数据块的位置信息，然后直接从 DataNode 读取数据。这种方式可以快速定位数据，提高文件访问的效率。</li>
<li><strong>容错和恢复机制</strong>：NameNode 可以监控 DataNode 的状态，实现系统的容错。在 DataNode 发生故障时，NameNode 可以指导其它 DataNode 复制丢失的数据块，保证数据的可靠性。</li>
<li><strong>简化数据管理</strong>：NameNode 的存在简化了数据的管理和维护。例如，在进行数据备份、系统升级或扩展时，管理员只需要关注 NameNode 上的元数据，而不是每个节点上存储的实际数据。</li>
</ul>
<p>然而，由于 NameNode 是中心节点，它也成为了系统的一个潜在瓶颈和单点故障。因此，HDFS 后来引入了主备 NameNode 机制来保证 NameNode 自身的可用性。</p>
<p>:::</p>
<h3 id="【中级】HDFS-使用-Block-的好处-？"><a href="#【中级】HDFS-使用-Block-的好处-？" class="headerlink" title="【中级】HDFS 使用 Block 的好处 ？"></a>【中级】HDFS 使用 Block 的好处 ？</h3><p>:::details 要点</p>
<p>HDFS 采用文件分块（Block）进行存储管理，主要是基于以下几个原因：</p>
<ul>
<li><strong>提高可靠性和容错性</strong> - 通过将文件分成多个块，并在不同的 DataNode 上存储这些块的副本，HDFS 可以提高数据的可靠性。即使某些 DataNode 出现故障，其他节点上的副本仍然可以用于数据恢复。</li>
<li><strong>提高数据处理效率</strong>：在处理大规模数据集时，将大文件分割成小块可以提高数据处理的效率。这样，可以并行地在多个节点上处理不同的块，从而加速数据处理和分析。</li>
<li><strong>提高网络传输效率</strong>：分块存储还有利于网络传输。当处理或传输一个大文件的部分数据时，只需处理或传输相关的几个块，而不是整个文件，这减少了网络传输负担。</li>
<li><strong>易于扩展</strong>：分块机制使得 HDFS 易于扩展。可以简单地通过增加更多的 DataNode 来扩大存储容量和处理能力，而不需要对现有的数据块进行任何修改。</li>
<li><strong>负载均衡</strong>：分块存储还有助于在集群中实现负载均衡。不同的数据块可以分布在不同的节点上，从而均衡各个节点的存储和处理负载。</li>
</ul>
<p>:::</p>
<h3 id="【中级】NameNode-与-SecondaryNameNode-的区别与联系-？"><a href="#【中级】NameNode-与-SecondaryNameNode-的区别与联系-？" class="headerlink" title="【中级】NameNode 与 SecondaryNameNode 的区别与联系 ？"></a>【中级】NameNode 与 SecondaryNameNode 的区别与联系 ？</h3><p>:::details 要点</p>
<p>NameNode 和 SecondaryNameNode 的<strong>区别</strong>：</p>
<ul>
<li>NameNode 是 HDFS 的主要节点，负责管理文件系统的命名空间。它维护着整个文件系统的目录和文件结构，以及所有文件的元数据，包括文件的数据块（block）信息、数据块的位置等。</li>
<li>SecondaryNameNode 是 NameNode 的辅助节点。<ul>
<li>SecondaryNameNode 不是 NameNode 的备份，不能在 NameNode 故障时接管其功能。</li>
<li>HDFS 在运行过程中，所有的事务（如文件创建、删除等）都会首先记录在 NameNode 的内存和 EditLog 中。SecondaryNameNode 定期从 NameNode 获取这些日志文件，与文件系统的命名空间镜像（FsImage）合并，然后把新的 FsImage 送回给 NameNode，以帮助减少 NameNode 的内存压力。</li>
</ul>
</li>
</ul>
<p>NameNode 和 SecondaryNameNode 的<strong>联系</strong>：</p>
<ul>
<li><strong>共同目标</strong>：二者共同目的是维护 HDFS 的稳定和高效运作。NameNode 作为核心，负责实时的元数据管理；而 SecondaryNameNode 辅助 NameNode，通过定期处理 FsImage 和 EditLog，减轻 NameNode 的负担。</li>
<li><strong>数据交互</strong>：SecondaryNameNode 的工作依赖于与 NameNode 的交互，从 NameNode 获取元数据的状态和编辑日志。</li>
</ul>
<p>:::</p>
<h3 id="【中级】什么是-FsImage-和-EditLog？"><a href="#【中级】什么是-FsImage-和-EditLog？" class="headerlink" title="【中级】什么是 FsImage 和 EditLog？"></a>【中级】什么是 FsImage 和 EditLog？</h3><p>:::details 要点</p>
<p>HDFS 中，<code>FsImage</code>和<code>EditLog</code>是两个关键的文件，用于存储和管理文件系统的元数据。它们的主要区别如下：</p>
<p><strong>FsImage（文件系统镜像）</strong></p>
<ul>
<li><strong>内容</strong>：<code>FsImage</code>包含 HDFS 元数据的完整快照，例如文件系统的目录树、文件和目录的属性等。</li>
<li><strong>静态性</strong>：它是在特定时间点上的静态快照。一旦创建，除非进行新的快照操作，否则内容不会改变。</li>
<li><strong>使用场景</strong>：在 NameNode 启动时使用，用于加载文件系统的最初状态。此外，在进行系统备份时也会生成新的<code>FsImage</code>。</li>
<li><strong>更新频率</strong>：不是实时更新的。通常在系统进行 checkpoint 操作时才会更新。</li>
</ul>
<p><strong>EditLog（编辑日志）</strong></p>
<ul>
<li><strong>内容</strong>：<code>EditLog</code>记录了自上一个<code>FsImage</code>快照以来所有对文件系统所做的增量更改。这些更改包括文件和目录的创建、删除、重命名等操作。</li>
<li><strong>动态性</strong>：它是一个动态更新的日志文件。每次对文件系统进行更改时，这个更改就会记录在<code>EditLog</code>中。</li>
<li><strong>使用场景</strong>：用于记录所有的文件系统更改操作。在 NameNode 重启时，<code>FsImage</code>将与<code>EditLog</code>结合使用，以重建文件系统的最新状态。</li>
<li><strong>更新频率</strong>：实时更新。每次对文件系统的更改都会迅速反映在<code>EditLog</code>中。</li>
</ul>
<p><strong>结合使用</strong></p>
<p>在 HDFS 中，<code>FsImage</code>和<code>EditLog</code>一起工作，以确保文件系统的元数据既能够被可靠地存储，又能够反映最新的更改。定期进行 checkpoint 操作（由 Secondary NameNode 或 Standby NameNode 执行）会将<code>EditLog</code>中的更改应用到<code>FsImage</code>中，创建一个新的、更新的快照。这样可以保证在系统重启或恢复时，可以快速加载最新的文件系统状态。</p>
<p>:::</p>
<h3 id="【中级】YARN-有哪些核心组件？"><a href="#【中级】YARN-有哪些核心组件？" class="headerlink" title="【中级】YARN 有哪些核心组件？"></a>【中级】YARN 有哪些核心组件？</h3><p>:::details 要点</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202502192257406.gif" alt="YARN Architecture"></p>
<p>YARN 有以下核心组件：</p>
<ul>
<li><strong>ResourceManager</strong> - ResourceManager 是<strong>管理资源</strong>和安排在 YARN 上运行的<strong>中央调度器</strong>。整个系统有且只有一个 ResourceManager，因为号令发布都来自一处，因此不存在调度不一致的情况（很多分布式系统都是通过经典的一主多从模式来解决一致性问题的）。它也包含了两个主要的子组件：<ul>
<li><strong>定时调度器（Scheduler）</strong> - 从本质上来说，定时调度器就是一种策略，或者说一种算法。当 Client 提交一个任务的时候，它会根据所需要的资源以及当前集群的资源状况进行分配。注意，它只负责向应用程序分配资源，并不做监控以及应用程序的状态跟踪。</li>
<li><strong>应用管理器（ApplicationManager）</strong> - 应用管理器就是负责管理 Client 提交的应用。上面不是说到定时调度器（Scheduler）不对用户提交的程序监控嘛，其实啊，监控应用的工作正是由应用管理器（ApplicationManager）完成的。</li>
</ul>
</li>
<li><strong>NodeManager</strong> - NodeManager 是 ResourceManager 在每台机器的上代理，负责容器的管理，并监控他们的资源使用情况（cpu、内存、磁盘及网络等），以及向 ResourceManager&#x2F;Scheduler 提供这些资源使用报告。</li>
<li><strong>ApplicationMaster</strong> - 每当 Client 提交一个 Application 时候，就会新建一个 ApplicationMaster 。由这个 ApplicationMaster 去与 ResourceManager 申请容器资源，获得资源后会将要运行的程序发送到容器上启动，然后进行分布式计算。这么设计的原因在于，数据量大的时候，移动数据成本太高，耗时太久，改为移动计算代价较小。</li>
<li><strong>Container</strong> - <code>Container</code> 是 YARN 对资源的抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。当 AM 向 RM 申请资源时，RM 为 AM 返回的资源是用 <code>Container</code> 表示的。<ul>
<li>YARN 会为每个任务分配一个 <code>Container</code>，该任务只能使用该 <code>Container</code> 中描述的资源。</li>
<li><code>ApplicationMaster</code> 可在 <code>Container</code> 内运行任何类型的任务。例如，<code>MapReduce ApplicationMaster</code> 请求一个容器来启动 map 或 reduce 任务，而 <code>Giraph ApplicationMaster</code> 请求一个容器来运行 Giraph 任务。</li>
<li>容器由 NodeManager 启动和管理，并被它所监控。</li>
<li>容器被 ResourceManager 所调度。</li>
</ul>
</li>
</ul>
<p>:::</p>
<h3 id="【中级】MapReduce-有哪些核心组件？"><a href="#【中级】MapReduce-有哪些核心组件？" class="headerlink" title="【中级】MapReduce 有哪些核心组件？"></a>【中级】MapReduce 有哪些核心组件？</h3><p>:::details 要点</p>
<p>MapReduce 有以下核心组件：</p>
<ul>
<li><strong>Job</strong> - <a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/Job.html">Job</a> 表示 MapReduce 作业配置。<code>Job</code> 通常用于指定 <code>Mapper</code>、combiner（如果有）、<code>Partitioner</code>、<code>Reducer</code>、<code>InputFormat</code>、<code>OutputFormat</code> 实现。</li>
<li><strong>Mapper</strong> - <a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/Mapper.html">Mapper</a> 负责将输入键值对<strong>映射</strong>到一组中间键值对。转换的中间记录不需要与输入记录具有相同的类型。一个给定的输入键值对可能映射到零个或多个输出键值对。</li>
<li><strong>Combiner</strong> - <code>combiner</code> 是 <code>map</code> 运算后的可选操作，它实际上是一个本地化的 <code>reduce</code> 操作。它执行中间输出的本地聚合，这有助于减少从 <code>Mapper</code> 传输到 <code>Reducer</code> 的数据量。</li>
<li><strong>Reducer</strong> - <a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/Reducer.html">Reducer</a> 将共享一个 key 的一组中间值归并为一个小的数值集。Reducer 有 3 个主要子阶段：shuffle，sort 和 reduce。<ul>
<li><strong>shuffle</strong> - Reducer 的输入就是 mapper 的排序输出。在这个阶段，框架通过 HTTP 获取所有 mapper 输出的相关分区。</li>
<li><strong>sort</strong> - 在这个阶段中，框架将按照 key （因为不同 mapper 的输出中可能会有相同的 key) 对 Reducer 的输入进行分组。shuffle 和 sort 两个阶段是同时发生的。</li>
<li><strong>reduce</strong> - 对按键分组的数据进行聚合统计。</li>
</ul>
</li>
<li><strong>Partitioner</strong> - <a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/Partitioner.html">Partitioner</a> 负责控制 map 中间输出结果的键的分区。<ul>
<li>键（或者键的子集）用于产生分区，通常通过一个散列函数。</li>
<li>分区总数与作业的 reduce 任务数是一样的。因此，它控制中间输出结果（也就是这条记录）的键发送给 m 个 reduce 任务中的哪一个来进行 reduce 操作。</li>
</ul>
</li>
<li><strong>InputFormat</strong> - <a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/InputFormat.html">InputFormat</a> 描述 MapReduce 作业的输入规范。MapReduce 框架依赖作业的 InputFormat 来完成以下工作：<ul>
<li>确认作业的输入规范。</li>
<li>把输入文件分割成多个逻辑的 InputSplit 实例，然后将每个实例分配给一个单独的 Mapper。<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/InputSplit.html">InputSplit</a> 表示要由单个 <code>Mapper</code> 处理的数据。</li>
<li>提供 RecordReader 的实现。<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/RecordReader.html">RecordReader</a> 从 <code>InputSplit</code> 中读取 <code>&lt;key， value&gt;</code> 对，并提供给 <code>Mapper</code> 实现进行处理。</li>
</ul>
</li>
<li><strong>OutputFormat</strong> - <a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/OutputFormat.html">OutputFormat</a> 描述 MapReduce 作业的输出规范。MapReduce 框架依赖作业的 OutputFormat 来完成以下工作：<ul>
<li>确认作业的输出规范，例如检查输出路径是否已经存在。</li>
<li>提供 RecordWriter 实现。<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/RecordWriter.html">RecordWriter</a> 将输出 <code>&lt;key， value&gt;</code> 对到文件系统。</li>
</ul>
</li>
</ul>
<p>:::</p>
<h2 id="工作流"><a href="#工作流" class="headerlink" title="工作流"></a>工作流</h2><h3 id="【中级】HDFS-的写数据流程是怎样的？"><a href="#【中级】HDFS-的写数据流程是怎样的？" class="headerlink" title="【中级】HDFS 的写数据流程是怎样的？"></a>【中级】HDFS 的写数据流程是怎样的？</h3><p>:::details 要点</p>
<p>HDFS 写数据流程大致为：</p>
<ol>
<li><strong>按 Block 大小分割数据</strong></li>
<li><strong>通过 NameNode 寻址 DataNode</strong></li>
<li><strong>向 DataNode 写数据</strong></li>
<li><strong>完成后通知 NameNode</strong></li>
</ol>
<blockquote>
<p>扩展：下面的漫画生动的展示了 HDFS 的写入流程，图片引用自博客：<a target="_blank" rel="noopener" href="https://blog.csdn.net/hudiefenmu/article/details/37655491">翻译经典 HDFS 原理讲解漫画</a></p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202502192300702.jpg"></p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202502192301943.jpg"></p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202502192302127.jpg"></p>
</blockquote>
<p>HDFS 写数据的源码流程：</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/cs/bigdata/hdfs/hdfs-write.png"></p>
<ol>
<li>客户端通过对 <code>DistributedFileSystem</code> 对象调用 <code>create()</code> 函数来<strong>新建文件</strong>。</li>
<li>分布式文件系统对 NameNode 创建一个 RPC 调用，<strong>在文件系统的命名空间中新建一个文件</strong>。</li>
<li>NameNode 对新建文件进行检查无误后，分布式文件系统返回给客户端一个 <code>FSDataOutputStream</code> 对象，<code>FSDataOutputStream</code> 对象封装一个 <code>DFSoutPutstream</code> 对象，负责处理 NameNode 和 DataNode 之间的通信，<strong>客户端开始写入数据</strong>。</li>
<li><code>FSDataOutputStream</code> 将<strong>数据分成一个一个的数据包，写入内部数据队列</strong>，DataStreamer 负责将数据包依次流式传输到由一组 NameNode 构成的管道中。</li>
<li><code>DFSOutputStream</code> 维护着确认队列来等待 DataNode 收到确认回执，<strong>收到管道中所有 DataNode 确认后，数据包从确认队列删除</strong>。</li>
<li><strong>客户端完成数据的写入</strong>，调用 <code>close()</code> 方法关闭传输通道。</li>
<li>NameNode <strong>确认完成</strong>。</li>
</ol>
<p>:::</p>
<h3 id="【中级】HDFS-的读数据流程是怎样的？"><a href="#【中级】HDFS-的读数据流程是怎样的？" class="headerlink" title="【中级】HDFS 的读数据流程是怎样的？"></a>【中级】HDFS 的读数据流程是怎样的？</h3><p>:::details 要点</p>
<p>HDFS 读数据流程大致为：</p>
<ol>
<li>客户端向 NameNode 查询文件信息</li>
<li>NameNode 返回相关信息<ul>
<li>该文件的所有数据块</li>
<li>每个数据块对应的 DataNode（按距离客户端的远近排序）</li>
</ul>
</li>
<li>客户端向 DataNode 读数据</li>
</ol>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202502192303732.jpg"></p>
<p>HDFS 读数据的源码流程：</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/cs/bigdata/hdfs/hdfs-read.png"></p>
<ol>
<li>客户端调用 <code>FileSystem</code> 对象的 <code>open()</code> 方法在 HDFS 中<strong>打开要读取的文件</strong>。</li>
<li>HDFS 通过使用 RPC（远程过程调用）来调用 NameNode，<strong>确定文件起始块（Block）的位置</strong>。</li>
<li><code>DistributedFileSystem</code> 类返回一个支持文件定位的输入流 <code>FSDataInputStream</code> 对象，<code>FSDataInputStream</code> 对象接着封装 <code>DFSInputStream</code> 对象（<strong>存储着文件起始几个块的 DataNode 地址</strong>），客户端对这个输入流调用 <code>read()</code> 方法。</li>
<li><code>DFSInputStream</code> 连接距离最近的 DataNode，通过反复调用 <code>read</code> 方法，<strong>将数据从 DataNode 传输到客户端</strong>。</li>
<li>到达块的末端时，<code>DFSInputStream</code> 关闭与该 DataNode 的连接，<strong>寻找下一个块的最佳 DataNode</strong>。</li>
<li>客户端完成读取，对 <code>FSDataInputStream</code> 调用 <code>close()</code> 方法<strong>关闭连接</strong>。</li>
</ol>
<p>:::</p>
<h3 id="【中级】MapReduce-是如何工作的？"><a href="#【中级】MapReduce-是如何工作的？" class="headerlink" title="【中级】MapReduce 是如何工作的？"></a>【中级】MapReduce 是如何工作的？</h3><p>:::details 要点</p>
<p>MapReduce 任务过程分为两个处理阶段：map 极端和 reduce 阶段。每阶段都以键值对作为输入和输出，其类型由程序员来选择。程序员还需要写两个函数：map 函数和 reduce 函数。</p>
<p>以词频统计为例，其工作流再细分一下，可以划分为以下阶段：</p>
<ol>
<li><strong>input</strong> - 读取文本文件；</li>
<li><strong>splitting</strong> - 将文件按照行进行拆分，此时得到的 <code>K1</code> 行数，<code>V1</code> 表示对应行的文本内容；</li>
<li><strong>mapping</strong> - 并行将每一行按照空格进行拆分，拆分得到的 <code>List(K2,V2)</code>，其中 <code>K2</code> 代表每一个单词，由于是做词频统计，所以 <code>V2</code> 的值为 1，代表出现 1 次；</li>
<li><strong>shuffling</strong> - 由于 <code>Mapping</code> 操作可能是在不同的机器上并行处理的，所以需要通过 <code>shuffling</code> 将相同 <code>key</code> 值的数据分发到同一个节点上去合并，这样才能统计出最终的结果，此时得到 <code>K2</code> 为每一个单词，<code>List(V2)</code> 为可迭代集合，<code>V2</code> 就是 Mapping 中的 V2；</li>
<li><strong>reducing</strong> - 这里的案例是统计单词出现的总次数，所以 <code>Reducing</code> 对 <code>List(V2)</code> 进行归约求和操作，最终输出。</li>
</ol>
<p>MapReduce 编程模型中 <code>splitting</code> 和 <code>shuffing</code> 操作都是由框架实现的，需要我们自己编程实现的只有 <code>mapping</code> 和 <code>reducing</code>，这也就是 MapReduce 这个称呼的来源。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20200601162305.png" alt="MapReduce 工作流"></p>
<p>:::</p>
<h3 id="【中级】YARN-是如何工作的？"><a href="#【中级】YARN-是如何工作的？" class="headerlink" title="【中级】YARN 是如何工作的？"></a>【中级】YARN 是如何工作的？</h3><p>:::details 要点</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202502192303306.jpeg"></p>
<p>这张图简单地标明了提交一个程序所经历的流程，接下来我们来具体说说每一步的过程。</p>
<ol>
<li>Client 向 ResourceManager 申请运行一个 Application 进程，这里我们假设是一个 MapReduce 作业。</li>
<li>ResourceManager 向 NodeManager 通信，为该 Application 进程分配第一个容器。并在这个容器中运行这个应用程序对应的 ApplicationMaster。</li>
<li>ApplicationMaster 启动以后，对作业（也就是 Application） 进行拆分，拆分 task 出来，这些 task 可以运行在一个或多个容器中。然后向 ResourceManager 申请要运行程序的容器，并定时向 ResourceManager 发送心跳。</li>
<li>申请到容器后，ApplicationMaster 会去和容器对应的 NodeManager 通信，而后将作业分发到对应的 NodeManager 中的容器去运行，这里会将拆分后的 MapReduce 进行分发，对应容器中运行的可能是 Map 任务，也可能是 Reduce 任务。</li>
<li>容器中运行的任务会向 ApplicationMaster 发送心跳，汇报自身情况。当程序运行完成后， ApplicationMaster 再向 ResourceManager 注销并释放容器资源。</li>
</ol>
<p>:::</p>
<h2 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h2><p><strong>复制主要指通过网络在多台机器上保存相同数据的副本</strong>。</p>
<p>复制数据，可能出于各种各样的原因：</p>
<ul>
<li><strong>提高可用性</strong> - 当部分组件出现位障，系统依然可以继续工作，系统依然可以继续工作。</li>
<li><strong>降低访问延迟</strong> - 使数据在地理位置上更接近用户。</li>
<li><strong>提高读吞吐量</strong> - 扩展至多台机器以同时提供数据访问服务。</li>
</ul>
<p>所有分布式系统都需要支持复制。</p>
<h3 id="【中级】HDFS-的副本机制是怎样的？"><a href="#【中级】HDFS-的副本机制是怎样的？" class="headerlink" title="【中级】HDFS 的副本机制是怎样的？"></a>【中级】HDFS 的副本机制是怎样的？</h3><p>:::details 要点</p>
<h4 id="基于块的副本"><a href="#基于块的副本" class="headerlink" title="基于块的副本"></a>基于块的副本</h4><p>由于 Hadoop 被设计运行在廉价的机器上，这意味着硬件是不可靠的，为了保证容错性，HDFS 提供了副本机制。HDFS 将文件分解为若干 Block，Block 是 HDFS 最小存储单元，每个 Block 有多个副本。</p>
<p>HDFS 的默认副本数为 3，更多的副本意味着更高的数据安全性，但同时也会带来更高的额外开销（存储成本和带宽成本）。3 个副本是在保障数据可靠性和系统成本之间的一个较好的平衡点。</p>
<p>副本数可以通过以下方式修改：</p>
<ul>
<li>在 HDFS 的配置文件 hdfs-site.xml 中，有一个名为 <code>dfs.replication</code> 的属性，可以设置<strong>全局的默认副本数</strong>。修改这个值后，需要重启 HDFS 使配置生效。</li>
<li>针对单个文件或目录修改副本数：如果只想改变某个特定文件或目录的副本数，而不影响整个系统的默认设置，可以使用 HDFS 的命令行工具。例如，使用命令<code>hdfs dfs -setrep -w &lt;副本数&gt; &lt;文件/目录路径&gt;</code> 来修改特定文件或目录的副本数。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20200224203958.png"></p>
<p><strong>NameNode 全权管理数据块的复制</strong>，它周期性地从集群中的每个 DataNode 接收心跳信号和块状态报告 (BlockReport)。接收到心跳信号意味着该 DataNode 节点工作正常。块状态报告包含了一个该 DataNode 上所有数据块的列表。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/cs/bigdata/hdfs/hdfs-replica.png"></p>
<h4 id="副本分布策略"><a href="#副本分布策略" class="headerlink" title="副本分布策略"></a>副本分布策略</h4><p>副本分布策略是 HDFS 可靠性和性能的关键。优化的副本存放策略是 HDFS 区分于其他大部分分布式文件系统的重要特性。HDFS 采用一种称为机架感知 (rack-aware) 的策略来改进数据的可靠性、可用性和网络带宽的利用率。大型 HDFS 实例一般运行在跨越多个机架的计算机组成的集群上，不同机架上的两台机器之间的通信需要经过交换机。在大多数情况下，同一个机架内的两台机器间的带宽会比不同机架的两台机器间的带宽大。</p>
<p>通过一个机架感知的过程，NameNode 可以确定每个 DataNode 所属的机架 id。一个简单但没有优化的策略就是将副本存放在不同的机架上。这样可以有效防止当整个机架失效时数据的丢失，并且允许读数据的时候充分利用多个机架的带宽。这种策略设置可以将副本均匀分布在集群中，有利于当组件失效情况下的负载均衡。但是，因为这种策略的一个写操作需要传输数据块到多个机架，这增加了写的代价。</p>
<p>HDFS 默认的副本数为 3，此时 HDFS 的副本分布策略是：</p>
<ul>
<li><strong>副本 1</strong> - 放在 Client 所在节点；对于远程 Client，系统会随机选择节点</li>
<li><strong>副本 2</strong> - 放在不同机架的节点上</li>
<li><strong>副本 3</strong> - 放在与第二个副本同一机架的不同节点上</li>
<li><strong>副本 N</strong> - 在满足以下条件的节点中随机选择<ul>
<li>每个节点只存储一份副本</li>
<li>每个机架最多存储两份副本</li>
</ul>
</li>
<li><strong>优选</strong> - 同等条件下优先选择空闲节点。<ul>
<li>如果某个 DataNode 节点上的空闲空间低于特定的临界点，按照均衡策略系统就会自动地将数据从这个 DataNode 移动到其他空闲的 DataNode。</li>
</ul>
</li>
</ul>
<h4 id="副本选择"><a href="#副本选择" class="headerlink" title="副本选择"></a>副本选择</h4><p>为了降低整体的带宽消耗和读取延时，HDFS 会尽量让客户端程序读取离它最近的副本。如果在客户端程序的同一个机架上有一个副本，那么就读取该副本。如果一个 HDFS 集群跨越多个数据中心，那么客户端也将首先读本地数据中心的副本。</p>
<p>为了最大限度地减少带宽消耗和读取延迟，HDFS 在执行读取请求时，优先读取距离读取器最近的副本。如果在与读取器节点相同的机架上存在副本，则优先选择该副本。如果 HDFS 群集跨越多个数据中心，则优先选择本地数据中心上的副本。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202502192304008.jpg"></p>
<p>:::</p>
<h3 id="【中级】HDFS-如何保证数据一致性？"><a href="#【中级】HDFS-如何保证数据一致性？" class="headerlink" title="【中级】HDFS 如何保证数据一致性？"></a>【中级】HDFS 如何保证数据一致性？</h3><p>:::details 要点</p>
<p>HDFS 的数据一致性主要依赖以下机制来保证：</p>
<ul>
<li><strong>NameNode 的中心化管理</strong> - NameNode 在 HDFS 中负责存储整个文件系统的元数据，包括文件和目录的结构、每个文件的数据块信息及其在 DataNode 上的位置等。这种中心化的管理，使得文件系统的组织和管理变得更加简洁高效，并且可以确保整个文件系统的一致性。</li>
<li><strong>数据块的复制（Replication）</strong> - HDFS 采用副本来保证数据的可靠性。一旦数据写入完成，副本就会分散存储在不同的 DataNodes 上。尽管这种方法不是强一致性模型，但通过足够数量的副本和及时的副本替换策略，HDFS 能够提供较高水平的数据一致性和可靠性。</li>
<li><strong>写入和复制的原子性保证</strong> - 在 HDFS 中，文件一旦创建，其内容就不能被更新，只能被追加或重写。这种方式简化了并发控制，因为写操作在文件级别上是原子的。在复制数据块时，HDFS 保证原子性复制，即一个数据块的所有副本在任何时间点上都是相同的。如果复制过程中出现错误，那么不完整的副本会被删除，系统会重新尝试复制直到成功。</li>
<li><strong>客户端的一致性协议</strong> - 客户端在与 HDFS 交互时，遵循特定的协议。例如，客户端在完成文件写入之后，需要向 NameNode 通知，以确保 NameNode 更新文件的元数据。这样可以保证 NameNode 的元数据与实际存储的数据保持一致。</li>
<li><strong>定期检查和错误恢复</strong><ul>
<li><strong>心跳和健康检查</strong> - DataNodes 定期向 NameNode 发送心跳和 Block 健康状况报告。NameNode 利用这些信息来检查和维护系统的整体一致性。例如，如果某个 DataNode 失败，NameNode 会重新组织数据块的副本。</li>
<li><strong>校验</strong> - HDFS 在存储和传输数据时，会计算数据的校验和。在读取数据时，会验证这些校验和，确保数据的完整性。</li>
</ul>
</li>
</ul>
<p>通过这些机制，HDFS 确保了系统中的数据在正常操作和故障情况下的一致性和可靠性。虽然 HDFS 不提供像传统数据库那样的强一致性保证，但它的设计和实现确保了在大规模数据处理场景中的有效性和健壮性。</p>
<p>:::</p>
<h2 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h2><h3 id="【中级】HDFS-有哪些故障类型？如何检测故障？"><a href="#【中级】HDFS-有哪些故障类型？如何检测故障？" class="headerlink" title="【中级】HDFS 有哪些故障类型？如何检测故障？"></a>【中级】HDFS 有哪些故障类型？如何检测故障？</h3><p>:::details 要点</p>
<p>HDFS 常见故障及检测方法：</p>
<ul>
<li><strong>节点故障</strong><ul>
<li>DataNode 每 3 秒向 NameNode 发送心跳</li>
<li>超时未收到心跳，NameNode 判定 DataNode 宕机</li>
</ul>
</li>
<li><strong>通信故障</strong><ul>
<li>客户端请求 DataNode 会收到 ACK</li>
</ul>
</li>
<li><strong>数据损坏</strong><ul>
<li>磁盘介质在存储过程中受环境或者老化影响，其存储的数据可能会出现错乱。HDFS 的应对措施是，对于存储在 DataNode 上的数据块，计算并存储校验和（CheckSum）。在读取数据的时候，重新计算读取出来的数据的校验和，如果校验不正确就抛出异常，应用程序捕获异常后就到其他 DataNode 上读取备份数据。</li>
<li>如果 DataNode 监测到本机的某块磁盘损坏，就将该块磁盘上存储的所有 BlockID 报告给 NameNode，NameNode 检查这些数据块还在哪些 DataNode 上有备份，通知相应的 DataNode 服务器将对应的数据块复制到其他服务器上，以保证数据块的备份数满足要求。</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202502192304755.jpg"></p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202502192305651.jpg"></p>
<p>:::</p>
<h3 id="【中级】HDFS-读写故障如何处理？"><a href="#【中级】HDFS-读写故障如何处理？" class="headerlink" title="【中级】HDFS 读写故障如何处理？"></a>【中级】HDFS 读写故障如何处理？</h3><p>:::details 要点</p>
<h4 id="写入故障处理"><a href="#写入故障处理" class="headerlink" title="写入故障处理"></a>写入故障处理</h4><ul>
<li>写入数据通过数据包传输</li>
<li>DataNode 接收数据后，返回 ACK</li>
<li>如果客户端没有收到 ACK，就判定 DataNode 宕机，跳过节点</li>
<li>没有充分备份的数据块信息通知到 NameNode</li>
</ul>
<h4 id="读取故障处理"><a href="#读取故障处理" class="headerlink" title="读取故障处理"></a>读取故障处理</h4><ul>
<li>读数据先要通过 NameNode 寻址该数据块的所有 DataNode</li>
<li>如果某 DataNode 宕机，则读取其他节点</li>
</ul>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202502192305332.jpg"></p>
<p>:::</p>
<h3 id="【中级】DataNode-故障如何处理？"><a href="#【中级】DataNode-故障如何处理？" class="headerlink" title="【中级】DataNode 故障如何处理？"></a>【中级】DataNode 故障如何处理？</h3><p>:::details 要点</p>
<p>DataNode 每 3 秒会向 NameNode 发送心跳消息，以证明自身正常工作。如果 DataNode 超时未发送心跳，NameNode 就会认为该 DataNode 已经宕机。</p>
<p>NameNode 会立即查找该 DataNode 上存储的数据块有哪些，以及这些数据块还存储在哪些其他 DataNode 上。</p>
<p>随后，NameNode 通知这些 DataNode 再复制一份数据块到其他 DataNode 上，保证 HDFS 存储的数据块副本数符合配置数。即使再出现服务器宕机，也不会丢失数据。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202502192306957.jpg"></p>
<p>:::</p>
<h3 id="【中级】NameNode-故障如何处理？"><a href="#【中级】NameNode-故障如何处理？" class="headerlink" title="【中级】NameNode 故障如何处理？"></a>【中级】NameNode 故障如何处理？</h3><p>:::details 要点</p>
<p>NameNode 是整个 HDFS 的核心，记录着 HDFS 文件分配表信息，所有的文件路径和数据块存储信息都保存在 NameNode，如果 NameNode 故障，整个 HDFS 系统集群都无法使用。如果 NameNode 上记录的数据丢失，整个集群所有 DataNode 存储的数据也就没用了。</p>
<p>NameNode 通过主备架构实现故障转移。</p>
<ul>
<li><strong>Active NameNode</strong> - 是正在工作的 NameNode；</li>
<li><strong>Standby NameNode</strong> - 是备份的 NameNode。</li>
</ul>
<p>Active NameNode 宕机后，Standby NameNode 快速升级为新的 Active NameNode。Standby NameNode 周期性同步 <code>edits</code> 编辑日志，定期合并 <code>FsImage</code> 与 <code>edits</code> 到本地磁盘。</p>
<blockquote>
<p>注：Hadoop 3.0 允许配置多个 Standby NameNode。</p>
</blockquote>
<h4 id="元数据文件"><a href="#元数据文件" class="headerlink" title="元数据文件"></a>元数据文件</h4><ul>
<li><strong>edits（编辑日志文件）</strong> - 保存了自最新检查点（Checkpoint）之后的所有文件更新操作。</li>
<li><strong>FsImage（元数据检查点镜像文件）</strong> - 保存了文件系统中所有的目录和文件信息，如：某个目录下有哪些子目录和文件，以及文件名、文件副本数、文件由哪些 Block 组成等。</li>
</ul>
<p>Active NameNode 内存中有一份最新的元数据（&#x3D; FsImage + edits）。</p>
<p>Standby NameNode 在检查点定期将内存中的元数据保存到 FsImage 文件中。</p>
<h4 id="利用-QJM-实现元数据高可用"><a href="#利用-QJM-实现元数据高可用" class="headerlink" title="利用 QJM 实现元数据高可用"></a>利用 QJM 实现元数据高可用</h4><blockquote>
<p>基于 Paxos 算法</p>
</blockquote>
<p>QJM 机制（Quorum Journal Manager）</p>
<p>只要保证 Quorum（法定人数）数量的操作成功，就认为这是一次最终成功的操作</p>
<p>QJM 共享存储系统</p>
<ul>
<li>部署奇数（2N+1）个 JournalNode</li>
<li>JournalNode 负责存储 edits 编辑日志</li>
<li>写 edits 的时候，只要超过半数（N+1）的 JournalNode 返回成功，就代表本次写入成功</li>
<li>最多可容忍 N 个 JournalNode 宕机</li>
</ul>
<p>利用 ZooKeeper 实现 Active 节点选举。</p>
<p>:::</p>
<h3 id="【中级】HDFS-安全模式有什么作用？"><a href="#【中级】HDFS-安全模式有什么作用？" class="headerlink" title="【中级】HDFS 安全模式有什么作用？"></a>【中级】HDFS 安全模式有什么作用？</h3><p>:::details 要点</p>
<p>在启动过程中，NameNode 进入安全模式。在这个模式下，它会检查数据块的健康状况和副本数量。只有在足够数量的数据块可用时，NameNode 才会退出安全模式，开始正常的操作。</p>
<p>:::</p>
<h2 id="HA"><a href="#HA" class="headerlink" title="HA"></a>HA</h2><h3 id="【高级】HDFS-如何实现高可用？"><a href="#【高级】HDFS-如何实现高可用？" class="headerlink" title="【高级】HDFS 如何实现高可用？"></a>【高级】HDFS 如何实现高可用？</h3><p>:::details 要点</p>
<p>HDFS 高可用架构如下：</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202502192307157.png"></p>
<p>HDFS 高可用架构主要由以下组件所构成：</p>
<ul>
<li><strong>Active NameNode 和 Standby NameNode</strong>：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务。</li>
<li><strong>主备切换控制器 ZKFailoverController</strong>：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换，当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换。</li>
<li><strong>Zookeeper 集群</strong>：为主备切换控制器提供主备选举支持。</li>
<li><strong>共享存储系统</strong>：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。</li>
<li><strong>DataNode 节点</strong>：除了通过共享存储系统共享 HDFS 的元数据信息之外，主 NameNode 和备 NameNode 还需要共享 HDFS 的数据块和 DataNode 之间的映射关系。DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。</li>
</ul>
<p>目前 Hadoop 支持使用 Quorum Journal Manager (QJM) 或 Network File System (NFS) 作为共享的存储系统，这里以 QJM 集群为例进行说明：Active NameNode 首先把 EditLog 提交到 JournalNode 集群，然后 Standby NameNode 再从 JournalNode 集群定时同步 EditLog，当 Active NameNode 宕机后， Standby NameNode 在确认元数据完全同步之后就可以对外提供服务。</p>
<p>需要说明的是向 JournalNode 集群写入 EditLog 是遵循 “过半写入则成功” 的策略，所以你至少要有 3 个 JournalNode 节点，当然你也可以继续增加节点数量，但是应该保证节点总数是奇数。同时如果有 2N+1 台 JournalNode，那么根据过半写的原则，最多可以容忍有 N 台 JournalNode 节点挂掉。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202502192308642.png"></p>
<p>:::</p>
<h3 id="【高级】NameNode-如何实现主备切换？"><a href="#【高级】NameNode-如何实现主备切换？" class="headerlink" title="【高级】NameNode 如何实现主备切换？"></a>【高级】NameNode 如何实现主备切换？</h3><p>:::details 要点</p>
<p>NameNode 实现主备切换的流程下图所示：</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202502192308888.png"></p>
<p>工作流程说明：</p>
<ol>
<li>HealthMonitor 初始化完成之后会启动内部的线程来定时调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法，对 NameNode 的健康状态进行检测。</li>
<li>HealthMonitor 如果检测到 NameNode 的健康状态发生变化，会回调 ZKFailoverController 注册的相应方法进行处理。</li>
<li>如果 ZKFailoverController 判断需要进行主备切换，会首先使用 ActiveStandbyElector 来进行自动的主备选举。</li>
<li>ActiveStandbyElector 与 Zookeeper 进行交互完成自动的主备选举。</li>
<li>ActiveStandbyElector 在主备选举完成后，会回调 ZKFailoverController 的相应方法来通知当前的 NameNode 成为主 NameNode 或备 NameNode。</li>
<li>ZKFailoverController 调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法将 NameNode 转换为 Active 状态或 Standby 状态。</li>
</ol>
<p>主备选举过程：</p>
<p>NameNode 在选举成功后，会在 zk 上创建了一个 <code>/hadoop-ha/$&#123;dfs.nameservices&#125;/ActiveStandbyElectorLock</code> 节点，而没有选举成功的备 NameNode 会监控这个节点，通过 Watcher 来监听这个节点的状态变化事件，ZKFC 的 ActiveStandbyElector 主要关注这个节点的 NodeDeleted 事件（这部分实现跟 Kafka 中 Controller 的选举一样）。</p>
<p>如果 Active NameNode 对应的 HealthMonitor 检测到 NameNode 的状态异常时， ZKFailoverController 会主动删除当前在 Zookeeper 上建立的临时节点 <code>/hadoop-ha/$&#123;dfs.nameservices&#125;/ActiveStandbyElectorLock</code>，这样处于 Standby 状态的 NameNode 的 ActiveStandbyElector 注册的监听器就会收到这个节点的 NodeDeleted 事件。收到这个事件之后，会马上再次进入到创建 <code>/hadoop-ha/$&#123;dfs.nameservices&#125;/ActiveStandbyElectorLock</code> 节点的流程，如果创建成功，这个本来处于 Standby 状态的 NameNode 就选举为主 NameNode 并随后开始切换为 Active 状态。</p>
<p>当然，如果是 Active 状态的 NameNode 所在的机器整个宕掉的话，那么根据 Zookeeper 的临时节点特性，<code>/hadoop-ha/$&#123;dfs.nameservices&#125;/ActiveStandbyElectorLock</code> 节点会自动被删除，从而也会自动进行一次主备切换。</p>
<p>:::</p>
<h3 id="【高级】如何应对-HDFS-脑裂问题？"><a href="#【高级】如何应对-HDFS-脑裂问题？" class="headerlink" title="【高级】如何应对 HDFS 脑裂问题？"></a>【高级】如何应对 HDFS 脑裂问题？</h3><p>:::details 要点</p>
<p>在实际中，NameNode 可能会出现这种情况，NameNode 在垃圾回收（GC）时，可能会在长时间内整个系统无响应，因此，也就无法向 zk 写入心跳信息，这样的话可能会导致临时节点掉线，备 NameNode 会切换到 Active 状态，这种情况，可能会导致整个集群会有同时有两个 NameNode，这就是脑裂问题。</p>
<p>脑裂问题的解决方案是隔离（Fencing），主要是在以下三处采用隔离措施：</p>
<ul>
<li>第三方共享存储：任一时刻，只有一个 NN 可以写入；</li>
<li>DataNode：需要保证只有一个 NN 发出与管理数据副本有关的删除命令；</li>
<li>Client：需要保证同一时刻只有一个 NN 能够对 Client 的请求发出正确的响应。</li>
</ul>
<p>关于这个问题目前解决方案的实现如下：</p>
<ul>
<li>ActiveStandbyElector 为了实现隔离，会在成功创建 Zookeeper 节点 <code>hadoop-ha/$&#123;dfs.nameservices&#125;/ActiveStandbyElectorLock</code> 从而成为 Active NameNode 之后，创建另外一个路径为 <code>/hadoop-ha/$&#123;dfs.nameservices&#125;/ActiveBreadCrumb</code> 的持久节点，这个节点里面保存了这个 Active NameNode 的地址信息；</li>
<li>Active NameNode 的 ActiveStandbyElector 在正常的状态下关闭 Zookeeper Session 的时候，会一起删除这个持久节点；</li>
<li>但如果 ActiveStandbyElector 在异常的状态下 Zookeeper Session 关闭 （比如前述的 Zookeeper 假死），那么由于 <code>/hadoop-ha/$&#123;dfs.nameservices&#125;/ActiveBreadCrumb</code> 是持久节点，会一直保留下来，后面当另一个 NameNode 选主成功之后，会注意到上一个 Active NameNode 遗留下来的这个节点，从而会回调 ZKFailoverController 的方法对旧的 Active NameNode 进行 fencing。</li>
</ul>
<p>在进行隔离的时候，会执行以下的操作：</p>
<p>首先尝试调用这个旧 Active NameNode 的 HAServiceProtocol RPC 接口的 transitionToStandby 方法，看能不能把它转换为 Standby 状态； 如果 transitionToStandby 方法调用失败，那么就执行 Hadoop 配置文件之中预定义的隔离措施。</p>
<p>Hadoop 目前主要提供两种隔离措施，通常会选择第一种：sshfence：通过 SSH 登录到目标机器上，执行命令 fuser 将对应的进程杀死； shellfence：执行一个用户自定义的 shell 脚本来将对应的进程隔离。 只有在成功地执行完成 fencing 之后，选主成功的 ActiveStandbyElector 才会回调 ZKFailoverController 的 becomeActive 方法将对应的 NameNode 转换为 Active 状态，开始对外提供服务。</p>
<p>NameNode 选举的实现机制与 Kafka 的 Controller 类似，那么 Kafka 是如何避免脑裂问题的呢？</p>
<p>Controller 给 Broker 发送的请求中，都会携带 controller epoch 信息，如果 broker 发现当前请求的 epoch 小于缓存中的值，那么就证明这是来自旧 Controller 的请求，就会决绝这个请求，正常情况下是没什么问题的； 但是异常情况下呢？如果 Broker 先收到异常 Controller 的请求进行处理呢？现在看 Kafka 在这一部分并没有适合的方案； 正常情况下，Kafka 新的 Controller 选举出来之后，Controller 会向全局所有 broker 发送一个 metadata 请求，这样全局所有 Broker 都可以知道当前最新的 controller epoch，但是并不能保证可以完全避免上面这个问题，还是有出现这个问题的几率的，只不过非常小，而且即使出现了由于 Kafka 的高可靠架构，影响也非常有限，至少从目前看，这个问题并不是严重的问题。</p>
<p>通过标识每次选举的版本号，并以最新版本选举结果为准，是分布式选举避免脑裂的常见做法。在其他分布式系统中，epoch 可能会被称为 term、version 等。</p>
<p>:::</p>
<h3 id="【高级】YARN-如何实现高可用？"><a href="#【高级】YARN-如何实现高可用？" class="headerlink" title="【高级】YARN 如何实现高可用？"></a>【高级】YARN 如何实现高可用？</h3><p>:::details 要点</p>
<p>YARN ResourceManager 的高可用与 HDFS NameNode 的高可用类似，但是 ResourceManager 不像 NameNode ，没有那么多的元数据信息需要维护，所以它的状态信息可以直接写到 Zookeeper 上，并依赖 Zookeeper 来进行主备选举。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202502192309573.png"></p>
<p>:::</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/Hadoop-HDFS.md">https://github.com/heibaiying/BigData-Notes/blob/master/notes/Hadoop-HDFS.md</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/hudiefenmu/article/details/37655491">翻译经典 HDFS 原理讲解漫画</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/b903eb35/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/b903eb35/" class="post-title-link" itemprop="url">Hive 常用 DDL 操作</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-02-24 21:14:47" itemprop="dateCreated datePublished" datetime="2020-02-24T21:14:47+08:00">2020-02-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hive/" itemprop="url" rel="index"><span itemprop="name">hive</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Hive-常用-DDL-操作"><a href="#Hive-常用-DDL-操作" class="headerlink" title="Hive 常用 DDL 操作"></a>Hive 常用 DDL 操作</h1><h2 id="Database"><a href="#Database" class="headerlink" title="Database"></a>Database</h2><h3 id="查看数据列表"><a href="#查看数据列表" class="headerlink" title="查看数据列表"></a>查看数据列表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> databases;</span><br></pre></td></tr></table></figure>

<h3 id="使用数据库"><a href="#使用数据库" class="headerlink" title="使用数据库"></a>使用数据库</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">USE database_name;</span><br></pre></td></tr></table></figure>

<h3 id="新建数据库"><a href="#新建数据库" class="headerlink" title="新建数据库"></a>新建数据库</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> (DATABASE<span class="operator">|</span>SCHEMA) [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] database_name   <span class="comment">--DATABASE|SCHEMA 是等价的</span></span><br><span class="line">  [COMMENT database_comment] <span class="comment">--数据库注释</span></span><br><span class="line">  [LOCATION hdfs_path] <span class="comment">--存储在 HDFS 上的位置</span></span><br><span class="line">  [<span class="keyword">WITH</span> DBPROPERTIES (property_name<span class="operator">=</span>property_value, ...)]; <span class="comment">--指定额外属性</span></span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> DATABASE IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> hive_test</span><br><span class="line">  COMMENT <span class="string">&#x27;hive database for test&#x27;</span></span><br><span class="line">  <span class="keyword">WITH</span> DBPROPERTIES (<span class="string">&#x27;create&#x27;</span><span class="operator">=</span><span class="string">&#x27;heibaiying&#x27;</span>);</span><br></pre></td></tr></table></figure>

<h3 id="查看数据库信息"><a href="#查看数据库信息" class="headerlink" title="查看数据库信息"></a>查看数据库信息</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DESC</span> DATABASE [EXTENDED] db_name; <span class="comment">--EXTENDED 表示是否显示额外属性</span></span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DESC</span> DATABASE  EXTENDED hive_test;</span><br></pre></td></tr></table></figure>

<h3 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> (DATABASE<span class="operator">|</span>SCHEMA) [IF <span class="keyword">EXISTS</span>] database_name [RESTRICT<span class="operator">|</span>CASCADE];</span><br></pre></td></tr></table></figure>

<ul>
<li>默认行为是 <code>RESTRICT</code>，如果数据库中存在表则删除失败。</li>
<li>要想删除库及其中的表，可以使用 <code>CASCADE</code> 级联删除。</li>
</ul>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> DATABASE IF <span class="keyword">EXISTS</span> hive_test CASCADE;</span><br></pre></td></tr></table></figure>

<h2 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h2><h3 id="建表语法"><a href="#建表语法" class="headerlink" title="建表语法"></a>建表语法</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [TEMPORARY] [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name     <span class="comment">--表名</span></span><br><span class="line">  [(col_name data_type [COMMENT col_comment],</span><br><span class="line">    ... [constraint_specification])]  <span class="comment">--列名 列数据类型</span></span><br><span class="line">  [COMMENT table_comment]   <span class="comment">--表描述</span></span><br><span class="line">  [PARTITIONED <span class="keyword">BY</span> (col_name data_type [COMMENT col_comment], ...)]  <span class="comment">--分区表分区规则</span></span><br><span class="line">  [</span><br><span class="line">    CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...)</span><br><span class="line">   [SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span><span class="operator">|</span><span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS</span><br><span class="line">  ]  <span class="comment">--分桶表分桶规则</span></span><br><span class="line">  [SKEWED <span class="keyword">BY</span> (col_name, col_name, ...) <span class="keyword">ON</span> ((col_value, col_value, ...), (col_value, col_value, ...), ...)</span><br><span class="line">   [STORED <span class="keyword">AS</span> DIRECTORIES]</span><br><span class="line">  ]  <span class="comment">--指定倾斜列和值</span></span><br><span class="line">  [</span><br><span class="line">   [<span class="type">ROW</span> FORMAT row_format]</span><br><span class="line">   [STORED <span class="keyword">AS</span> file_format]</span><br><span class="line">     <span class="operator">|</span> STORED <span class="keyword">BY</span> <span class="string">&#x27;storage.handler.class.name&#x27;</span> [<span class="keyword">WITH</span> SERDEPROPERTIES (...)]</span><br><span class="line">  ]  <span class="comment">-- 指定行分隔符、存储文件格式或采用自定义存储格式</span></span><br><span class="line">  [LOCATION hdfs_path]  <span class="comment">-- 指定表的存储位置</span></span><br><span class="line">  [TBLPROPERTIES (property_name<span class="operator">=</span>property_value, ...)]  <span class="comment">--指定表的属性</span></span><br><span class="line">  [<span class="keyword">AS</span> select_statement];   <span class="comment">--从查询结果创建表</span></span><br></pre></td></tr></table></figure>

<h3 id="内部表"><a href="#内部表" class="headerlink" title="内部表"></a>内部表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> emp(</span><br><span class="line">  empno <span class="type">INT</span>,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr <span class="type">INT</span>,</span><br><span class="line">  hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  deptno <span class="type">INT</span>)</span><br><span class="line">  <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br></pre></td></tr></table></figure>

<h3 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_external(</span><br><span class="line">  empno <span class="type">INT</span>,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr <span class="type">INT</span>,</span><br><span class="line">  hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  deptno <span class="type">INT</span>)</span><br><span class="line">  <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;</span><br><span class="line">  LOCATION <span class="string">&#x27;/hive/emp_external&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>使用 <code>desc format emp_external</code> 命令可以查看表的详细信息如下：</p>
<h3 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_partition(</span><br><span class="line">  empno <span class="type">INT</span>,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr <span class="type">INT</span>,</span><br><span class="line">  hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>)</span><br><span class="line">  )</span><br><span class="line">  PARTITIONED <span class="keyword">BY</span> (deptno <span class="type">INT</span>)   <span class="comment">-- 按照部门编号进行分区</span></span><br><span class="line">  <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;</span><br><span class="line">  LOCATION <span class="string">&#x27;/hive/emp_partition&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h3 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_bucket(</span><br><span class="line">  empno <span class="type">INT</span>,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr <span class="type">INT</span>,</span><br><span class="line">  hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  deptno <span class="type">INT</span>)</span><br><span class="line">  CLUSTERED <span class="keyword">BY</span>(empno) SORTED <span class="keyword">BY</span>(empno <span class="keyword">ASC</span>) <span class="keyword">INTO</span> <span class="number">4</span> BUCKETS  <span class="comment">--按照员工编号散列到四个 bucket 中</span></span><br><span class="line">  <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;</span><br><span class="line">  LOCATION <span class="string">&#x27;/hive/emp_bucket&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h3 id="倾斜表"><a href="#倾斜表" class="headerlink" title="倾斜表"></a>倾斜表</h3><p>通过指定一个或者多个列经常出现的值（严重偏斜），Hive 会自动将涉及到这些值的数据拆分为单独的文件。在查询时，如果涉及到倾斜值，它就直接从独立文件中获取数据，而不是扫描所有文件，这使得性能得到提升。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_skewed(</span><br><span class="line">  empno <span class="type">INT</span>,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr <span class="type">INT</span>,</span><br><span class="line">  hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>)</span><br><span class="line">  )</span><br><span class="line">  SKEWED <span class="keyword">BY</span> (empno) <span class="keyword">ON</span> (<span class="number">66</span>,<span class="number">88</span>,<span class="number">100</span>)  <span class="comment">--指定 empno 的倾斜值 66,88,100</span></span><br><span class="line">  <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;</span><br><span class="line">  LOCATION <span class="string">&#x27;/hive/emp_skewed&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h3 id="临时表"><a href="#临时表" class="headerlink" title="临时表"></a>临时表</h3><p>临时表仅对当前 session 可见，临时表的数据将存储在用户的暂存目录中，并在会话结束后删除。如果临时表与永久表表名相同，则对该表名的任何引用都将解析为临时表，而不是永久表。临时表还具有以下两个限制：</p>
<ul>
<li>不支持分区列；</li>
<li>不支持创建索引。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">TABLE</span> emp_temp(</span><br><span class="line">  empno <span class="type">INT</span>,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr <span class="type">INT</span>,</span><br><span class="line">  hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>)</span><br><span class="line">  )</span><br><span class="line">  <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br></pre></td></tr></table></figure>

<h3 id="CTAS-创建表"><a href="#CTAS-创建表" class="headerlink" title="CTAS 创建表"></a>CTAS 创建表</h3><p>支持从查询语句的结果创建表：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> emp_copy <span class="keyword">AS</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno<span class="operator">=</span><span class="string">&#x27;20&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h3 id="复制表结构"><a href="#复制表结构" class="headerlink" title="复制表结构"></a>复制表结构</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [TEMPORARY] [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name  <span class="comment">--创建表表名</span></span><br><span class="line">   <span class="keyword">LIKE</span> existing_table_or_view_name  <span class="comment">--被复制表的表名</span></span><br><span class="line">   [LOCATION hdfs_path]; <span class="comment">--存储位置</span></span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span>  IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>  emp_co  <span class="keyword">LIKE</span> emp</span><br></pre></td></tr></table></figure>

<h3 id="加载数据到表"><a href="#加载数据到表" class="headerlink" title="加载数据到表"></a>加载数据到表</h3><p>加载数据到表中属于 DML 操作，这里为了方便大家测试，先简单介绍一下加载本地数据到表中：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 加载数据到 emp 表中</span></span><br><span class="line">load data <span class="keyword">local</span> inpath &quot;/usr/file/emp.txt&quot; <span class="keyword">into</span> <span class="keyword">table</span> emp;</span><br></pre></td></tr></table></figure>

<p>其中 emp.txt 的内容如下，你可以直接复制使用，也可以到本仓库的<a target="_blank" rel="noopener" href="https://github.com/heibaiying/BigData-Notes/tree/master/resources">resources</a> 目录下载：</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">7369	SMITH	CLERK	7902	1980-12-17 00:00:00	800.00		20</span><br><span class="line">7499	ALLEN	SALESMAN	7698	1981-02-20 00:00:00	1600.00	300.00	30</span><br><span class="line">7521	WARD	SALESMAN	7698	1981-02-22 00:00:00	1250.00	500.00	30</span><br><span class="line">7566	JONES	MANAGER	7839	1981-04-02 00:00:00	2975.00		20</span><br><span class="line">7654	MARTIN	SALESMAN	7698	1981-09-28 00:00:00	1250.00	1400.00	30</span><br><span class="line">7698	BLAKE	MANAGER	7839	1981-05-01 00:00:00	2850.00		30</span><br><span class="line">7782	CLARK	MANAGER	7839	1981-06-09 00:00:00	2450.00		10</span><br><span class="line">7788	SCOTT	ANALYST	7566	1987-04-19 00:00:00	1500.00		20</span><br><span class="line">7839	KING	PRESIDENT		1981-11-17 00:00:00	5000.00		10</span><br><span class="line">7844	TURNER	SALESMAN	7698	1981-09-08 00:00:00	1500.00	0.00	30</span><br><span class="line">7876	ADAMS	CLERK	7788	1987-05-23 00:00:00	1100.00		20</span><br><span class="line">7900	JAMES	CLERK	7698	1981-12-03 00:00:00	950.00		30</span><br><span class="line">7902	FORD	ANALYST	7566	1981-12-03 00:00:00	3000.00		20</span><br><span class="line">7934	MILLER	CLERK	7782	1982-01-23 00:00:00	1300.00		10</span><br></pre></td></tr></table></figure>

<p>加载后可查询表中数据</p>
<h2 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h2><h3 id="重命名表"><a href="#重命名表" class="headerlink" title="重命名表"></a>重命名表</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER TABLE</span> table_name RENAME <span class="keyword">TO</span> new_table_name;</span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER TABLE</span> emp_temp RENAME <span class="keyword">TO</span> new_emp; <span class="comment">--把 emp_temp 表重命名为 new_emp</span></span><br></pre></td></tr></table></figure>

<h3 id="修改列"><a href="#修改列" class="headerlink" title="修改列"></a>修改列</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER TABLE</span> table_name [<span class="keyword">PARTITION</span> partition_spec] CHANGE [<span class="keyword">COLUMN</span>] col_old_name col_new_name column_type</span><br><span class="line">  [COMMENT col_comment] [<span class="keyword">FIRST</span><span class="operator">|</span>AFTER column_name] [CASCADE<span class="operator">|</span>RESTRICT];</span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 修改字段名和类型</span></span><br><span class="line"><span class="keyword">ALTER TABLE</span> emp_temp CHANGE empno empno_new <span class="type">INT</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 修改字段 sal 的名称 并将其放置到 empno 字段后</span></span><br><span class="line"><span class="keyword">ALTER TABLE</span> emp_temp CHANGE sal sal_new <span class="type">decimal</span>(<span class="number">7</span>,<span class="number">2</span>)  AFTER ename;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 为字段增加注释</span></span><br><span class="line"><span class="keyword">ALTER TABLE</span> emp_temp CHANGE mgr mgr_new <span class="type">INT</span> COMMENT <span class="string">&#x27;this is column mgr&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h3 id="新增列"><a href="#新增列" class="headerlink" title="新增列"></a>新增列</h3><p>示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER TABLE</span> emp_temp <span class="keyword">ADD</span> COLUMNS (address STRING COMMENT <span class="string">&#x27;home address&#x27;</span>);</span><br></pre></td></tr></table></figure>

<h2 id="清空表-删除表"><a href="#清空表-删除表" class="headerlink" title="清空表&#x2F;删除表"></a>清空表&#x2F;删除表</h2><h3 id="清空表-hive-ddl-md"><a href="#清空表-hive-ddl-md" class="headerlink" title="清空表 hive-ddl.md"></a>清空表 hive-ddl.md</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 清空整个表或表指定分区中的数据</span></span><br><span class="line"><span class="keyword">TRUNCATE</span> <span class="keyword">TABLE</span> table_name [<span class="keyword">PARTITION</span> (partition_column <span class="operator">=</span> partition_col_value,  ...)];</span><br></pre></td></tr></table></figure>

<ul>
<li>目前只有内部表才能执行 TRUNCATE 操作，外部表执行时会抛出异常 <code>Cannot truncate non-managed table XXXX</code>。</li>
</ul>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">TRUNCATE</span> <span class="keyword">TABLE</span> emp_mgt_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">20</span>);</span><br></pre></td></tr></table></figure>

<h3 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> [IF <span class="keyword">EXISTS</span>] table_name [PURGE];</span><br></pre></td></tr></table></figure>

<ul>
<li>内部表：不仅会删除表的元数据，同时会删除 HDFS 上的数据；</li>
<li>外部表：只会删除表的元数据，不会删除 HDFS 上的数据；</li>
<li>删除视图引用的表时，不会给出警告（但视图已经无效了，必须由用户删除或重新创建）。</li>
</ul>
<h2 id="其他命令"><a href="#其他命令" class="headerlink" title="其他命令"></a>其他命令</h2><h3 id="Describe"><a href="#Describe" class="headerlink" title="Describe"></a>Describe</h3><p>查看数据库：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DESCRIBE</span><span class="operator">|</span><span class="keyword">Desc</span> DATABASE [EXTENDED] db_name;  <span class="comment">--EXTENDED 是否显示额外属性</span></span><br></pre></td></tr></table></figure>

<p>查看表：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DESCRIBE</span><span class="operator">|</span><span class="keyword">Desc</span> [EXTENDED<span class="operator">|</span>FORMATTED] table_name <span class="comment">--FORMATTED 以友好的展现方式查看表详情</span></span><br></pre></td></tr></table></figure>

<h3 id="Show"><a href="#Show" class="headerlink" title="Show"></a>Show</h3><p><strong>1. 查看数据库列表</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 语法</span></span><br><span class="line"><span class="keyword">SHOW</span> (DATABASES<span class="operator">|</span>SCHEMAS) [<span class="keyword">LIKE</span> <span class="string">&#x27;identifier_with_wildcards&#x27;</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 示例：</span></span><br><span class="line"><span class="keyword">SHOW</span> DATABASES <span class="keyword">like</span> <span class="string">&#x27;hive*&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>LIKE 子句允许使用正则表达式进行过滤，但是 SHOW 语句当中的 LIKE 子句只支持 <code>*</code>（通配符）和 <code>|</code>（条件或）两个符号。例如 <code>employees</code>，<code>emp *</code>，<code>emp * | * ees</code>，所有这些都将匹配名为 <code>employees</code> 的数据库。</p>
<p><strong>2. 查看表的列表</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 语法</span></span><br><span class="line"><span class="keyword">SHOW</span> TABLES [<span class="keyword">IN</span> database_name] [<span class="string">&#x27;identifier_with_wildcards&#x27;</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 示例</span></span><br><span class="line"><span class="keyword">SHOW</span> TABLES <span class="keyword">IN</span> <span class="keyword">default</span>;</span><br></pre></td></tr></table></figure>

<p><strong>3. 查看视图列表</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> VIEWS [<span class="keyword">IN</span><span class="operator">/</span><span class="keyword">FROM</span> database_name] [<span class="keyword">LIKE</span> <span class="string">&#x27;pattern_with_wildcards&#x27;</span>];   <span class="comment">--仅支持 Hive 2.2.0</span></span><br></pre></td></tr></table></figure>

<p><strong>4. 查看表的分区列表</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> PARTITIONS table_name;</span><br></pre></td></tr></table></figure>

<p><strong>5. 查看表&#x2F;视图的创建语句</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> <span class="keyword">CREATE TABLE</span> ([db_name.]table_name<span class="operator">|</span>view_name);</span><br></pre></td></tr></table></figure>

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL">LanguageManual DDL</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/f03b2e49/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/f03b2e49/" class="post-title-link" itemprop="url">Hive 常用 DML 操作</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-02-24 21:14:47" itemprop="dateCreated datePublished" datetime="2020-02-24T21:14:47+08:00">2020-02-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hive/" itemprop="url" rel="index"><span itemprop="name">hive</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Hive-常用-DML-操作"><a href="#Hive-常用-DML-操作" class="headerlink" title="Hive 常用 DML 操作"></a>Hive 常用 DML 操作</h1><h2 id="加载文件数据到表"><a href="#加载文件数据到表" class="headerlink" title="加载文件数据到表"></a>加载文件数据到表</h2><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA [LOCAL] INPATH &#x27;filepath&#x27; [OVERWRITE]</span><br><span class="line">INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]</span><br></pre></td></tr></table></figure>

<ul>
<li><code>LOCAL</code> 关键字代表从本地文件系统加载文件，省略则代表从 HDFS 上加载文件：</li>
<li>从本地文件系统加载文件时， <code>filepath</code> 可以是绝对路径也可以是相对路径 (建议使用绝对路径)；</li>
<li>从 HDFS 加载文件时候，<code>filepath</code> 为文件完整的 URL 地址：如 <code>hdfs://namenode:port/user/hive/project/ data1</code></li>
<li><code>filepath</code> 可以是文件路径 (在这种情况下 Hive 会将文件移动到表中)，也可以目录路径 (在这种情况下，Hive 会将该目录中的所有文件移动到表中)；</li>
<li>如果使用 <code>OVERWRITE</code> 关键字，则将删除目标表（或分区）的内容，使用新的数据填充；不使用此关键字，则数据以追加的方式加入；</li>
<li>加载的目标可以是表或分区。如果是分区表，则必须指定加载数据的分区；</li>
<li>加载文件的格式必须与建表时使用 <code>STORED AS</code> 指定的存储格式相同。</li>
</ul>
<blockquote>
<p>使用建议：</p>
<p><strong>不论是本地路径还是 URL 都建议使用完整的</strong>。虽然可以使用不完整的 URL 地址，此时 Hive 将使用 hadoop 中的 fs.default.name 配置来推断地址，但是为避免不必要的错误，建议使用完整的本地路径或 URL 地址；</p>
<p><strong>加载对象是分区表时建议显示指定分区</strong>。在 Hive 3.0 之后，内部将加载 (LOAD) 重写为 INSERT AS SELECT，此时如果不指定分区，INSERT AS SELECT 将假设最后一组列是分区列，如果该列不是表定义的分区，它将抛出错误。为避免错误，还是建议显示指定分区。</p>
</blockquote>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>新建分区表：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> emp_ptn(</span><br><span class="line">  empno <span class="type">INT</span>,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr <span class="type">INT</span>,</span><br><span class="line">  hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>)</span><br><span class="line">  )</span><br><span class="line">  PARTITIONED <span class="keyword">BY</span> (deptno <span class="type">INT</span>)   <span class="comment">-- 按照部门编号进行分区</span></span><br><span class="line">  <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br></pre></td></tr></table></figure>

<p>从 HDFS 上加载数据到分区表：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA  INPATH &quot;hdfs://hadoop001:8020/mydir/emp.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">20</span>);</span><br></pre></td></tr></table></figure>

<blockquote>
<p>emp.txt 文件可在本仓库的 resources 目录中下载</p>
</blockquote>
<p>加载后表中数据如下,分区列 deptno 全部赋值成 20：</p>
<h2 id="查询结果插入到表"><a href="#查询结果插入到表" class="headerlink" title="查询结果插入到表"></a>查询结果插入到表</h2><h3 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename1 [<span class="keyword">PARTITION</span> (partcol1<span class="operator">=</span>val1, partcol2<span class="operator">=</span>val2 ...) [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>]]</span><br><span class="line">select_statement1 <span class="keyword">FROM</span> from_statement;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT INTO</span> <span class="keyword">TABLE</span> tablename1 [<span class="keyword">PARTITION</span> (partcol1<span class="operator">=</span>val1, partcol2<span class="operator">=</span>val2 ...)]</span><br><span class="line">select_statement1 <span class="keyword">FROM</span> from_statement;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>Hive 0.13.0 开始，建表时可以通过使用 TBLPROPERTIES（“immutable”&#x3D;“true”）来创建不可变表 (immutable table) ，如果不可以变表中存在数据，则 INSERT INTO 失败。（注：INSERT OVERWRITE 的语句不受 <code>immutable</code> 属性的影响）;</p>
</li>
<li><p>可以对表或分区执行插入操作。如果表已分区，则必须通过指定所有分区列的值来指定表的特定分区；</p>
</li>
<li><p>从 Hive 1.1.0 开始，TABLE 关键字是可选的；</p>
</li>
<li><p>从 Hive 1.2.0 开始 ，可以采用 <code>INSERT INTO tablename(z，x，c1)</code> 指明插入列；</p>
</li>
<li><p>可以将 SELECT 语句的查询结果插入多个表（或分区），称为多表插入。语法如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> from_statement</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename1</span><br><span class="line">[<span class="keyword">PARTITION</span> (partcol1<span class="operator">=</span>val1, partcol2<span class="operator">=</span>val2 ...) [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>]] select_statement1</span><br><span class="line">[<span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename2 [<span class="keyword">PARTITION</span> ... [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>]] select_statement2]</span><br><span class="line">[<span class="keyword">INSERT INTO</span> <span class="keyword">TABLE</span> tablename2 [<span class="keyword">PARTITION</span> ...] select_statement2] ...;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="动态插入分区"><a href="#动态插入分区" class="headerlink" title="动态插入分区"></a>动态插入分区</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename <span class="keyword">PARTITION</span> (partcol1[<span class="operator">=</span>val1], partcol2[<span class="operator">=</span>val2] ...)</span><br><span class="line">select_statement <span class="keyword">FROM</span> from_statement;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT INTO</span> <span class="keyword">TABLE</span> tablename <span class="keyword">PARTITION</span> (partcol1[<span class="operator">=</span>val1], partcol2[<span class="operator">=</span>val2] ...)</span><br><span class="line">select_statement <span class="keyword">FROM</span> from_statement;</span><br></pre></td></tr></table></figure>

<p>在向分区表插入数据时候，分区列名是必须的，但是列值是可选的。如果给出了分区列值，我们将其称为静态分区，否则它是动态分区。动态分区列必须在 <code>SELECT</code> 语句的列中最后指定，并且与它们在 PARTITION() 子句中出现的顺序相同。</p>
<p>注意：Hive 0.9.0 之前的版本动态分区插入是默认禁用的，而 0.9.0 之后的版本则默认启用。以下是动态分区的相关配置：</p>
<table>
<thead>
<tr>
<th>配置</th>
<th>默认值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>hive.exec.dynamic.partition</code></td>
<td><code>true</code></td>
<td>需要设置为 true 才能启用动态分区插入</td>
</tr>
<tr>
<td><code>hive.exec.dynamic.partition.mode</code></td>
<td><code>strict</code></td>
<td>在严格模式 (strict) 下，用户必须至少指定一个静态分区，以防用户意外覆盖所有分区，在非严格模式下，允许所有分区都是动态的</td>
</tr>
<tr>
<td><code>hive.exec.max.dynamic.partitions.pernode</code></td>
<td>100</td>
<td>允许在每个 mapper&#x2F;reducer 节点中创建的最大动态分区数</td>
</tr>
<tr>
<td><code>hive.exec.max.dynamic.partitions</code></td>
<td>1000</td>
<td>允许总共创建的最大动态分区数</td>
</tr>
<tr>
<td><code>hive.exec.max.created.files</code></td>
<td>100000</td>
<td>作业中所有 mapper&#x2F;reducer 创建的 HDFS 文件的最大数量</td>
</tr>
<tr>
<td><code>hive.error.on.empty.partition</code></td>
<td><code>false</code></td>
<td>如果动态分区插入生成空结果，是否抛出异常</td>
</tr>
</tbody></table>
<h3 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h3><p>（1）新建 emp 表，作为查询对象表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> emp(</span><br><span class="line">    empno <span class="type">INT</span>,</span><br><span class="line">    ename STRING,</span><br><span class="line">    job STRING,</span><br><span class="line">    mgr <span class="type">INT</span>,</span><br><span class="line">    hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">    sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">    comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">    deptno <span class="type">INT</span>)</span><br><span class="line">    <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line"> <span class="comment">-- 加载数据到 emp 表中 这里直接从本地加载</span></span><br><span class="line">load data <span class="keyword">local</span> inpath &quot;/usr/file/emp.txt&quot; <span class="keyword">into</span> <span class="keyword">table</span> emp;</span><br></pre></td></tr></table></figure>

<p>完成后 <code>emp</code> 表中数据如下：</p>
<p>（2）为清晰演示，先清空 <code>emp_ptn</code> 表中加载的数据：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">TRUNCATE</span> <span class="keyword">TABLE</span> emp_ptn;</span><br></pre></td></tr></table></figure>

<p>（3）静态分区演示：从 <code>emp</code> 表中查询部门编号为 20 的员工数据，并插入 <code>emp_ptn</code> 表中，语句如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">20</span>)</span><br><span class="line"><span class="keyword">SELECT</span> empno,ename,job,mgr,hiredate,sal,comm <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno<span class="operator">=</span><span class="number">20</span>;</span><br></pre></td></tr></table></figure>

<p>完成后 <code>emp_ptn</code> 表中数据如下：</p>
<p>（4）接着演示动态分区：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 由于我们只有一个分区，且还是动态分区，所以需要关闭严格默认。因为在严格模式下，用户必须至少指定一个静态分区</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode<span class="operator">=</span>nonstrict;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 动态分区   此时查询语句的最后一列为动态分区列，即 deptno</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno)</span><br><span class="line"><span class="keyword">SELECT</span> empno,ename,job,mgr,hiredate,sal,comm,deptno <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno<span class="operator">=</span><span class="number">30</span>;</span><br></pre></td></tr></table></figure>

<p>完成后 <code>emp_ptn</code> 表中数据如下：</p>
<h2 id="使用-SQL-语句插入值"><a href="#使用-SQL-语句插入值" class="headerlink" title="使用 SQL 语句插入值"></a>使用 SQL 语句插入值</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT INTO</span> <span class="keyword">TABLE</span> tablename [<span class="keyword">PARTITION</span> (partcol1[<span class="operator">=</span>val1], partcol2[<span class="operator">=</span>val2] ...)]</span><br><span class="line"><span class="keyword">VALUES</span> ( <span class="keyword">value</span> [, <span class="keyword">value</span> ...] )</span><br></pre></td></tr></table></figure>

<ul>
<li>使用时必须为表中的每个列都提供值。不支持只向部分列插入值（可以为缺省值的列提供空值来消除这个弊端）；</li>
<li>如果目标表表支持 ACID 及其事务管理器，则插入后自动提交；</li>
<li>不支持支持复杂类型 (array, map, struct, union) 的插入。</li>
</ul>
<h2 id="更新和删除数据"><a href="#更新和删除数据" class="headerlink" title="更新和删除数据"></a>更新和删除数据</h2><h3 id="语法-2"><a href="#语法-2" class="headerlink" title="语法"></a>语法</h3><p>更新和删除的语法比较简单，和关系型数据库一致。需要注意的是这两个操作都只能在支持 ACID 的表，也就是事务表上才能执行。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 更新</span></span><br><span class="line"><span class="keyword">UPDATE</span> tablename <span class="keyword">SET</span> <span class="keyword">column</span> <span class="operator">=</span> <span class="keyword">value</span> [, <span class="keyword">column</span> <span class="operator">=</span> <span class="keyword">value</span> ...] [<span class="keyword">WHERE</span> expression]</span><br><span class="line"></span><br><span class="line"><span class="comment">--删除</span></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> tablename [<span class="keyword">WHERE</span> expression]</span><br></pre></td></tr></table></figure>

<h3 id="示例-2"><a href="#示例-2" class="headerlink" title="示例"></a>示例</h3><p><strong>1. 修改配置</strong></p>
<p>首先需要更改 <code>hive-site.xml</code>，添加如下配置，开启事务支持，配置完成后需要重启 Hive 服务。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.support.concurrency<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.enforce.bucketing<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.dynamic.partition.mode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nonstrict<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.txn.manager<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.compactor.initiator.on<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.in.test<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>2. 创建测试表</strong></p>
<p>创建用于测试的事务表，建表时候指定属性 <code>transactional = true</code> 则代表该表是事务表。需要注意的是，按照<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions">官方文档</a> 的说明，目前 Hive 中的事务表有以下限制：</p>
<ul>
<li>必须是 buckets Table;</li>
<li>仅支持 ORC 文件格式；</li>
<li>不支持 LOAD DATA …语句。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> emp_ts(</span><br><span class="line">  empno <span class="type">int</span>,</span><br><span class="line">  ename String</span><br><span class="line">)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span> (empno) <span class="keyword">INTO</span> <span class="number">2</span> BUCKETS STORED <span class="keyword">AS</span> ORC</span><br><span class="line">TBLPROPERTIES (&quot;transactional&quot;<span class="operator">=</span>&quot;true&quot;);</span><br></pre></td></tr></table></figure>

<p><strong>3. 插入测试数据</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT INTO</span> <span class="keyword">TABLE</span> emp_ts  <span class="keyword">VALUES</span> (<span class="number">1</span>,&quot;ming&quot;),(<span class="number">2</span>,&quot;hong&quot;);</span><br></pre></td></tr></table></figure>

<p>插入数据依靠的是 MapReduce 作业，执行成功后数据如下：</p>
<p><strong>4. 测试更新和删除</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--更新数据</span></span><br><span class="line"><span class="keyword">UPDATE</span> emp_ts <span class="keyword">SET</span> ename <span class="operator">=</span> &quot;lan&quot;  <span class="keyword">WHERE</span>  empno<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--删除数据</span></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> emp_ts <span class="keyword">WHERE</span> empno<span class="operator">=</span><span class="number">2</span>;</span><br></pre></td></tr></table></figure>

<p>更新和删除数据依靠的也是 MapReduce 作业，执行成功后数据如下：</p>
<h2 id="查询结果写出到文件系统"><a href="#查询结果写出到文件系统" class="headerlink" title="查询结果写出到文件系统"></a>查询结果写出到文件系统</h2><h3 id="语法-3"><a href="#语法-3" class="headerlink" title="语法"></a>语法</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] DIRECTORY directory1</span><br><span class="line">  [<span class="type">ROW</span> FORMAT row_format] [STORED <span class="keyword">AS</span> file_format]</span><br><span class="line">  <span class="keyword">SELECT</span> ... <span class="keyword">FROM</span> ...</span><br></pre></td></tr></table></figure>

<ul>
<li><p>OVERWRITE 关键字表示输出文件存在时，先删除后再重新写入；</p>
</li>
<li><p>和 Load 语句一样，建议无论是本地路径还是 URL 地址都使用完整的；</p>
</li>
<li><p>写入文件系统的数据被序列化为文本，其中列默认由^A 分隔，行由换行符分隔。如果列不是基本类型，则将其序列化为 JSON 格式。其中行分隔符不允许自定义，但列分隔符可以自定义，如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 定义列分隔符为&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;./test-04&#x27;</span></span><br><span class="line"><span class="type">row</span> format delimited</span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">MAP KEYS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;:&#x27;</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> src;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="示例-3"><a href="#示例-3" class="headerlink" title="示例"></a>示例</h3><p>这里我们将上面创建的 <code>emp_ptn</code> 表导出到本地文件系统，语句如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">LOCAL</span> DIRECTORY <span class="string">&#x27;/usr/file/ouput&#x27;</span></span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp_ptn;</span><br></pre></td></tr></table></figure>

<p>导出结果如下：</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions">Hive Transactions</a></li>
<li><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML">Hive Data Manipulation Language</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/5473f9d5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/5473f9d5/" class="post-title-link" itemprop="url">Hive 数据查询详解</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-02-24 21:14:47" itemprop="dateCreated datePublished" datetime="2020-02-24T21:14:47+08:00">2020-02-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hive/" itemprop="url" rel="index"><span itemprop="name">hive</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Hive-数据查询详解"><a href="#Hive-数据查询详解" class="headerlink" title="Hive 数据查询详解"></a>Hive 数据查询详解</h1><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>为了演示查询操作，这里需要预先创建三张表，并加载测试数据。</p>
<blockquote>
<p>数据文件 emp.txt 和 dept.txt 可以从本仓库的<a target="_blank" rel="noopener" href="https://github.com/heibaiying/BigData-Notes/tree/master/resources">resources</a> 目录下载。</p>
</blockquote>
<h3 id="员工表"><a href="#员工表" class="headerlink" title="员工表"></a>员工表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">-- 建表语句</span></span><br><span class="line"> <span class="keyword">CREATE TABLE</span> emp(</span><br><span class="line">     empno <span class="type">INT</span>,     <span class="comment">-- 员工表编号</span></span><br><span class="line">     ename STRING,  <span class="comment">-- 员工姓名</span></span><br><span class="line">     job STRING,    <span class="comment">-- 职位类型</span></span><br><span class="line">     mgr <span class="type">INT</span>,</span><br><span class="line">     hiredate <span class="type">TIMESTAMP</span>,  <span class="comment">--雇佣日期</span></span><br><span class="line">     sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),  <span class="comment">--工资</span></span><br><span class="line">     comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">     deptno <span class="type">INT</span>)   <span class="comment">--部门编号</span></span><br><span class="line">    <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">--加载数据</span></span><br><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp;</span><br></pre></td></tr></table></figure>

<h3 id="部门表"><a href="#部门表" class="headerlink" title="部门表"></a>部门表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表语句</span></span><br><span class="line"><span class="keyword">CREATE TABLE</span> dept(</span><br><span class="line">    deptno <span class="type">INT</span>,   <span class="comment">--部门编号</span></span><br><span class="line">    dname STRING,  <span class="comment">--部门名称</span></span><br><span class="line">    loc STRING    <span class="comment">--部门所在的城市</span></span><br><span class="line">)</span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line"><span class="comment">--加载数据</span></span><br><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH &quot;/usr/file/dept.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> dept;</span><br></pre></td></tr></table></figure>

<h3 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h3><p>这里需要额外创建一张分区表，主要是为了演示分区查询：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_ptn(</span><br><span class="line">      empno <span class="type">INT</span>,</span><br><span class="line">      ename STRING,</span><br><span class="line">      job STRING,</span><br><span class="line">      mgr <span class="type">INT</span>,</span><br><span class="line">      hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">      sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">      comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>)</span><br><span class="line">  )</span><br><span class="line"> PARTITIONED <span class="keyword">BY</span> (deptno <span class="type">INT</span>)   <span class="comment">-- 按照部门编号进行分区</span></span><br><span class="line"> <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">--加载数据</span></span><br><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">20</span>)</span><br><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">30</span>)</span><br><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">40</span>)</span><br><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">50</span>)</span><br></pre></td></tr></table></figure>

<h2 id="单表查询"><a href="#单表查询" class="headerlink" title="单表查询"></a>单表查询</h2><h3 id="SELECT"><a href="#SELECT" class="headerlink" title="SELECT"></a>SELECT</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询表中全部数据</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp;</span><br></pre></td></tr></table></figure>

<h3 id="WHERE"><a href="#WHERE" class="headerlink" title="WHERE"></a>WHERE</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询 10 号部门中员工编号大于 7782 的员工信息</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> empno <span class="operator">&gt;</span> <span class="number">7782</span> <span class="keyword">AND</span> deptno <span class="operator">=</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure>

<h3 id="DISTINCT"><a href="#DISTINCT" class="headerlink" title="DISTINCT"></a>DISTINCT</h3><p>Hive 支持使用 DISTINCT 关键字去重。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询所有工作类型</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> job <span class="keyword">FROM</span> emp;</span><br></pre></td></tr></table></figure>

<h3 id="分区查询"><a href="#分区查询" class="headerlink" title="分区查询"></a>分区查询</h3><p>分区查询 (Partition Based Queries)，可以指定某个分区或者分区范围。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询分区表中部门编号在[20,40]之间的员工</span></span><br><span class="line"><span class="keyword">SELECT</span> emp_ptn.<span class="operator">*</span> <span class="keyword">FROM</span> emp_ptn</span><br><span class="line"><span class="keyword">WHERE</span> emp_ptn.deptno <span class="operator">&gt;=</span> <span class="number">20</span> <span class="keyword">AND</span> emp_ptn.deptno <span class="operator">&lt;=</span> <span class="number">40</span>;</span><br></pre></td></tr></table></figure>

<h3 id="LIMIT"><a href="#LIMIT" class="headerlink" title="LIMIT"></a>LIMIT</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询薪资最高的 5 名员工</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">ORDER</span> <span class="keyword">BY</span> sal <span class="keyword">DESC</span> LIMIT <span class="number">5</span>;</span><br></pre></td></tr></table></figure>

<h3 id="GROUP-BY"><a href="#GROUP-BY" class="headerlink" title="GROUP BY"></a>GROUP BY</h3><p>Hive 支持使用 GROUP BY 进行分组聚合操作。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查询各个部门薪酬综合</span></span><br><span class="line"><span class="keyword">SELECT</span> deptno,<span class="built_in">SUM</span>(sal) <span class="keyword">FROM</span> emp <span class="keyword">GROUP</span> <span class="keyword">BY</span> deptno;</span><br></pre></td></tr></table></figure>

<p><code>hive.map.aggr</code> 控制程序如何进行聚合。默认值为 false。如果设置为 true，Hive 会在 map 阶段就执行一次聚合。这可以提高聚合效率，但需要消耗更多内存。</p>
<h3 id="ORDER-AND-SORT"><a href="#ORDER-AND-SORT" class="headerlink" title="ORDER AND SORT"></a>ORDER AND SORT</h3><p>可以使用 ORDER BY 或者 Sort BY 对查询结果进行排序，排序字段可以是整型也可以是字符串：如果是整型，则按照大小排序；如果是字符串，则按照字典序排序。ORDER BY 和 SORT BY 的区别如下：</p>
<ul>
<li>使用 ORDER BY 时会有一个 Reducer 对全部查询结果进行排序，可以保证数据的全局有序性；</li>
<li>使用 SORT BY 时只会在每个 Reducer 中进行排序，这可以保证每个 Reducer 的输出数据是有序的，但不能保证全局有序。</li>
</ul>
<p>由于 ORDER BY 的时间可能很长，如果你设置了严格模式 (hive.mapred.mode &#x3D; strict)，则其后面必须再跟一个 <code>limit</code> 子句。</p>
<blockquote>
<p>注 ：hive.mapred.mode 默认值是 nonstrict ，也就是非严格模式。</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询员工工资，结果按照部门升序，按照工资降序排列</span></span><br><span class="line"><span class="keyword">SELECT</span> empno, deptno, sal <span class="keyword">FROM</span> emp <span class="keyword">ORDER</span> <span class="keyword">BY</span> deptno <span class="keyword">ASC</span>, sal <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure>

<h3 id="HAVING"><a href="#HAVING" class="headerlink" title="HAVING"></a>HAVING</h3><p>可以使用 HAVING 对分组数据进行过滤。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询工资总和大于 9000 的所有部门</span></span><br><span class="line"><span class="keyword">SELECT</span> deptno,<span class="built_in">SUM</span>(sal) <span class="keyword">FROM</span> emp <span class="keyword">GROUP</span> <span class="keyword">BY</span> deptno <span class="keyword">HAVING</span> <span class="built_in">SUM</span>(sal)<span class="operator">&gt;</span><span class="number">9000</span>;</span><br></pre></td></tr></table></figure>

<h3 id="DISTRIBUTE-BY"><a href="#DISTRIBUTE-BY" class="headerlink" title="DISTRIBUTE BY"></a>DISTRIBUTE BY</h3><p>默认情况下，MapReduce 程序会对 Map 输出结果的 Key 值进行散列，并均匀分发到所有 Reducer 上。如果想要把具有相同 Key 值的数据分发到同一个 Reducer 进行处理，这就需要使用 DISTRIBUTE BY 字句。</p>
<p>需要注意的是，DISTRIBUTE BY 虽然能保证具有相同 Key 值的数据分发到同一个 Reducer，但是不能保证数据在 Reducer 上是有序的。情况如下：</p>
<p>把以下 5 个数据发送到两个 Reducer 上进行处理：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">k1</span></span><br><span class="line"><span class="attr">k2</span></span><br><span class="line"><span class="attr">k4</span></span><br><span class="line"><span class="attr">k3</span></span><br><span class="line"><span class="attr">k1</span></span><br></pre></td></tr></table></figure>

<p>Reducer1 得到如下乱序数据：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">k1</span></span><br><span class="line"><span class="attr">k2</span></span><br><span class="line"><span class="attr">k1</span></span><br></pre></td></tr></table></figure>

<p>Reducer2 得到数据如下：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">k4</span></span><br><span class="line"><span class="attr">k3</span></span><br></pre></td></tr></table></figure>

<p>如果想让 Reducer 上的数据时有序的，可以结合 <code>SORT BY</code> 使用 (示例如下)，或者使用下面我们将要介绍的 CLUSTER BY。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 将数据按照部门分发到对应的 Reducer 上处理</span></span><br><span class="line"><span class="keyword">SELECT</span> empno, deptno, sal <span class="keyword">FROM</span> emp DISTRIBUTE <span class="keyword">BY</span> deptno SORT <span class="keyword">BY</span> deptno <span class="keyword">ASC</span>;</span><br></pre></td></tr></table></figure>

<h3 id="CLUSTER-BY"><a href="#CLUSTER-BY" class="headerlink" title="CLUSTER BY"></a>CLUSTER BY</h3><p>如果 <code>SORT BY</code> 和 <code>DISTRIBUTE BY</code> 指定的是相同字段，且 SORT BY 排序规则是 ASC，此时可以使用 <code>CLUSTER BY</code> 进行替换，同时 <code>CLUSTER BY</code> 可以保证数据在全局是有序的。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> empno, deptno, sal <span class="keyword">FROM</span> emp CLUSTER  <span class="keyword">BY</span> deptno ;</span><br></pre></td></tr></table></figure>

<h2 id="多表联结查询"><a href="#多表联结查询" class="headerlink" title="多表联结查询"></a>多表联结查询</h2><p>Hive 支持内连接，外连接，左外连接，右外连接，笛卡尔连接，这和传统数据库中的概念是一致的，可以参见下图。</p>
<p>需要特别强调：JOIN 语句的关联条件必须用 ON 指定，不能用 WHERE 指定，否则就会先做笛卡尔积，再过滤，这会导致你得不到预期的结果 (下面的演示会有说明)。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20200224195733.png" alt="img"></p>
<h3 id="INNER-JOIN"><a href="#INNER-JOIN" class="headerlink" title="INNER JOIN"></a>INNER JOIN</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询员工编号为 7369 的员工的详细信息</span></span><br><span class="line"><span class="keyword">SELECT</span> e.<span class="operator">*</span>,d.<span class="operator">*</span> <span class="keyword">FROM</span></span><br><span class="line">emp e <span class="keyword">JOIN</span> dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno <span class="operator">=</span> d.deptno</span><br><span class="line"><span class="keyword">WHERE</span> empno<span class="operator">=</span><span class="number">7369</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--如果是三表或者更多表连接，语法如下</span></span><br><span class="line"><span class="keyword">SELECT</span> a.val, b.val, c.val <span class="keyword">FROM</span> a <span class="keyword">JOIN</span> b <span class="keyword">ON</span> (a.key <span class="operator">=</span> b.key1) <span class="keyword">JOIN</span> c <span class="keyword">ON</span> (c.key <span class="operator">=</span> b.key1)</span><br></pre></td></tr></table></figure>

<h3 id="LEFT-OUTER-JOIN"><a href="#LEFT-OUTER-JOIN" class="headerlink" title="LEFT OUTER JOIN"></a>LEFT OUTER JOIN</h3><p>LEFT OUTER JOIN 和 LEFT JOIN 是等价的。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 左连接</span></span><br><span class="line"><span class="keyword">SELECT</span> e.<span class="operator">*</span>,d.<span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">LEFT</span> <span class="keyword">OUTER</span>  <span class="keyword">JOIN</span>  dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>

<h3 id="RIGHT-OUTER-JOIN"><a href="#RIGHT-OUTER-JOIN" class="headerlink" title="RIGHT OUTER JOIN"></a>RIGHT OUTER JOIN</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--右连接</span></span><br><span class="line"><span class="keyword">SELECT</span> e.<span class="operator">*</span>,d.<span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">RIGHT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span>  dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>

<p>执行右连接后，由于 40 号部门下没有任何员工，所以此时员工信息为 NULL。这个查询可以很好的复述上面提到的——JOIN 语句的关联条件必须用 ON 指定，不能用 WHERE 指定。你可以把 ON 改成 WHERE，你会发现无论如何都查不出 40 号部门这条数据，因为笛卡尔运算不会有 (NULL, 40) 这种情况。</p>
<p><img src="https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hive-right-join.png" alt="img"></p>
<h3 id="FULL-OUTER-JOIN"><a href="#FULL-OUTER-JOIN" class="headerlink" title="FULL OUTER JOIN"></a>FULL OUTER JOIN</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.<span class="operator">*</span>,d.<span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span>  dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>

<h3 id="LEFT-SEMI-JOIN"><a href="#LEFT-SEMI-JOIN" class="headerlink" title="LEFT SEMI JOIN"></a>LEFT SEMI JOIN</h3><p>LEFT SEMI JOIN （左半连接）是 IN&#x2F;EXISTS 子查询的一种更高效的实现。</p>
<ul>
<li>JOIN 子句中右边的表只能在 ON 子句中设置过滤条件;</li>
<li>查询结果只包含左边表的数据，所以只能 SELECT 左表中的列。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询在纽约办公的所有员工信息</span></span><br><span class="line"><span class="keyword">SELECT</span> emp.<span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> emp <span class="keyword">LEFT</span> SEMI <span class="keyword">JOIN</span> dept</span><br><span class="line"><span class="keyword">ON</span> emp.deptno <span class="operator">=</span> dept.deptno <span class="keyword">AND</span> dept.loc<span class="operator">=</span>&quot;NEW YORK&quot;;</span><br><span class="line"></span><br><span class="line"><span class="comment">--上面的语句就等价于</span></span><br><span class="line"><span class="keyword">SELECT</span> emp.<span class="operator">*</span> <span class="keyword">FROM</span> emp</span><br><span class="line"><span class="keyword">WHERE</span> emp.deptno <span class="keyword">IN</span> (<span class="keyword">SELECT</span> deptno <span class="keyword">FROM</span> dept <span class="keyword">WHERE</span> loc<span class="operator">=</span>&quot;NEW YORK&quot;);</span><br></pre></td></tr></table></figure>

<h3 id="JOIN"><a href="#JOIN" class="headerlink" title="JOIN"></a>JOIN</h3><p>笛卡尔积连接，这个连接日常的开发中可能很少遇到，且性能消耗比较大，基于这个原因，如果在严格模式下 (hive.mapred.mode &#x3D; strict)，Hive 会阻止用户执行此操作。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">JOIN</span> dept;</span><br></pre></td></tr></table></figure>

<h2 id="JOIN-优化"><a href="#JOIN-优化" class="headerlink" title="JOIN 优化"></a>JOIN 优化</h2><h3 id="STREAMTABLE"><a href="#STREAMTABLE" class="headerlink" title="STREAMTABLE"></a>STREAMTABLE</h3><p>在多表进行联结的时候，如果每个 ON 字句都使用到共同的列（如下面的 <code>b.key</code>），此时 Hive 会进行优化，将多表 JOIN 在同一个 map &#x2F; reduce 作业上进行。同时假定查询的最后一个表（如下面的 c 表）是最大的一个表，在对每行记录进行 JOIN 操作时，它将尝试将其他的表缓存起来，然后扫描最后那个表进行计算。因此用户需要保证查询的表的大小从左到右是依次增加的。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`<span class="keyword">SELECT</span> a.val, b.val, c.val <span class="keyword">FROM</span> a <span class="keyword">JOIN</span> b <span class="keyword">ON</span> (a.key <span class="operator">=</span> b.key) <span class="keyword">JOIN</span> c <span class="keyword">ON</span> (c.key <span class="operator">=</span> b.key)`</span><br></pre></td></tr></table></figure>

<p>然后，用户并非需要总是把最大的表放在查询语句的最后面，Hive 提供了 <code>/*+ STREAMTABLE() */</code> 标志，用于标识最大的表，示例如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="comment">/*+ STREAMTABLE(d) */</span>  e.<span class="operator">*</span>,d.<span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">JOIN</span> dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno <span class="operator">=</span> d.deptno</span><br><span class="line"><span class="keyword">WHERE</span> job<span class="operator">=</span><span class="string">&#x27;CLERK&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h3 id="MAPJOIN"><a href="#MAPJOIN" class="headerlink" title="MAPJOIN"></a>MAPJOIN</h3><p>如果所有表中只有一张表是小表，那么 Hive 把这张小表加载到内存中。这时候程序会在 map 阶段直接拿另外一个表的数据和内存中表数据做匹配，由于在 map 就进行了 JOIN 操作，从而可以省略 reduce 过程，这样效率可以提升很多。Hive 中提供了 <code>/*+ MAPJOIN() */</code> 来标记小表，示例如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="comment">/*+ MAPJOIN(d) */</span> e.<span class="operator">*</span>,d.<span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">JOIN</span> dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno <span class="operator">=</span> d.deptno</span><br><span class="line"><span class="keyword">WHERE</span> job<span class="operator">=</span><span class="string">&#x27;CLERK&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h2 id="SELECT-的其他用途"><a href="#SELECT-的其他用途" class="headerlink" title="SELECT 的其他用途"></a>SELECT 的其他用途</h2><p>查看当前数据库：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> current_database()</span><br></pre></td></tr></table></figure>

<h2 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h2><p>在上面演示的语句中，大多数都会触发 MapReduce, 少部分不会触发，比如 <code>select * from emp limit 5</code> 就不会触发 MR，此时 Hive 只是简单的读取数据文件中的内容，然后格式化后进行输出。在需要执行 MapReduce 的查询中，你会发现执行时间可能会很长，这时候你可以选择开启本地模式。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--本地模式默认关闭，需要手动开启此功能</span></span><br><span class="line"><span class="keyword">SET</span> hive.exec.mode.local.auto<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure>

<p>启用后，Hive 将分析查询中每个 map-reduce 作业的大小，如果满足以下条件，则可以在本地运行它：</p>
<ul>
<li>作业的总输入大小低于：hive.exec.mode.local.auto.inputbytes.max（默认为 128MB）；</li>
<li>map-tasks 的总数小于：hive.exec.mode.local.auto.tasks.max（默认为 4）；</li>
<li>所需的 reduce 任务总数为 1 或 0。</li>
</ul>
<p>因为我们测试的数据集很小，所以你再次去执行上面涉及 MR 操作的查询，你会发现速度会有显著的提升。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">LanguageManual Select</a></li>
<li><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins">LanguageManual Joins</a></li>
<li><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+GroupBy">LanguageManual GroupBy</a></li>
<li><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+SortBy">LanguageManual SortBy</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/b463223a/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/b463223a/" class="post-title-link" itemprop="url">Hive 简介</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-02-24 21:14:47" itemprop="dateCreated datePublished" datetime="2020-02-24T21:14:47+08:00">2020-02-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hive/" itemprop="url" rel="index"><span itemprop="name">hive</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Hive-简介"><a href="#Hive-简介" class="headerlink" title="Hive 简介"></a>Hive 简介</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>Hive 是一个构建在 Hadoop 之上的数据仓库，它可以将结构化的数据文件映射成表，并提供类 SQL 查询功能</strong>，用于查询的 SQL 语句会被转化为 MapReduce 作业，然后提交到 Hadoop 上运行。</p>
<p><strong>特点</strong>：</p>
<ol>
<li>简单、容易上手 （提供了类似 sql 的查询语言 hql)，使得精通 sql 但是不了解 Java 编程的人也能很好地进行大数据分析；</li>
<li>灵活性高，可以自定义用户函数 (UDF) 和存储格式；</li>
<li>为超大的数据集设计的计算和存储能力，集群扩展容易；</li>
<li>统一的元数据管理，可与 presto／impala／sparksql 等共享数据；</li>
<li>执行延迟高，不适合做数据的实时处理，但适合做海量数据的离线处理。</li>
</ol>
<h2 id="Hive-的体系架构"><a href="#Hive-的体系架构" class="headerlink" title="Hive 的体系架构"></a>Hive 的体系架构</h2><p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20200224193019.png" alt="img"></p>
<h3 id="command-line-shell-thrift-jdbc"><a href="#command-line-shell-thrift-jdbc" class="headerlink" title="command-line shell &amp; thrift&#x2F;jdbc"></a>command-line shell &amp; thrift&#x2F;jdbc</h3><p>可以用 command-line shell 和 thrift／jdbc 两种方式来操作数据：</p>
<ul>
<li><strong>command-line shell</strong>：通过 hive 命令行的的方式来操作数据；</li>
<li><strong>thrift／jdbc</strong>：通过 thrift 协议按照标准的 JDBC 的方式操作数据。</li>
</ul>
<h3 id="Metastore"><a href="#Metastore" class="headerlink" title="Metastore"></a>Metastore</h3><p>在 Hive 中，表名、表结构、字段名、字段类型、表的分隔符等统一被称为元数据。所有的元数据默认存储在 Hive 内置的 derby 数据库中，但由于 derby 只能有一个实例，也就是说不能有多个命令行客户端同时访问，所以在实际生产环境中，通常使用 MySQL 代替 derby。</p>
<p>Hive 进行的是统一的元数据管理，就是说你在 Hive 上创建了一张表，然后在 presto／impala／sparksql 中都是可以直接使用的，它们会从 Metastore 中获取统一的元数据信息，同样的你在 presto／impala／sparksql 中创建一张表，在 Hive 中也可以直接使用。</p>
<h3 id="HQL-的执行流程"><a href="#HQL-的执行流程" class="headerlink" title="HQL 的执行流程"></a>HQL 的执行流程</h3><p>Hive 在执行一条 HQL 的时候，会经过以下步骤：</p>
<ol>
<li>语法解析：Antlr 定义 SQL 的语法规则，完成 SQL 词法，语法解析，将 SQL 转化为抽象 语法树 AST Tree；</li>
<li>语义解析：遍历 AST Tree，抽象出查询的基本组成单元 QueryBlock；</li>
<li>生成逻辑执行计划：遍历 QueryBlock，翻译为执行操作树 OperatorTree；</li>
<li>优化逻辑执行计划：逻辑层优化器进行 OperatorTree 变换，合并不必要的 ReduceSinkOperator，减少 shuffle 数据量；</li>
<li>生成物理执行计划：遍历 OperatorTree，翻译为 MapReduce 任务；</li>
<li>优化物理执行计划：物理层优化器进行 MapReduce 任务的变换，生成最终的执行计划。</li>
</ol>
<blockquote>
<p>关于 Hive SQL 的详细执行流程可以参考美团技术团队的文章：<a target="_blank" rel="noopener" href="https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html">Hive SQL 的编译过程</a></p>
</blockquote>
<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h3 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h3><p>Hive 表中的列支持以下基本数据类型：</p>
<table>
<thead>
<tr>
<th>大类</th>
<th>类型</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Integers（整型）</strong></td>
<td>TINYINT—1 字节的有符号整数 <br/>SMALLINT—2 字节的有符号整数<br/> INT—4 字节的有符号整数<br/> BIGINT—8 字节的有符号整数</td>
</tr>
<tr>
<td><strong>Boolean（布尔型）</strong></td>
<td>BOOLEAN—TRUE&#x2F;FALSE</td>
</tr>
<tr>
<td><strong>Floating point numbers（浮点型）</strong></td>
<td>FLOAT— 单精度浮点型 <br/>DOUBLE—双精度浮点型</td>
</tr>
<tr>
<td><strong>Fixed point numbers（定点数）</strong></td>
<td>DECIMAL—用户自定义精度定点数，比如 DECIMAL(7,2)</td>
</tr>
<tr>
<td><strong>String types（字符串）</strong></td>
<td>STRING—指定字符集的字符序列<br/> VARCHAR—具有最大长度限制的字符序列 <br/>CHAR—固定长度的字符序列</td>
</tr>
<tr>
<td><strong>Date and time types（日期时间类型）</strong></td>
<td>TIMESTAMP — 时间戳 <br/>TIMESTAMP WITH LOCAL TIME ZONE — 时间戳，纳秒精度<br/> DATE—日期类型</td>
</tr>
<tr>
<td><strong>Binary types（二进制类型）</strong></td>
<td>BINARY—字节序列</td>
</tr>
</tbody></table>
<blockquote>
<p>TIMESTAMP 和 TIMESTAMP WITH LOCAL TIME ZONE 的区别如下：</p>
<ul>
<li><strong>TIMESTAMP WITH LOCAL TIME ZONE</strong>：用户提交时间给数据库时，会被转换成数据库所在的时区来保存。查询时则按照查询客户端的不同，转换为查询客户端所在时区的时间。</li>
<li><strong>TIMESTAMP</strong> ：提交什么时间就保存什么时间，查询时也不做任何转换。</li>
</ul>
</blockquote>
<h3 id="隐式转换"><a href="#隐式转换" class="headerlink" title="隐式转换"></a>隐式转换</h3><p>Hive 中基本数据类型遵循以下的层次结构，按照这个层次结构，子类型到祖先类型允许隐式转换。例如 INT 类型的数据允许隐式转换为 BIGINT 类型。额外注意的是：按照类型层次结构允许将 STRING 类型隐式转换为 DOUBLE 类型。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20200224193613.png" alt="img"></p>
<h3 id="复杂类型"><a href="#复杂类型" class="headerlink" title="复杂类型"></a>复杂类型</h3><table>
<thead>
<tr>
<th>类型</th>
<th>描述</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td><strong>STRUCT</strong></td>
<td>类似于对象，是字段的集合，字段的类型可以不同，可以使用 <code>名称。字段名</code> 方式进行访问</td>
<td>STRUCT (‘xiaoming’, 12 , ‘2018-12-12’)</td>
</tr>
<tr>
<td><strong>MAP</strong></td>
<td>键值对的集合，可以使用 <code>名称 [key]</code> 的方式访问对应的值</td>
<td>map(‘a’, 1, ‘b’, 2)</td>
</tr>
<tr>
<td><strong>ARRAY</strong></td>
<td>数组是一组具有相同类型和名称的变量的集合，可以使用 <code>名称 [index]</code> 访问对应的值</td>
<td>ARRAY(‘a’, ‘b’, ‘c’, ‘d’)</td>
</tr>
</tbody></table>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>如下给出一个基本数据类型和复杂数据类型的使用示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> students(</span><br><span class="line">  name      STRING,   <span class="comment">-- 姓名</span></span><br><span class="line">  age       <span class="type">INT</span>,      <span class="comment">-- 年龄</span></span><br><span class="line">  subject   <span class="keyword">ARRAY</span><span class="operator">&lt;</span>STRING<span class="operator">&gt;</span>,   <span class="comment">--学科</span></span><br><span class="line">  score     MAP<span class="operator">&lt;</span>STRING,<span class="type">FLOAT</span><span class="operator">&gt;</span>,  <span class="comment">--各个学科考试成绩</span></span><br><span class="line">  address   STRUCT<span class="operator">&lt;</span>houseNumber:<span class="type">int</span>, street:STRING, city:STRING, province：STRING<span class="operator">&gt;</span>  <span class="comment">--家庭居住地址</span></span><br><span class="line">) <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br></pre></td></tr></table></figure>

<h2 id="内容格式"><a href="#内容格式" class="headerlink" title="内容格式"></a>内容格式</h2><p>当数据存储在文本文件中，必须按照一定格式区别行和列，如使用逗号作为分隔符的 CSV 文件 (Comma-Separated Values) 或者使用制表符作为分隔值的 TSV 文件 (Tab-Separated Values)。但此时也存在一个缺点，就是正常的文件内容中也可能出现逗号或者制表符。</p>
<p>所以 Hive 默认使用了几个平时很少出现的字符，这些字符一般不会作为内容出现在文件中。Hive 默认的行和列分隔符如下表所示。</p>
<table>
<thead>
<tr>
<th>分隔符</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><strong>\n</strong></td>
<td>对于文本文件来说，每行是一条记录，所以可以使用换行符来分割记录</td>
</tr>
<tr>
<td><strong>^A (Ctrl+A)</strong></td>
<td>分割字段 （列），在 CREATE TABLE 语句中也可以使用八进制编码 <code>\001</code> 来表示</td>
</tr>
<tr>
<td><strong>^B</strong></td>
<td>用于分割 ARRAY 或者 STRUCT 中的元素，或者用于 MAP 中键值对之间的分割，<br/>在 CREATE TABLE 语句中也可以使用八进制编码 <code>\002</code> 表示</td>
</tr>
<tr>
<td><strong>^C</strong></td>
<td>用于 MAP 中键和值之间的分割，在 CREATE TABLE 语句中也可以使用八进制编码 <code>\003</code> 表示</td>
</tr>
</tbody></table>
<p>使用示例如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> page_view(viewTime <span class="type">INT</span>, userid <span class="type">BIGINT</span>)</span><br><span class="line"> <span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line">   FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\001&#x27;</span></span><br><span class="line">   COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\002&#x27;</span></span><br><span class="line">   MAP KEYS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\003&#x27;</span></span><br><span class="line"> STORED <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure>

<h2 id="存储格式"><a href="#存储格式" class="headerlink" title="存储格式"></a>存储格式</h2><h3 id="支持的存储格式"><a href="#支持的存储格式" class="headerlink" title="支持的存储格式"></a>支持的存储格式</h3><p>Hive 会在 HDFS 为每个数据库上创建一个目录，数据库中的表是该目录的子目录，表中的数据会以文件的形式存储在对应的表目录下。Hive 支持以下几种文件存储格式：</p>
<table>
<thead>
<tr>
<th>格式</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>TextFile</strong></td>
<td>存储为纯文本文件。 这是 Hive 默认的文件存储格式。这种存储方式数据不做压缩，磁盘开销大，数据解析开销大。</td>
</tr>
<tr>
<td><strong>SequenceFile</strong></td>
<td>SequenceFile 是 Hadoop API 提供的一种二进制文件，它将数据以&lt;key,value&gt;的形式序列化到文件中。这种二进制文件内部使用 Hadoop 的标准的 Writable 接口实现序列化和反序列化。它与 Hadoop API 中的 MapFile 是互相兼容的。Hive 中的 SequenceFile 继承自 Hadoop API 的 SequenceFile，不过它的 key 为空，使用 value 存放实际的值，这样是为了避免 MR 在运行 map 阶段进行额外的排序操作。</td>
</tr>
<tr>
<td><strong>RCFile</strong></td>
<td>RCFile 文件格式是 FaceBook 开源的一种 Hive 的文件存储格式，首先将表分为几个行组，对每个行组内的数据按列存储，每一列的数据都是分开存储。</td>
</tr>
<tr>
<td><strong>ORC Files</strong></td>
<td>ORC 是在一定程度上扩展了 RCFile，是对 RCFile 的优化。</td>
</tr>
<tr>
<td><strong>Avro Files</strong></td>
<td>Avro 是一个数据序列化系统，设计用于支持大批量数据交换的应用。它的主要特点有：支持二进制序列化方式，可以便捷，快速地处理大量数据；动态语言友好，Avro 提供的机制使动态语言可以方便地处理 Avro 数据。</td>
</tr>
<tr>
<td><strong>Parquet</strong></td>
<td>Parquet 是基于 Dremel 的数据模型和算法实现的，面向分析型业务的列式存储格式。它通过按列进行高效压缩和特殊的编码技术，从而在降低存储空间的同时提高了 IO 效率。</td>
</tr>
</tbody></table>
<blockquote>
<p>以上压缩格式中 ORC 和 Parquet 的综合性能突出，使用较为广泛，推荐使用这两种格式。</p>
</blockquote>
<h3 id="指定存储格式"><a href="#指定存储格式" class="headerlink" title="指定存储格式"></a>指定存储格式</h3><p>通常在创建表的时候使用 <code>STORED AS</code> 参数指定：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> page_view(viewTime <span class="type">INT</span>, userid <span class="type">BIGINT</span>)</span><br><span class="line"> <span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line">   FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\001&#x27;</span></span><br><span class="line">   COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\002&#x27;</span></span><br><span class="line">   MAP KEYS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\003&#x27;</span></span><br><span class="line"> STORED <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure>

<p>各个存储文件类型指定方式如下：</p>
<ul>
<li>STORED AS TEXTFILE</li>
<li>STORED AS SEQUENCEFILE</li>
<li>STORED AS ORC</li>
<li>STORED AS PARQUET</li>
<li>STORED AS AVRO</li>
<li>STORED AS RCFILE</li>
</ul>
<h2 id="内部表和外部表"><a href="#内部表和外部表" class="headerlink" title="内部表和外部表"></a>内部表和外部表</h2><p>内部表又叫做管理表 (Managed&#x2F;Internal Table)，创建表时不做任何指定，默认创建的就是内部表。想要创建外部表 (External Table)，则需要使用 External 进行修饰。 内部表和外部表主要区别如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>内部表</th>
<th>外部表</th>
</tr>
</thead>
<tbody><tr>
<td>数据存储位置</td>
<td>内部表数据存储的位置由 hive.metastore.warehouse.dir 参数指定，默认情况下表的数据存储在 HDFS 的 <code>/user/hive/warehouse/数据库名。db/表名/</code> 目录下</td>
<td>外部表数据的存储位置创建表时由 <code>Location</code> 参数指定；</td>
</tr>
<tr>
<td>导入数据</td>
<td>在导入数据到内部表，内部表将数据移动到自己的数据仓库目录下，数据的生命周期由 Hive 来进行管理</td>
<td>外部表不会将数据移动到自己的数据仓库目录下，只是在元数据中存储了数据的位置</td>
</tr>
<tr>
<td>删除表</td>
<td>删除元数据（metadata）和文件</td>
<td>只删除元数据（metadata）</td>
</tr>
</tbody></table>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">Hive Getting Started</a></li>
<li><a target="_blank" rel="noopener" href="https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html">Hive SQL 的编译过程</a></li>
<li><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL">LanguageManual DDL</a></li>
<li><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types">LanguageManual Types</a></li>
<li><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/Managed+vs.+External+Tables">Managed vs. External Tables</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/3ed08002/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/3ed08002/" class="post-title-link" itemprop="url">Hive 表</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-02-24 21:14:47" itemprop="dateCreated datePublished" datetime="2020-02-24T21:14:47+08:00">2020-02-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hive/" itemprop="url" rel="index"><span itemprop="name">hive</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Hive-表"><a href="#Hive-表" class="headerlink" title="Hive 表"></a>Hive 表</h1><h2 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>Hive 中的表对应为 HDFS 上的指定目录，在查询数据时候，默认会对全表进行扫描，这样时间和性能的消耗都非常大。</p>
<p><strong>分区为 HDFS 上表目录的子目录</strong>，数据按照分区存储在子目录中。如果查询的 <code>where</code> 子句中包含分区条件，则直接从该分区去查找，而不是扫描整个表目录，合理的分区设计可以极大提高查询速度和性能。</p>
<p>分区表并非 Hive 独有的概念，实际上这个概念非常常见。通常，在管理大规模数据集的时候都需要进行分区，比如将日志文件按天进行分区，从而保证数据细粒度的划分，使得查询性能得到提升。比如，在我们常用的 Oracle 数据库中，当表中的数据量不断增大，查询数据的速度就会下降，这时也可以对表进行分区。表进行分区后，逻辑上表仍然是一张完整的表，只是将表中的数据存放到多个表空间（物理文件上），这样查询数据时，就不必要每次都扫描整张表，从而提升查询性能。</p>
<h3 id="创建分区表"><a href="#创建分区表" class="headerlink" title="创建分区表"></a>创建分区表</h3><p>在 Hive 中可以使用 <code>PARTITIONED BY</code> 子句创建分区表。表可以包含一个或多个分区列，程序会为分区列中的每个不同值组合创建单独的数据目录。下面的我们创建一张雇员表作为测试：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_partition(</span><br><span class="line">   empno <span class="type">INT</span>,</span><br><span class="line">   ename STRING,</span><br><span class="line">   job STRING,</span><br><span class="line">   mgr <span class="type">INT</span>,</span><br><span class="line">   hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">   sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">   comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>)</span><br><span class="line">   )</span><br><span class="line">   PARTITIONED <span class="keyword">BY</span> (deptno <span class="type">INT</span>)   <span class="comment">-- 按照部门编号进行分区</span></span><br><span class="line">   <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;</span><br><span class="line">   LOCATION <span class="string">&#x27;/hive/emp_partition&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h3 id="加载数据到分区表"><a href="#加载数据到分区表" class="headerlink" title="加载数据到分区表"></a>加载数据到分区表</h3><p>加载数据到分区表时候必须要指定数据所处的分区：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">加载部门编号为 20 的数据到表中</span></span><br><span class="line">LOAD DATA LOCAL INPATH &quot;/usr/file/emp20.txt&quot; OVERWRITE INTO TABLE emp_partition PARTITION (deptno=20)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">加载部门编号为 30 的数据到表中</span></span><br><span class="line">LOAD DATA LOCAL INPATH &quot;/usr/file/emp30.txt&quot; OVERWRITE INTO TABLE emp_partition PARTITION (deptno=30)</span><br></pre></td></tr></table></figure>

<h3 id="查看分区目录"><a href="#查看分区目录" class="headerlink" title="查看分区目录"></a>查看分区目录</h3><p>这时候我们直接查看表目录，可以看到表目录下存在两个子目录，分别是 <code>deptno=20</code> 和 <code>deptno=30</code>, 这就是分区目录，分区目录下才是我们加载的数据文件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hadoop fs -<span class="built_in">ls</span>  hdfs://hadoop001:8020/hive/emp_partition/</span></span><br></pre></td></tr></table></figure>

<p>这时候当你的查询语句的 <code>where</code> 包含 <code>deptno=20</code>，则就去对应的分区目录下进行查找，而不用扫描全表。</p>
<p><img src="https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hive-hadoop-partitation.png" alt="img"></p>
<h2 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>分区提供了一个隔离数据和优化查询的可行方案，但是并非所有的数据集都可以形成合理的分区，分区的数量也不是越多越好，过多的分区条件可能会导致很多分区上没有数据。同时 Hive 会限制动态分区可以创建的最大分区数，用来避免过多分区文件对文件系统产生负担。鉴于以上原因，Hive 还提供了一种更加细粒度的数据拆分方案：分桶表 (bucket Table)。</p>
<p>分桶表会将指定列的值进行哈希散列，并对 bucket（桶数量）取余，然后存储到对应的 bucket（桶）中。</p>
<h3 id="理解分桶表"><a href="#理解分桶表" class="headerlink" title="理解分桶表"></a>理解分桶表</h3><p>单从概念上理解分桶表可能会比较晦涩，其实和分区一样，分桶这个概念同样不是 Hive 独有的，对于 Java 开发人员而言，这可能是一个每天都会用到的概念，因为 Hive 中的分桶概念和 Java 数据结构中的 HashMap 的分桶概念是一致的。</p>
<p>当调用 HashMap 的 put() 方法存储数据时，程序会先对 key 值调用 hashCode() 方法计算出 hashcode，然后对数组长度取模计算出 index，最后将数据存储在数组 index 位置的链表上，链表达到一定阈值后会转换为红黑树 (JDK1.8+)。下图为 HashMap 的数据结构图：</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20200224194352.png" alt="img"></p>
<p>图片引用自：<a target="_blank" rel="noopener" href="http://www.itcuties.com/java/hashmap-hashtable/">HashMap vs. Hashtable</a></p>
<h3 id="创建分桶表"><a href="#创建分桶表" class="headerlink" title="创建分桶表"></a>创建分桶表</h3><p>在 Hive 中，我们可以通过 <code>CLUSTERED BY</code> 指定分桶列，并通过 <code>SORTED BY</code> 指定桶中数据的排序参考列。下面为分桶表建表语句示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_bucket(</span><br><span class="line">  empno <span class="type">INT</span>,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr <span class="type">INT</span>,</span><br><span class="line">  hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  deptno <span class="type">INT</span>)</span><br><span class="line">  CLUSTERED <span class="keyword">BY</span>(empno) SORTED <span class="keyword">BY</span>(empno <span class="keyword">ASC</span>) <span class="keyword">INTO</span> <span class="number">4</span> BUCKETS  <span class="comment">--按照员工编号散列到四个 bucket 中</span></span><br><span class="line">  <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;</span><br><span class="line">  LOCATION <span class="string">&#x27;/hive/emp_bucket&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h3 id="加载数据到分桶表"><a href="#加载数据到分桶表" class="headerlink" title="加载数据到分桶表"></a>加载数据到分桶表</h3><p>这里直接使用 <code>Load</code> 语句向分桶表加载数据，数据时可以加载成功的，但是数据并不会分桶。</p>
<p>这是由于分桶的实质是对指定字段做了 hash 散列然后存放到对应文件中，这意味着向分桶表中插入数据是必然要通过 MapReduce，且 Reducer 的数量必须等于分桶的数量。由于以上原因，分桶表的数据通常只能使用 CTAS(CREATE TABLE AS SELECT) 方式插入，因为 CTAS 操作会触发 MapReduce。加载数据步骤如下：</p>
<h4 id="设置强制分桶"><a href="#设置强制分桶" class="headerlink" title="设置强制分桶"></a>设置强制分桶</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.enforce.bucketing <span class="operator">=</span> <span class="literal">true</span>; <span class="comment">--Hive 2.x 不需要这一步</span></span><br></pre></td></tr></table></figure>

<p>在 Hive 0.x and 1.x 版本，必须使用设置 <code>hive.enforce.bucketing = true</code>，表示强制分桶，允许程序根据表结构自动选择正确数量的 Reducer 和 cluster by column 来进行分桶。</p>
<h4 id="CTAS-导入数据"><a href="#CTAS-导入数据" class="headerlink" title="CTAS 导入数据"></a>CTAS 导入数据</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT INTO</span> <span class="keyword">TABLE</span> emp_bucket <span class="keyword">SELECT</span> <span class="operator">*</span>  <span class="keyword">FROM</span> emp;  <span class="comment">--这里的 emp 表就是一张普通的雇员表</span></span><br></pre></td></tr></table></figure>

<p>可以从执行日志看到 CTAS 触发 MapReduce 操作，且 Reducer 数量和建表时候指定 bucket 数量一致：</p>
<p><img src="https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hive-hadoop-mapreducer.png" alt="img"></p>
<h3 id="查看分桶文件"><a href="#查看分桶文件" class="headerlink" title="查看分桶文件"></a>查看分桶文件</h3><p>bucket（桶） 本质上就是表目录下的具体文件：</p>
<p><img src="https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hive-hadoop-bucket.png" alt="img"></p>
<h2 id="分区表和分桶表结合使用"><a href="#分区表和分桶表结合使用" class="headerlink" title="分区表和分桶表结合使用"></a>分区表和分桶表结合使用</h2><p>分区表和分桶表的本质都是将数据按照不同粒度进行拆分，从而使得在查询时候不必扫描全表，只需要扫描对应的分区或分桶，从而提升查询效率。两者可以结合起来使用，从而保证表数据在不同粒度上都能得到合理的拆分。下面是 Hive 官方给出的示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> page_view_bucketed(</span><br><span class="line">	viewTime <span class="type">INT</span>,</span><br><span class="line">    userid <span class="type">BIGINT</span>,</span><br><span class="line">    page_url STRING,</span><br><span class="line">    referrer_url STRING,</span><br><span class="line">    ip STRING )</span><br><span class="line"> PARTITIONED <span class="keyword">BY</span>(dt STRING)</span><br><span class="line"> CLUSTERED <span class="keyword">BY</span>(userid) SORTED <span class="keyword">BY</span>(viewTime) <span class="keyword">INTO</span> <span class="number">32</span> BUCKETS</span><br><span class="line"> <span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line">   FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\001&#x27;</span></span><br><span class="line">   COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\002&#x27;</span></span><br><span class="line">   MAP KEYS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\003&#x27;</span></span><br><span class="line"> STORED <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure>

<p>此时导入数据时需要指定分区：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE page_view_bucketed</span><br><span class="line">PARTITION (dt=&#x27;2009-02-25&#x27;)</span><br><span class="line">SELECT * FROM page_view WHERE dt=&#x27;2009-02-25&#x27;;</span><br></pre></td></tr></table></figure>

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL+BucketedTables">LanguageManual DDL BucketedTables</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/4222fb31/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/4222fb31/" class="post-title-link" itemprop="url">Hive 视图和索引</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-02-24 21:14:47" itemprop="dateCreated datePublished" datetime="2020-02-24T21:14:47+08:00">2020-02-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hive/" itemprop="url" rel="index"><span itemprop="name">hive</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Hive-视图和索引"><a href="#Hive-视图和索引" class="headerlink" title="Hive 视图和索引"></a>Hive 视图和索引</h1><h2 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>Hive 中的视图和 RDBMS 中视图的概念一致，都是一组数据的逻辑表示，本质上就是一条 SELECT 语句的结果集。视图是纯粹的逻辑对象，没有关联的存储 (Hive 3.0.0 引入的物化视图除外)，当查询引用视图时，Hive 可以将视图的定义与查询结合起来，例如将查询中的过滤器推送到视图中。</p>
<h3 id="创建视图"><a href="#创建视图" class="headerlink" title="创建视图"></a>创建视图</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]view_name   <span class="comment">-- 视图名称</span></span><br><span class="line">  [(column_name [COMMENT column_comment], ...) ]    <span class="comment">--列名</span></span><br><span class="line">  [COMMENT view_comment]  <span class="comment">--视图注释</span></span><br><span class="line">  [TBLPROPERTIES (property_name <span class="operator">=</span> property_value, ...)]  <span class="comment">--额外信息</span></span><br><span class="line">  <span class="keyword">AS</span> <span class="keyword">SELECT</span> ...;</span><br></pre></td></tr></table></figure>

<p>在 Hive 中可以使用 <code>CREATE VIEW</code> 创建视图，如果已存在具有相同名称的表或视图，则会抛出异常，建议使用 <code>IF NOT EXISTS</code> 预做判断。在使用视图时候需要注意以下事项：</p>
<ul>
<li><p>视图是只读的，不能用作 <code>LOAD</code> &#x2F; <code>INSERT</code> &#x2F; <code>ALTER</code> 的目标；</p>
</li>
<li><p>在创建视图时候视图就已经固定，对基表的后续更改（如添加列）将不会反映在视图；</p>
</li>
<li><p>删除基表并不会删除视图，需要手动删除视图；</p>
</li>
<li><p>视图可能包含 <code>ORDER BY</code> 和 <code>LIMIT</code> 子句。如果引用视图的查询语句也包含这类子句，其执行优先级低于视图对应字句。例如，视图 <code>custom_view</code> 指定 LIMIT 5，查询语句为 <code>select * from custom_view LIMIT 10</code>，此时结果最多返回 5 行。</p>
</li>
<li><p>创建视图时，如果未提供列名，则将从 SELECT 语句中自动派生列名；</p>
</li>
<li><p>创建视图时，如果 SELECT 语句中包含其他表达式，例如 x + y，则列名称将以_C0，_C1 等形式生成；</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span>  IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> custom_view <span class="keyword">AS</span> <span class="keyword">SELECT</span> empno, empno<span class="operator">+</span>deptno , <span class="number">1</span><span class="operator">+</span><span class="number">2</span> <span class="keyword">FROM</span> emp;</span><br></pre></td></tr></table></figure></li>
</ul>
<p><img src="https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hive-1-2-view.png" alt="img"></p>
<h3 id="查看视图"><a href="#查看视图" class="headerlink" title="查看视图"></a>查看视图</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查看所有视图： 没有单独查看视图列表的语句，只能使用 show tables</span></span><br><span class="line"><span class="keyword">show</span> tables;</span><br><span class="line"><span class="comment">-- 查看某个视图</span></span><br><span class="line"><span class="keyword">desc</span> view_name;</span><br><span class="line"><span class="comment">-- 查看某个视图详细信息</span></span><br><span class="line"><span class="keyword">desc</span> formatted view_name;</span><br></pre></td></tr></table></figure>

<h3 id="删除视图"><a href="#删除视图" class="headerlink" title="删除视图"></a>删除视图</h3><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">VIEW</span> [<span class="keyword">IF</span> <span class="keyword">EXISTS</span>] [db_name.]view_name;</span><br></pre></td></tr></table></figure>

<p>删除视图时，如果被删除的视图被其他视图所引用，这时候程序不会发出警告，但是引用该视图其他视图已经失效，需要进行重建或者删除。</p>
<h3 id="修改视图"><a href="#修改视图" class="headerlink" title="修改视图"></a>修改视图</h3><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">VIEW</span> [db_name.]view_name <span class="keyword">AS</span> select_statement;</span><br></pre></td></tr></table></figure>

<p>被更改的视图必须存在，且视图不能具有分区，如果视图具有分区，则修改失败。</p>
<h3 id="修改视图属性"><a href="#修改视图属性" class="headerlink" title="修改视图属性"></a>修改视图属性</h3><p>语法：</p>
<figure class="highlight sas"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">VIEW</span> [db_name.]view_name <span class="keyword">SET</span> TBLPROPERTIES table_properties;</span><br><span class="line"></span><br><span class="line">table_properties:</span><br><span class="line">  : (property_name = property_value, property_name = property_value, ...)</span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">VIEW</span> custom_view <span class="keyword">SET</span> TBLPROPERTIES (<span class="string">&#x27;create&#x27;</span>=<span class="string">&#x27;heibaiying&#x27;</span>,<span class="string">&#x27;date&#x27;</span>=<span class="string">&#x27;2019-05-05&#x27;</span>);</span><br></pre></td></tr></table></figure>

<p><img src="https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hive-view-properties.png" alt="img"></p>
<h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p>Hive 在 0.7.0 引入了索引的功能，索引的设计目标是提高表某些列的查询速度。如果没有索引，带有谓词的查询（如’WHERE table1.column &#x3D; 10’）会加载整个表或分区并处理所有行。但是如果 column 存在索引，则只需要加载和处理文件的一部分。</p>
<h3 id="索引原理"><a href="#索引原理" class="headerlink" title="索引原理"></a>索引原理</h3><p>在指定列上建立索引，会产生一张索引表（表结构如下），里面的字段包括：索引列的值、该值对应的 HDFS 文件路径、该值在文件中的偏移量。在查询涉及到索引字段时，首先到索引表查找索引列值对应的 HDFS 文件路径及偏移量，这样就避免了全表扫描。</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+--------------+----------------+----------+--+</span><br><span class="line">|<span class="string">   col_name   </span>|<span class="string">   data_type    </span>|<span class="string"> comment     </span>|</span><br><span class="line">+--------------+----------------+----------+--+</span><br><span class="line">|<span class="string"> empno        </span>|<span class="string"> int            </span>|<span class="string">  建立索引的列  </span>|</span><br><span class="line">|<span class="string"> _bucketname  </span>|<span class="string"> string         </span>|<span class="string">  HDFS 文件路径  </span>|</span><br><span class="line">|<span class="string"> _offsets     </span>|<span class="string"> array&lt;bigint&gt;  </span>|<span class="string">  偏移量       </span>|</span><br><span class="line">+--------------+----------------+----------+--+</span><br></pre></td></tr></table></figure>

<h3 id="创建索引"><a href="#创建索引" class="headerlink" title="创建索引"></a>创建索引</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> INDEX index_name     <span class="comment">--索引名称</span></span><br><span class="line">  <span class="keyword">ON</span> <span class="keyword">TABLE</span> base_table_name (col_name, ...)  <span class="comment">--建立索引的列</span></span><br><span class="line">  <span class="keyword">AS</span> index_type    <span class="comment">--索引类型</span></span><br><span class="line">  [<span class="keyword">WITH</span> DEFERRED REBUILD]    <span class="comment">--重建索引</span></span><br><span class="line">  [IDXPROPERTIES (property_name<span class="operator">=</span>property_value, ...)]  <span class="comment">--索引额外属性</span></span><br><span class="line">  [<span class="keyword">IN</span> <span class="keyword">TABLE</span> index_table_name]    <span class="comment">--索引表的名字</span></span><br><span class="line">  [</span><br><span class="line">     [ <span class="type">ROW</span> FORMAT ...] STORED <span class="keyword">AS</span> ...</span><br><span class="line">     <span class="operator">|</span> STORED <span class="keyword">BY</span> ...</span><br><span class="line">  ]   <span class="comment">--索引表行分隔符 、 存储格式</span></span><br><span class="line">  [LOCATION hdfs_path]  <span class="comment">--索引表存储位置</span></span><br><span class="line">  [TBLPROPERTIES (...)]   <span class="comment">--索引表表属性</span></span><br><span class="line">  [COMMENT &quot;index comment&quot;];  <span class="comment">--索引注释</span></span><br></pre></td></tr></table></figure>

<h3 id="查看索引"><a href="#查看索引" class="headerlink" title="查看索引"></a>查看索引</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--显示表上所有列的索引</span></span><br><span class="line"><span class="keyword">SHOW</span> FORMATTED INDEX <span class="keyword">ON</span> table_name;</span><br></pre></td></tr></table></figure>

<h3 id="删除索引"><a href="#删除索引" class="headerlink" title="删除索引"></a>删除索引</h3><p>删除索引会删除对应的索引表。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> INDEX [IF <span class="keyword">EXISTS</span>] index_name <span class="keyword">ON</span> table_name;</span><br></pre></td></tr></table></figure>

<p>如果存在索引的表被删除了，其对应的索引和索引表都会被删除。如果被索引表的某个分区被删除了，那么分区对应的分区索引也会被删除。</p>
<h3 id="重建索引"><a href="#重建索引" class="headerlink" title="重建索引"></a>重建索引</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> INDEX index_name <span class="keyword">ON</span> table_name [<span class="keyword">PARTITION</span> partition_spec] REBUILD;</span><br></pre></td></tr></table></figure>

<p>重建索引。如果指定了 PARTITION，则仅重建该分区的索引。</p>
<h2 id="索引案例"><a href="#索引案例" class="headerlink" title="索引案例"></a>索引案例</h2><h3 id="创建索引-1"><a href="#创建索引-1" class="headerlink" title="创建索引"></a>创建索引</h3><p>在 emp 表上针对 <code>empno</code> 字段创建名为 <code>emp_index</code>,索引数据存储在 <code>emp_index_table</code> 索引表中</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> index emp_index <span class="keyword">on</span> <span class="keyword">table</span> emp(empno) <span class="keyword">as</span></span><br><span class="line"><span class="string">&#x27;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&#x27;</span></span><br><span class="line"><span class="keyword">with</span> deferred rebuild</span><br><span class="line"><span class="keyword">in</span> <span class="keyword">table</span> emp_index_table ;</span><br></pre></td></tr></table></figure>

<p>此时索引表中是没有数据的，需要重建索引才会有索引的数据。</p>
<h3 id="重建索引-1"><a href="#重建索引-1" class="headerlink" title="重建索引"></a>重建索引</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> index emp_index <span class="keyword">on</span> emp rebuild;</span><br></pre></td></tr></table></figure>

<p>Hive 会启动 MapReduce 作业去建立索引，建立好后查看索引表数据如下。三个表字段分别代表：索引列的值、该值对应的 HDFS 文件路径、该值在文件中的偏移量。</p>
<p><img src="https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hive-index-table.png" alt="img"></p>
<h3 id="自动使用索引"><a href="#自动使用索引" class="headerlink" title="自动使用索引"></a>自动使用索引</h3><p>默认情况下，虽然建立了索引，但是 Hive 在查询时候是不会自动去使用索引的，需要开启相关配置。开启配置后，涉及到索引列的查询就会使用索引功能去优化查询。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line"><span class="keyword">SET</span> hive.optimize.index.filter<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">SET</span> hive.optimize.index.filter.compact.minsize<span class="operator">=</span><span class="number">0</span>;</span><br></pre></td></tr></table></figure>

<h3 id="查看索引-1"><a href="#查看索引-1" class="headerlink" title="查看索引"></a>查看索引</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> INDEX <span class="keyword">ON</span> emp;</span><br></pre></td></tr></table></figure>

<p><img src="https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hive-index-show.png" alt="img"></p>
<h2 id="索引的缺陷"><a href="#索引的缺陷" class="headerlink" title="索引的缺陷"></a>索引的缺陷</h2><p>索引表最主要的一个缺陷在于：索引表无法自动 rebuild，这也就意味着如果表中有数据新增或删除，则必须手动 rebuild，重新执行 MapReduce 作业，生成索引表数据。</p>
<p>同时按照<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Indexing">官方文档</a> 的说明，Hive 会从 3.0 开始移除索引功能，主要基于以下两个原因：</p>
<ul>
<li>具有自动重写的物化视图 (Materialized View) 可以产生与索引相似的效果（Hive 2.3.0 增加了对物化视图的支持，在 3.0 之后正式引入）。</li>
<li>使用列式存储文件格式（Parquet，ORC）进行存储时，这些格式支持选择性扫描，可以跳过不需要的文件或块。</li>
</ul>
<blockquote>
<p>ORC 内置的索引功能可以参阅这篇文章：<a target="_blank" rel="noopener" href="http://lxw1234.com/archives/2016/04/632.htm">Hive 性能优化之 ORC 索引–Row Group Index vs Bloom Filter Index</a></p>
</blockquote>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-Create/Drop/AlterView">Create&#x2F;Drop&#x2F;Alter View</a></li>
<li><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/Materialized+views">Materialized views</a></li>
<li><a target="_blank" rel="noopener" href="http://lxw1234.com/archives/2015/05/207.htm">Hive 索引</a></li>
<li><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Indexing">Overview of Hive Indexes</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/c9cd1487/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/c9cd1487/" class="post-title-link" itemprop="url">Hive 运维</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-02-24 21:14:47" itemprop="dateCreated datePublished" datetime="2020-02-24T21:14:47+08:00">2020-02-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hive/" itemprop="url" rel="index"><span itemprop="name">hive</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>8.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>8 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Hive-运维"><a href="#Hive-运维" class="headerlink" title="Hive 运维"></a>Hive 运维</h1><h2 id="Hive-安装"><a href="#Hive-安装" class="headerlink" title="Hive 安装"></a>Hive 安装</h2><h3 id="下载并解压"><a href="#下载并解压" class="headerlink" title="下载并解压"></a>下载并解压</h3><p>下载所需版本的 Hive，这里我下载版本为 <code>cdh5.15.2</code>。下载地址：<a target="_blank" rel="noopener" href="http://archive.cloudera.com/cdh5/cdh/5/">http://archive.cloudera.com/cdh5/cdh/5/</a></p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载后进行解压</span></span><br><span class="line"> <span class="attribute">tar</span> -zxvf hive-<span class="number">1</span>.<span class="number">1</span>.<span class="number">0</span>-cdh5.<span class="number">15</span>.<span class="number">2</span>.tar.gz</span><br></pre></td></tr></table></figure>

<h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">vim /etc/profile</span></span><br></pre></td></tr></table></figure>

<p>添加环境变量：</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> <span class="attribute">HIVE_HOME</span>=/usr/app/hive-1.1.0-cdh5.15.2</span><br><span class="line"><span class="built_in">export</span> <span class="attribute">PATH</span>=<span class="variable">$HIVE_HOME</span>/bin:$PATH</span><br></pre></td></tr></table></figure>

<p>使得配置的环境变量立即生效：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">source</span> /etc/profile</span></span><br></pre></td></tr></table></figure>

<h3 id="修改配置"><a href="#修改配置" class="headerlink" title="修改配置"></a>修改配置</h3><p><strong>1. hive-env.sh</strong></p>
<p>进入安装目录下的 <code>conf/</code> 目录，拷贝 Hive 的环境配置模板 <code>flume-env.sh.template</code></p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cp</span> hive-env.<span class="keyword">sh</span>.template hive-env.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure>

<p>修改 <code>hive-env.sh</code>，指定 Hadoop 的安装路径：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">HADOOP_HOME</span>=/usr/app/hadoop-<span class="number">2</span>.<span class="number">6</span>.<span class="number">0</span>-cdh5.<span class="number">15</span>.<span class="number">2</span></span><br></pre></td></tr></table></figure>

<p><strong>2. hive-site.xml</strong></p>
<p>新建 hive-site.xml 文件，内容如下，主要是配置存放元数据的 MySQL 的地址、驱动、用户名和密码等信息：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop001:3306/hadoop_hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="拷贝数据库驱动"><a href="#拷贝数据库驱动" class="headerlink" title="拷贝数据库驱动"></a>拷贝数据库驱动</h3><p>将 MySQL 驱动包拷贝到 Hive 安装目录的 <code>lib</code> 目录下, MySQL 驱动的下载地址为：<a target="_blank" rel="noopener" href="https://dev.mysql.com/downloads/connector/j/">https://dev.mysql.com/downloads/connector/j/</a>。</p>
<h3 id="初始化元数据库"><a href="#初始化元数据库" class="headerlink" title="初始化元数据库"></a>初始化元数据库</h3><ul>
<li><p>当使用的 hive 是 1.x 版本时，可以不进行初始化操作，Hive 会在第一次启动的时候会自动进行初始化，但不会生成所有的元数据信息表，只会初始化必要的一部分，在之后的使用中用到其余表时会自动创建；</p>
</li>
<li><p>当使用的 hive 是 2.x 版本时，必须手动初始化元数据库。初始化命令：</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># schematool 命令在安装目录的 bin 目录下，由于上面已经配置过环境变量，在任意位置执行即可</span></span><br><span class="line"><span class="keyword">schematool </span>-dbType mysql -initSchema</span><br></pre></td></tr></table></figure></li>
</ul>
<p>这里我使用的是 CDH 的 <code>hive-1.1.0-cdh5.15.2.tar.gz</code>，对应 <code>Hive 1.1.0</code> 版本，可以跳过这一步。</p>
<h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><p>由于已经将 Hive 的 bin 目录配置到环境变量，直接使用以下命令启动，成功进入交互式命令行后执行 <code>show databases</code> 命令，无异常则代表搭建成功。</p>
<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># hive</span></span><br></pre></td></tr></table></figure>

<p><img src="https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hive-install-2.png" alt="img"></p>
<p>在 Mysql 中也能看到 Hive 创建的库和存放元数据信息的表</p>
<p><img src="https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hive-mysql-tables.png" alt="img"></p>
<h2 id="HiveServer2-beeline"><a href="#HiveServer2-beeline" class="headerlink" title="HiveServer2&#x2F;beeline"></a>HiveServer2&#x2F;beeline</h2><p>Hive 内置了 HiveServer 和 HiveServer2 服务，两者都允许客户端使用多种编程语言进行连接，但是 HiveServer 不能处理多个客户端的并发请求，因此产生了 HiveServer2。</p>
<p>HiveServer2（HS2）允许远程客户端可以使用各种编程语言向 Hive 提交请求并检索结果，支持多客户端并发访问和身份验证。HS2 是由多个服务组成的单个进程，其包括基于 Thrift 的 Hive 服务（TCP 或 HTTP）和用于 Web UI 的 Jetty Web 服务。</p>
<p>HiveServer2 拥有自己的 CLI 工具——Beeline。Beeline 是一个基于 SQLLine 的 JDBC 客户端。由于目前 HiveServer2 是 Hive 开发维护的重点，所以官方更加推荐使用 Beeline 而不是 Hive CLI。以下主要讲解 Beeline 的配置方式。</p>
<h3 id="修改-Hadoop-配置"><a href="#修改-Hadoop-配置" class="headerlink" title="修改 Hadoop 配置"></a>修改 Hadoop 配置</h3><p>修改 hadoop 集群的 core-site.xml 配置文件，增加如下配置，指定 hadoop 的 root 用户可以代理本机上所有的用户。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>之所以要配置这一步，是因为 hadoop 2.0 以后引入了安全伪装机制，使得 hadoop 不允许上层系统（如 hive）直接将实际用户传递到 hadoop 层，而应该将实际用户传递给一个超级代理，由该代理在 hadoop 上执行操作，以避免任意客户端随意操作 hadoop。如果不配置这一步，在之后的连接中可能会抛出 <code>AuthorizationException</code> 异常。</p>
<blockquote>
<p>关于 Hadoop 的用户代理机制，可以参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/u012948976/article/details/49904675#%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E8%A7%A3%E8%AF%BB">hadoop 的用户代理机制</a> 或 <a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Superusers.html">Superusers Acting On Behalf Of Other Users</a></p>
</blockquote>
<h3 id="启动-hiveserver2"><a href="#启动-hiveserver2" class="headerlink" title="启动 hiveserver2"></a>启动 hiveserver2</h3><p>由于上面已经配置过环境变量，这里直接启动即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">nohup</span> hiveserver2 &amp;</span></span><br></pre></td></tr></table></figure>

<h3 id="使用-beeline"><a href="#使用-beeline" class="headerlink" title="使用 beeline"></a>使用 beeline</h3><p>可以使用以下命令进入 beeline 交互式命令行，出现 <code>Connected</code> 则代表连接成功。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beeline -u jdbc:hive2://hadoop001:10000 -n root</span><br></pre></td></tr></table></figure>

<h3 id="Beeline-选项"><a href="#Beeline-选项" class="headerlink" title="Beeline 选项"></a>Beeline 选项</h3><p>Beeline 拥有更多可使用参数，可以使用 <code>beeline --help</code> 查看，完整参数如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">Usage: java org.apache.hive.cli.beeline.BeeLine</span><br><span class="line">   -u &lt;database url&gt;               the JDBC URL to connect to</span><br><span class="line">   -r                              reconnect to last saved connect url (in conjunction with !save)</span><br><span class="line">   -n &lt;username&gt;                   the username to connect as</span><br><span class="line">   -p &lt;password&gt;                   the password to connect as</span><br><span class="line">   -d &lt;driver class&gt;               the driver class to use</span><br><span class="line">   -i &lt;init file&gt;                  script file for initialization</span><br><span class="line">   -e &lt;query&gt;                      query that should be executed</span><br><span class="line">   -f &lt;exec file&gt;                  script file that should be executed</span><br><span class="line">   -w (or) --password-file &lt;password file&gt;  the password file to read password from</span><br><span class="line">   --hiveconf property=value       Use value for given property</span><br><span class="line">   --hivevar name=value            hive variable name and value</span><br><span class="line">                                   This is Hive specific settings in which variables</span><br><span class="line">                                   can be set at session level and referenced in Hive</span><br><span class="line">                                   commands or queries.</span><br><span class="line">   --property-file=&lt;property-file&gt; the file to read connection properties (url, driver, user, password) from</span><br><span class="line">   --color=[true/false]            control whether color is used for display</span><br><span class="line">   --showHeader=[true/false]       show column names in query results</span><br><span class="line">   --headerInterval=ROWS;          the interval between which heades are displayed</span><br><span class="line">   --fastConnect=[true/false]      skip building table/column list for tab-completion</span><br><span class="line">   --autoCommit=[true/false]       enable/disable automatic transaction commit</span><br><span class="line">   --verbose=[true/false]          show verbose error messages and debug info</span><br><span class="line">   --showWarnings=[true/false]     display connection warnings</span><br><span class="line">   --showNestedErrs=[true/false]   display nested errors</span><br><span class="line">   --numberFormat=[pattern]        format numbers using DecimalFormat pattern</span><br><span class="line">   --force=[true/false]            continue running script even after errors</span><br><span class="line">   --maxWidth=MAXWIDTH             the maximum width of the terminal</span><br><span class="line">   --maxColumnWidth=MAXCOLWIDTH    the maximum width to use when displaying columns</span><br><span class="line">   --silent=[true/false]           be more silent</span><br><span class="line">   --autosave=[true/false]         automatically save preferences</span><br><span class="line">   --outputformat=[table/vertical/csv2/tsv2/dsv/csv/tsv]  format mode for result display</span><br><span class="line">   --incrementalBufferRows=NUMROWS the number of rows to buffer when printing rows on stdout,</span><br><span class="line">                                   defaults to 1000; only applicable if --incremental=true</span><br><span class="line">                                   and --outputformat=table</span><br><span class="line">   --truncateTable=[true/false]    truncate table column when it exceeds length</span><br><span class="line">   --delimiterForDSV=DELIMITER     specify the delimiter for delimiter-separated values output format (default: |)</span><br><span class="line">   --isolation=LEVEL               set the transaction isolation level</span><br><span class="line">   --nullemptystring=[true/false]  set to true to get historic behavior of printing null as empty string</span><br><span class="line">   --maxHistoryRows=MAXHISTORYROWS The maximum number of rows to store beeline history.</span><br><span class="line">   --convertBinaryArrayToString=[true/false]    display binary column data as string or as byte array</span><br><span class="line">   --help                          display this message</span><br></pre></td></tr></table></figure>

<h3 id="常用参数"><a href="#常用参数" class="headerlink" title="常用参数"></a>常用参数</h3><p>在 Hive CLI 中支持的参数，Beeline 都支持，常用的参数如下。更多参数说明可以参见官方文档 <a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline%E2%80%93NewCommandLineShell">Beeline Command Options</a></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>-u</code></td>
<td>数据库地址</td>
</tr>
<tr>
<td><code>-n</code></td>
<td>用户名</td>
</tr>
<tr>
<td><code>-p</code></td>
<td>密码</td>
</tr>
<tr>
<td>-d</td>
<td>驱动 (可选)</td>
</tr>
<tr>
<td><code>-e</code>*</td>
<td>执行 SQL 命令</td>
</tr>
<tr>
<td><code>-f</code>*</td>
<td>执行 SQL 脚本</td>
</tr>
<tr>
<td><code>-i (or)--init</code></td>
<td>在进入交互模式之前运行初始化脚本</td>
</tr>
<tr>
<td><code>--property-file</code></td>
<td>指定配置文件</td>
</tr>
<tr>
<td><code>--hiveconf property=value</code></td>
<td>指定配置属性</td>
</tr>
<tr>
<td><code>--hivevar name=value</code></td>
<td>用户自定义属性，在会话级别有效</td>
</tr>
</tbody></table>
<p>示例： 使用用户名和密码连接 Hive</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beeline -u jdbc:hive2://localhost:10000  -n username -p password</span><br></pre></td></tr></table></figure>

<h2 id="Hive-命令"><a href="#Hive-命令" class="headerlink" title="Hive 命令"></a>Hive 命令</h2><h3 id="Help"><a href="#Help" class="headerlink" title="Help"></a>Help</h3><p>使用 <code>hive -H</code> 或者 <code>hive --help</code> 命令可以查看所有命令的帮助，显示如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">usage: hive</span><br><span class="line"> -d,--define &lt;key=value&gt;          Variable subsitution to apply to hive</span><br><span class="line">                                  commands. e.g. -d A=B or --define A=B  --定义用户自定义变量</span><br><span class="line">    --database &lt;databasename&gt;     Specify the database to use  -- 指定使用的数据库</span><br><span class="line"> -e &lt;quoted-query-string&gt;         SQL from command line   -- 执行指定的 SQL</span><br><span class="line"> -f &lt;filename&gt;                    SQL from files   --执行 SQL 脚本</span><br><span class="line"> -H,--help                        Print help information  -- 打印帮助信息</span><br><span class="line">    --hiveconf &lt;property=value&gt;   Use value for given property    --自定义配置</span><br><span class="line">    --hivevar &lt;key=value&gt;         Variable subsitution to apply to hive  --自定义变量</span><br><span class="line">                                  commands. e.g. --hivevar A=B</span><br><span class="line"> -i &lt;filename&gt;                    Initialization SQL file  --在进入交互模式之前运行初始化脚本</span><br><span class="line"> -S,--silent                      Silent mode in interactive shell    --静默模式</span><br><span class="line"> -v,--verbose                     Verbose mode (echo executed SQL to the  console)  --详细模式</span><br></pre></td></tr></table></figure>

<h3 id="交互式命令行"><a href="#交互式命令行" class="headerlink" title="交互式命令行"></a>交互式命令行</h3><p>直接使用 <code>Hive</code> 命令，不加任何参数，即可进入交互式命令行。</p>
<h3 id="执行-SQL-命令"><a href="#执行-SQL-命令" class="headerlink" title="执行 SQL 命令"></a>执行 SQL 命令</h3><p>在不进入交互式命令行的情况下，可以使用 <code>hive -e</code>执行 SQL 命令。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive -e &#x27;select * from emp&#x27;;</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/hive-e.png"><img src="https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hive-e.png" alt="img"></a></p>
<h3 id="执行-SQL-脚本"><a href="#执行-SQL-脚本" class="headerlink" title="执行 SQL 脚本"></a>执行 SQL 脚本</h3><p>用于执行的 sql 脚本可以在本地文件系统，也可以在 HDFS 上。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">本地文件系统</span></span><br><span class="line">hive -f /usr/file/simple.sql;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">HDFS文件系统</span></span><br><span class="line">hive -f hdfs://hadoop001:8020/tmp/simple.sql;</span><br></pre></td></tr></table></figure>

<p>其中 <code>simple.sql</code> 内容如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>

<h3 id="配置-Hive-变量"><a href="#配置-Hive-变量" class="headerlink" title="配置 Hive 变量"></a>配置 Hive 变量</h3><p>可以使用 <code>--hiveconf</code> 设置 Hive 运行时的变量。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive -e &#x27;select * from emp&#x27; \</span><br><span class="line">--hiveconf hive.exec.scratchdir=/tmp/hive_scratch  \</span><br><span class="line">--hiveconf mapred.reduce.tasks=4;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>hive.exec.scratchdir：指定 HDFS 上目录位置，用于存储不同 map&#x2F;reduce 阶段的执行计划和这些阶段的中间输出结果。</p>
</blockquote>
<h3 id="配置文件启动"><a href="#配置文件启动" class="headerlink" title="配置文件启动"></a>配置文件启动</h3><p>使用 <code>-i</code> 可以在进入交互模式之前运行初始化脚本，相当于指定配置文件启动。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive -i /usr/file/hive-init.conf;</span><br></pre></td></tr></table></figure>

<p>其中 <code>hive-init.conf</code> 的内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.mode.local.auto = true;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>hive.exec.mode.local.auto 默认值为 false，这里设置为 true ，代表开启本地模式。</p>
</blockquote>
<h3 id="用户自定义变量"><a href="#用户自定义变量" class="headerlink" title="用户自定义变量"></a>用户自定义变量</h3><p><code>--define</code>和 <code>--hivevar</code>在功能上是等价的，都是用来实现自定义变量，这里给出一个示例:</p>
<p>定义变量：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive  --define  n=ename --hiveconf  --hivevar j=job;</span><br></pre></td></tr></table></figure>

<p>在查询中引用自定义变量：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">以下两条语句等价</span></span><br><span class="line">hive &gt; select $&#123;n&#125; from emp;</span><br><span class="line">hive &gt; select $&#123;hivevar:n&#125; from emp;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">以下两条语句等价</span></span><br><span class="line">hive &gt; select $&#123;j&#125; from emp;</span><br><span class="line">hive &gt; select $&#123;hivevar:j&#125; from emp;</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/hive-n-j.png"><img src="https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hive-n-j.png" alt="img"></a></p>
<h2 id="Hive-配置"><a href="#Hive-配置" class="headerlink" title="Hive 配置"></a>Hive 配置</h2><p>可以通过三种方式对 Hive 的相关属性进行配置，分别介绍如下：</p>
<h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>方式一为使用配置文件，使用配置文件指定的配置是永久有效的。Hive 有以下三个可选的配置文件：</p>
<ul>
<li><code>hive-site.xml</code> - Hive 的主要配置文件；</li>
<li><code>hivemetastore-site.xml</code> - 关于元数据的配置；</li>
<li><code>hiveserver2-site.xml</code> - 关于 HiveServer2 的配置。</li>
</ul>
<p>示例如下,在 hive-site.xml 配置 <code>hive.exec.scratchdir</code>：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/tmp/mydir<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Scratch space for Hive jobs<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="hiveconf"><a href="#hiveconf" class="headerlink" title="hiveconf"></a>hiveconf</h3><p>方式二为在启动命令行 (Hive CLI &#x2F; Beeline) 的时候使用 <code>--hiveconf</code> 指定配置，这种方式指定的配置作用于整个 Session。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --hiveconf hive.exec.scratchdir=/tmp/mydir</span><br></pre></td></tr></table></figure>

<h3 id="set"><a href="#set" class="headerlink" title="set"></a>set</h3><p>方式三为在交互式环境下 (Hive CLI &#x2F; Beeline)，使用 set 命令指定。这种设置的作用范围也是 Session 级别的，配置对于执行该命令后的所有命令生效。set 兼具设置参数和查看参数的功能。如下：</p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000&gt; set hive.exec.scratchdir=/tmp/mydir;</span><br><span class="line">No rows affected (0.025 seconds)</span><br><span class="line"><span class="section">0: jdbc:hive2://hadoop001:10000&gt; set hive.exec.scratchdir;</span></span><br><span class="line"><span class="section">+----------------------------------+--+</span></span><br><span class="line"><span class="section">|               set                |</span></span><br><span class="line"><span class="section">+----------------------------------+--+</span></span><br><span class="line"><span class="section">| hive.exec.scratchdir=/tmp/mydir  |</span></span><br><span class="line"><span class="section">+----------------------------------+--+</span></span><br></pre></td></tr></table></figure>

<h3 id="配置优先级"><a href="#配置优先级" class="headerlink" title="配置优先级"></a>配置优先级</h3><p>配置的优先顺序如下 (由低到高)：<br><code>hive-site.xml</code> - &gt;<code>hivemetastore-site.xml</code>- &gt; <code>hiveserver2-site.xml</code> - &gt;<code>-- hiveconf</code>- &gt; <code>set</code></p>
<h3 id="配置参数"><a href="#配置参数" class="headerlink" title="配置参数"></a>配置参数</h3><p>Hive 可选的配置参数非常多，在用到时查阅官方文档即可<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration">AdminManual Configuration</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/heibaiying/BigData-Notes">BigData-Notes</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/blog/page/32/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/blog/">1</a><span class="space">&hellip;</span><a class="page-number" href="/blog/page/32/">32</a><span class="page-number current">33</span><a class="page-number" href="/blog/page/34/">34</a><span class="space">&hellip;</span><a class="page-number" href="/blog/page/51/">51</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/blog/page/34/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2015 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">钝悟 ◾ Dunwu</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">4.5m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">68:08</span>
  </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/dunwu/blog" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '64px',
  right: 'unset',
  left: '32px',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.8.0/dist/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"dunwu","repo":"blog","client_id":"c45bc13ca1d3d3aa4836","client_secret":"1907a9f0c22087badad3938e1d7dcba9078f88ac","admin_user":"dunwu","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdn.jsdelivr.net/npm/gitalk@1.8.0/dist/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"f0e6ba1eaf9ad58cc7d1dd36428ea52b"}</script>
<script src="/blog/js/third-party/comments/gitalk.js" defer></script>

</body>
</html>
