<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/uploads/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/uploads/favicon.ico">
  <link rel="mask-icon" href="/blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blog/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous" defer></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"dunwu.github.io","root":"/blog/","images":"/blog/images","scheme":"Pisces","darkmode":true,"version":"8.25.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"atom-one-light","dark":"atom-one-dark"},"prism":{"light":"atom-one-light","dark":"atom-one-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":true,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/blog/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false,"trigger":"auto"}}</script><script src="/blog/js/config.js" defer></script>

    <meta name="description" content="钝悟的个人博客">
<meta property="og:type" content="website">
<meta property="og:title" content="Dunwu Blog">
<meta property="og:url" content="https://dunwu.github.io/blog/page/30/index.html">
<meta property="og:site_name" content="Dunwu Blog">
<meta property="og:description" content="钝悟的个人博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="钝悟 ◾ Dunwu">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://dunwu.github.io/blog/page/30/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/30/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Dunwu Blog</title>
  








  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/blog/js/utils.js" defer></script><script src="/blog/js/motion.js" defer></script><script src="/blog/js/sidebar.js" defer></script><script src="/blog/js/next-boot.js" defer></script>

  <script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.5.0/dist/search.js" integrity="sha256-xFC6PJ82SL9b3WkGjFavNiA9gm5z6UBxWPiu4CYjptg=" crossorigin="anonymous" defer></script>
<script src="/blog/js/third-party/search/local-search.js" defer></script>





  <script src="/blog/js/third-party/pace.js" defer></script>


  





  <noscript>
    <link rel="stylesheet" href="/blog/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Dunwu Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">大道至简，知易行难</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/blog/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/blog/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/blog/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">428</span></a></li><li class="menu-item menu-item-categories"><a href="/blog/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">124</span></a></li><li class="menu-item menu-item-archives"><a href="/blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">508</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="钝悟 ◾ Dunwu"
      src="/blog/uploads/avatar.gif">
  <p class="site-author-name" itemprop="name">钝悟 ◾ Dunwu</p>
  <div class="site-description" itemprop="description">钝悟的个人博客</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/blog/archives/">
          <span class="site-state-item-count">508</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/blog/categories/">
        <span class="site-state-item-count">124</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/blog/tags/">
        <span class="site-state-item-count">428</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/dunwu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;dunwu" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:forbreak@163.com" title="E-Mail → mailto:forbreak@163.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdn.jsdelivr.net/npm/@creativecommons/vocabulary@2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/a57cb309/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/a57cb309/" class="post-title-link" itemprop="url">Redis 持久化</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-06-24 10:45:38" itemprop="dateCreated datePublished" datetime="2020-06-24T10:45:38+08:00">2020-06-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">数据库</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/KV%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">KV数据库</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/KV%E6%95%B0%E6%8D%AE%E5%BA%93/redis/" itemprop="url" rel="index"><span itemprop="name">redis</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Redis-持久化"><a href="#Redis-持久化" class="headerlink" title="Redis 持久化"></a>Redis 持久化</h1><blockquote>
<p>Redis 是内存型数据库，为了保证数据在宕机后不会丢失，需要将内存中的数据持久化到硬盘上。</p>
<p>Redis 支持两种持久化方式：RDB 和 AOF。这两种持久化方式既可以同时使用，也可以单独使用。</p>
<p>关键词：<code>RDB</code>、<code>AOF</code>、<code>SAVE</code>、<code>BGSAVE</code>、<code>appendfsync</code></p>
</blockquote>
<h2 id="RDB-快照"><a href="#RDB-快照" class="headerlink" title="RDB 快照"></a>RDB 快照</h2><h3 id="RDB-简介"><a href="#RDB-简介" class="headerlink" title="RDB 简介"></a>RDB 简介</h3><p><strong>RDB 即“快照”，它将某时刻的所有 Redis 数据库中的所有键值对数据保存到一个经过压缩的“二进制文件”（RDB 文件）中</strong>。</p>
<p><strong>RDB 持久化即可以“手动”执行，也可以定期“自动”执行</strong>。</p>
<p><strong>RDB 文件的“载入”工作是在服务器“启动”时“自动”执行的</strong>。</p>
<p>对于不同类型的键值对， RDB 文件会使用不同的方式来保存它们。</p>
<p>创建 RDB 后，用户可以对 RDB 进行备份，可以将 RDB 复制到其他服务器从而创建具有相同数据的服务器副本，还可以在重启服务器时使用。一句话来说：<strong>RDB 适用于作为“冷备”</strong>。</p>
<h3 id="RDB-的优点和缺点"><a href="#RDB-的优点和缺点" class="headerlink" title="RDB 的优点和缺点"></a>RDB 的优点和缺点</h3><p><strong>RDB 的优点</strong></p>
<ul>
<li>RDB 文件非常紧凑，<strong>适合作为“冷备”</strong>。比如你可以在每个小时报保存一下过去 24 小时内的数据，同时每天保存过去 30 天的数据，这样即使出了问题你也可以根据需求恢复到不同版本的数据集。</li>
<li>快照在保存 RDB 文件时父进程唯一需要做的就是 fork 出一个子进程，接下来的工作全部由子进程来做，父进程不需要再做其他 IO 操作，所以快照持久化方式可以最大化 Redis 的性能。</li>
<li><strong>恢复大数据集时，RDB 比 AOF 更快</strong>。</li>
</ul>
<p><strong>RDB 的缺点</strong></p>
<ul>
<li><strong>如果系统发生故障，将会丢失最后一次创建快照之后的数据</strong>。如果你希望在 Redis 意外停止工作（例如电源中断）的情况下丢失的数据最少的话，那么 快照不适合你。虽然你可以配置不同的 save 时间点(例如每隔 5 分钟并且对数据集有 100 个写的操作)，是 Redis 要完整的保存整个数据集是一个比较繁重的工作，你通常会每隔 5 分钟或者更久做一次完整的保存，万一在 Redis 意外宕机，你可能会丢失几分钟的数据。</li>
<li><strong>如果数据量很大，保存快照的时间会很长</strong>。快照需要经常 fork 子进程来保存数据集到硬盘上。当数据集比较大的时候，fork 的过程是非常耗时的，可能会导致 Redis 在一些毫秒级内不能响应客户端的请求。如果数据集巨大并且 CPU 性能不是很好的情况下，这种情况会持续 1 秒。AOF 也需要 fork，但是你可以调节重写日志文件的频率来提高数据集的耐久度。</li>
</ul>
<h3 id="RDB-的创建"><a href="#RDB-的创建" class="headerlink" title="RDB 的创建"></a>RDB 的创建</h3><p>有两个 Redis 命令可以用于生成 RDB 文件：<a target="_blank" rel="noopener" href="https://redis.io/commands/save"><strong><code>SAVE</code></strong></a> 和 <a target="_blank" rel="noopener" href="https://redis.io/commands/bgsave"><strong><code>BGSAVE</code></strong></a> 。</p>
<p><a target="_blank" rel="noopener" href="https://redis.io/commands/save"><strong><code>SAVE</code></strong></a> 命令由服务器进程直接执行保存操作，直到 RDB 创建完成为止。所以<strong>该命令“会阻塞”服务器</strong>，在阻塞期间，服务器不能响应任何命令请求。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">SAVE</span></span><br><span class="line">&quot;OK&quot;</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://redis.io/commands/bgsave"><strong><code>BGSAVE</code></strong></a> 命令会<strong>“派生”</strong>（fork）一个子进程，由子进程负责创建 RDB 文件，服务器进程继续处理命令请求，所以<strong>该命令“不会阻塞”服务器</strong>。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202503272238061.png"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">BGSAVE</span></span><br><span class="line">&quot;Background saving started&quot;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>🔔 <strong>【注意】</strong></p>
<p><code>BGSAVE</code> 命令的实现采用的是写时复制技术（Copy-On-Write，缩写为 CoW）。</p>
<p><code>BGSAVE</code> 命令执行期间，<code>SAVE</code>、<code>BGSAVE</code>、<code>BGREWRITEAOF</code> 三个命令会被拒绝，以免与当前的 <code>BGSAVE</code> 操作产生竞态条件，降低性能。</p>
</blockquote>
<p>创建 RDB 的工作由 <code>rdb.c/rdbSave</code> 函数完成。</p>
<h3 id="RDB-的载入"><a href="#RDB-的载入" class="headerlink" title="RDB 的载入"></a>RDB 的载入</h3><p><strong>RDB 文件的“载入”工作是在服务器“启动”时“自动”执行的</strong>。Redis 并没有专门用于载入 RDB 文件的命令。</p>
<p>服务器载入 RDB 文件期间，会一直处于阻塞状态，直到载入完成为止。</p>
<p>载入 RDB 的工作由 <code>rdb.c/rdbLoad</code> 函数完成。</p>
<blockquote>
<p>🔔 <strong>【注意】</strong></p>
<p>因为 AOF 的更新频率通常比 RDB 的更新频率高，所以：</p>
<ul>
<li>如果服务器开了 AOF，则服务器会优先使用 AOF 来还原数据。</li>
<li>只有在 AOF 处于关闭时，服务器才会使用 RDB 来还原数据。</li>
</ul>
</blockquote>
<h3 id="自动间隔保存"><a href="#自动间隔保存" class="headerlink" title="自动间隔保存"></a>自动间隔保存</h3><p>Redis 支持通过在 <code>redis.conf</code> 文件中配置 <code>save</code> 选项，让服务器每隔一段时间自动执行一次 <code>BGSAVE</code> 命令。<code>save</code> 选项可以设置多个保存条件，只要其中任意一个条件被满足，服务器就会执行 <code>BGSAVE</code> 命令。</p>
<p>【示例】<code>redis.conf</code> 中自动保存配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">900 秒内，至少对数据库进行了 1 次修改</span></span><br><span class="line">save 900 1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">300 秒内，至少对数据库进行了 10 次修改</span></span><br><span class="line">save 300 10</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">60 秒内，至少对数据库进行了 10000 次修改</span></span><br><span class="line">save 60 10000</span><br></pre></td></tr></table></figure>

<p>只要满足以上任意条件，Redis 服务就会执行 <code>BGSAVE</code> 命令。</p>
<p>自动间隔的保存条件定义在 <code>redis.h/redisServer</code> 中：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">redisServer</span> &#123;</span></span><br><span class="line">    <span class="comment">// 记录了保存条件的数组</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">saveparam</span> *<span class="title">saveparams</span>;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 自从上次 SAVE 执行以来，数据库被修改的次数</span></span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> dirty;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 上一次完成 SAVE 的时间</span></span><br><span class="line">    <span class="type">time_t</span> lastsave;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 服务器的保存条件（BGSAVE 自动执行的条件）</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">saveparam</span> &#123;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 多少秒之内</span></span><br><span class="line">    <span class="type">time_t</span> seconds;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 发生多少次修改</span></span><br><span class="line">    <span class="type">int</span> changes;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>redisServer 中的 <code>saveparams</code> 数组维护了多个自动间隔保存条件。</p>
<p>服务每次成功执行一个修改命令后，<code>dirty</code> 计数器就会加 1；而 <code>lastsave</code> 则记录了上一次完成 SAVE 的时间。Redis 会通过一个 <code>serverCron</code> 函数周期性检查 <code>save</code> 选项所设条件是否满足，如果满足，则执行 <code>BGSVAE</code> 命令。</p>
<h3 id="RDB-的文件结构"><a href="#RDB-的文件结构" class="headerlink" title="RDB 的文件结构"></a>RDB 的文件结构</h3><p><strong>RDB 文件是一个经过压缩的“二进制文件”</strong>，由多个部分组成。</p>
<p>对于不同类型（STRING、HASH、LIST、SET、SORTED SET）的键值对，RDB 文件会使用不同的方式来保存它们。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202503272240429.png"></p>
<p>Redis 本身提供了一个 RDB 文件检查工具 <code>redis-check-dump</code>。</p>
<h3 id="RDB-的配置"><a href="#RDB-的配置" class="headerlink" title="RDB 的配置"></a>RDB 的配置</h3><p>Redis RDB 默认配置如下：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">save</span> <span class="number">900</span> <span class="number">1</span></span><br><span class="line"><span class="attribute">save</span> <span class="number">300</span> <span class="number">10</span></span><br><span class="line"><span class="attribute">save</span> <span class="number">60</span> <span class="number">10000</span></span><br><span class="line"><span class="attribute">stop</span>-writes-<span class="literal">on</span>-bgsave-error yes</span><br><span class="line"><span class="attribute">rdbcompression</span> yes</span><br><span class="line"><span class="attribute">rdbchecksum</span> yes</span><br><span class="line"><span class="attribute">dbfilename</span> dump.rdb</span><br><span class="line"><span class="attribute">dir</span> ./</span><br></pre></td></tr></table></figure>

<p>Redis 的配置文件 <code>redis.conf</code> 中与 RDB 有关的选项：</p>
<ul>
<li><p><code>save</code> - Redis 会根据 <code>save</code> 选项，让服务器每隔一段时间自动执行一次 <code>BGSAVE</code> 命令</p>
</li>
<li><p><code>stop-writes-on-bgsave-error</code> - 当 <code>BGSAVE</code> 命令出现错误时停止写 RDB 文件</p>
</li>
<li><p><code>rdbcompression</code> - RDB 文件开启压缩功能</p>
</li>
<li><p><code>rdbchecksum</code> - 对 RDB 文件进行校验</p>
</li>
<li><p><code>dbfilename</code> - RDB 文件名</p>
</li>
<li><p><code>dir</code> - RDB 文件和 AOF 文件的存储路径</p>
</li>
</ul>
<h2 id="AOF-日志"><a href="#AOF-日志" class="headerlink" title="AOF 日志"></a>AOF 日志</h2><h3 id="AOF-简介"><a href="#AOF-简介" class="headerlink" title="AOF 简介"></a>AOF 简介</h3><p><code>AOF(Append Only File)</code> 是将所有写命令追加写入“日志文件”，以此来记录数据的变化。当服务器重启时，会重新载入和执行 AOF 文件中的命令，就可以恢复原始的数据。AOF 适合作为<strong>“热备”</strong>。</p>
<p>AOF 可以通过 <code>appendonly yes</code> 配置选项来开启。</p>
<h3 id="AOF-的优点和缺点"><a href="#AOF-的优点和缺点" class="headerlink" title="AOF 的优点和缺点"></a>AOF 的优点和缺点</h3><p><strong>AOF 的优点</strong></p>
<ul>
<li><strong>如果系统发生故障，AOF 丢失数据比 RDB 少</strong>。你可以使用不同的 fsync 策略：无 fsync；每秒 fsync；每次写的时候 fsync。使用默认的每秒 fsync 策略，Redis 的性能依然很好(fsync 是由后台线程进行处理的,主线程会尽力处理客户端请求)，一旦出现故障，你最多丢失 1 秒的数据。</li>
<li><strong>AOF 文件可修复</strong> - AOF 文件是一个只进行追加的日志文件，所以不需要写入 seek，即使由于某些原因(磁盘空间已满，写的过程中宕机等等)未执行完整的写入命令，你也也可使用 redis-check-aof 工具修复这些问题。</li>
<li><strong>AOF 文件可压缩</strong>。Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写：重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。</li>
<li><strong>AOF 文件可读</strong> - AOF 文件有序地保存了对数据库执行的所有写入操作，这些写入操作以 Redis 命令的格式保存。因此 AOF 文件的内容非常容易被人读懂，对文件进行分析（parse）也很轻松。 导出（export） AOF 文件也非常简单。举个例子，如果你不小心执行了 FLUSHALL 命令，但只要 AOF 文件未被重写，那么只要停止服务器，移除 AOF 文件末尾的 FLUSHALL 命令，并重启 Redis ，就可以将数据集恢复到 FLUSHALL 执行之前的状态。</li>
</ul>
<p><strong>AOF 的缺点</strong></p>
<ul>
<li><strong>AOF 文件体积一般比 RDB 大</strong> - 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。</li>
<li><strong>恢复大数据集时，AOF 比 RDB 慢。</strong> - 根据所使用的 fsync 策略，AOF 的速度可能会慢于快照。在一般情况下，每秒 fsync 的性能依然非常高，而关闭 fsync 可以让 AOF 的速度和快照一样快，即使在高负荷之下也是如此。不过在处理巨大的写入载入时，快照可以提供更有保证的最大延迟时间（latency）。</li>
</ul>
<h3 id="AOF-的创建"><a href="#AOF-的创建" class="headerlink" title="AOF 的创建"></a>AOF 的创建</h3><p><strong>Redis 命令请求会先保存到 AOF 缓冲区，再定期写入并同步到 AOF 文件</strong>。</p>
<p>AOF 的实现可以分为命令追加（append）、文件写入、文件同步（sync）三个步骤。</p>
<ul>
<li><strong>命令追加</strong> - 当 Redis 服务器开启 AOF 功能时，服务器在执行完一个写命令后，会以 Redis 命令协议格式将被执行的写命令追加到 AOF 缓冲区的末尾。</li>
<li><strong>文件写入</strong>和<strong>文件同步</strong><ul>
<li>Redis 的服务器进程就是一个事件循环，这个循环中的文件事件负责接收客户端的命令请求，以及向客户端发送命令回复。而时间事件则负责执行想 <code>serverCron</code> 这样的定时运行的函数。</li>
<li>因为服务器在处理文件事件时可能会执行写命令，这些写命令会被追加到 AOF 缓冲区，服务器每次结束事件循环前，都会根据 <code>appendfsync</code> 选项来判断 AOF 缓冲区内容是否需要写入和同步到 AOF 文件中。</li>
</ul>
</li>
</ul>
<p><code>appendfsync</code> 不同选项决定了不同的持久化行为：</p>
<ul>
<li><strong><code>always</code></strong> - 将 AOF 缓冲区中所有内容写入并同步到 AOF 文件。这种方式是最数据最安全的，但也是性能最差的。</li>
<li><strong><code>no</code></strong> - 将 AOF 缓冲区所有内容写入到 AOF 文件，但并不对 AOF 文件进行同步，何时同步由操作系统决定。这种方式是数据最不安全的，一旦出现故障，未来得及同步的所有数据都会丢失。</li>
<li><strong><code>everysec</code></strong> - <code>appendfsync</code> 默认选项。将 AOF 缓冲区所有内容写入到 AOF 文件，如果上次同步 AOF 文件的时间距离现在超过一秒钟，那么再次对 AOF 文件进行同步，这个同步操作是有一个线程专门负责执行的。这张方式是前面两种的这种方案——性能足够好，且即使出现故障，仅丢失一秒钟内的数据。</li>
</ul>
<p><code>appendfsync</code> 选项的不同值对 AOF 持久化功能的安全性、以及 Redis 服务器的性能有很大的影响。</p>
<h3 id="AOF-的载入"><a href="#AOF-的载入" class="headerlink" title="AOF 的载入"></a>AOF 的载入</h3><p>因为 AOF 文件中包含了重建数据库所需的所有写命令，所以服务器只要载入并执行一遍 AOF 文件中保存的写命令，就可以还原服务器关闭前的数据库状态。</p>
<p>AOF 载入过程如下：</p>
<ol>
<li>服务器启动载入程序。</li>
<li>创建一个伪客户端。因为 Redis 命令只能在客户端上下文中执行，所以需要创建一个伪客户端来载入、执行 AOF 文件中记录的命令。</li>
<li>从 AOF 文件中分析并读取一条写命令。</li>
<li>使用伪客户端执行写命令。</li>
<li>循环执行步骤 3、4，直到所有写命令都被处理完毕为止。</li>
<li>载入完毕。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202503272247006.png"></p>
<h3 id="AOF-的重写"><a href="#AOF-的重写" class="headerlink" title="AOF 的重写"></a>AOF 的重写</h3><p>随着 Redis 不断运行，AOF 的体积也会不断增长，这将导致两个问题：</p>
<ul>
<li>AOF 耗尽磁盘可用空间。</li>
<li>Redis 重启后需要执行 AOF 文件记录的所有写命令来还原数据集，如果 AOF 过大，则还原操作执行的时间就会非常长。</li>
</ul>
<p>为了解决 AOF 体积膨胀问题，Redis 提供了 AOF 重写功能，来对 AOF 文件进行压缩。<strong>AOF 重写可以产生一个新的 AOF 文件，这个新的 AOF 文件和原来的 AOF 文件所保存的数据库状态一致，但体积更小</strong>。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202503272248857.png"></p>
<p>AOF 重写并非读取和分析现有 AOF 文件的内容，而是直接从数据库中读取当前的数据库状态。即<strong>从数据库中读取键的当前值，然后用一条命令去记录该键值对</strong>，以此代替之前可能存在冗余的命令。</p>
<h3 id="AOF-后台重写"><a href="#AOF-后台重写" class="headerlink" title="AOF 后台重写"></a>AOF 后台重写</h3><p>作为一种辅助性功能，显然 Redis 并不想在 AOF 重写时阻塞 Redis 服务接收其他命令。因此，Redis 决定通过 <code>BGREWRITEAOF</code> 命令创建一个子进程，然后由子进程负责对 AOF 文件进行重写，这与 <code>BGSAVE</code> 原理类似。</p>
<ul>
<li>在执行 <code>BGREWRITEAOF</code> 命令时，Redis 服务器会维护一个 AOF 重写缓冲区。当 AOF 重写子进程开始工作后，Redis 每执行完一个写命令，会同时将这个命令发送给 AOF 缓冲区和 AOF 重写缓冲区。</li>
<li>由于彼此不是在同一个进程中工作，AOF 重写不影响 AOF 写入和同步。当子进程完成创建新 AOF 文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新 AOF 文件的末尾，使得新旧两个 AOF 文件所保存的数据库状态一致。</li>
<li>最后，服务器用新的 AOF 文件替换就的 AOF 文件，以此来完成 AOF 重写操作。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202503272248959.png"></p>
<blockquote>
<p><code>BGREWRITEAOF</code> 命令的实现采用的是写时复制技术（Copy-On-Write，缩写为 CoW）。</p>
</blockquote>
<p>可以通过设置 <code>auto-aof-rewrite-percentage</code> 和 <code>auto-aof-rewrite-min-size</code>，使得 Redis 在满足条件时，自动执行 <code>BGREWRITEAOF</code>。</p>
<p>假设配置如下：</p>
<figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span>-aof-rewrite-percentage <span class="number">100</span></span><br><span class="line"><span class="keyword">auto</span>-aof-rewrite-min-size <span class="number">64</span>mb</span><br></pre></td></tr></table></figure>

<p>表明，当 AOF 大于 <code>64MB</code>，且 AOF 体积比上一次重写后的体积大了至少 <code>100%</code> 时，执行 <code>BGREWRITEAOF</code>。</p>
<h3 id="AOF-的配置"><a href="#AOF-的配置" class="headerlink" title="AOF 的配置"></a>AOF 的配置</h3><p>AOF 的默认配置：</p>
<figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">appendonly no</span><br><span class="line">appendfsync everysec</span><br><span class="line">no-appendfsync-on-<span class="built_in">rewrite</span> no</span><br><span class="line"><span class="built_in">auto</span>-aof-<span class="built_in">rewrite</span>-percentage <span class="number">100</span></span><br><span class="line"><span class="built_in">auto</span>-aof-<span class="built_in">rewrite</span>-min-size <span class="number">64</span>mb</span><br></pre></td></tr></table></figure>

<p>AOF 持久化通过在 <code>redis.conf</code> 中的 <code>appendonly yes</code> 配置选项来开启。</p>
<ul>
<li><strong><code>appendonly</code></strong> - 开启 AOF 功能。</li>
<li><strong><code>appendfilename</code></strong> - AOF 文件名。</li>
<li><strong><code>appendfsync</code></strong> - 用于设置同步频率，它有以下可选项：<ul>
<li><strong><code>always</code></strong> - 每个 Redis 写命令都要同步写入硬盘。这样做会严重降低 Redis 的速度。</li>
<li><strong><code>everysec</code></strong> - 每秒执行一次同步，显示地将多个写命令同步到硬盘。为了兼顾数据安全和写入性能，推荐使用 <code>appendfsync everysec</code> 选项。Redis 每秒同步一次 AOF 文件时的性能和不使用任何持久化特性时的性能相差无几。</li>
<li><strong><code>no</code></strong> - 让操作系统来决定应该何时进行同步。</li>
</ul>
</li>
<li><code>no-appendfsync-on-rewrite</code> - AOF 重写时不支持追加命令。</li>
<li><code>auto-aof-rewrite-percentage</code> - AOF 重写百分比。</li>
<li><code>auto-aof-rewrite-min-size</code> - AOF 重写文件的最小大小。</li>
<li><code>dir</code> - RDB 文件和 AOF 文件的存储路径。</li>
</ul>
<h2 id="RDB-和-AOF"><a href="#RDB-和-AOF" class="headerlink" title="RDB 和 AOF"></a>RDB 和 AOF</h2><blockquote>
<p>当 Redis 启动时， 如果 RDB 和 AOF 功能都开启了，那么程序会优先使用 AOF 文件来恢复数据集，因为 AOF 文件所保存的数据通常是最完整的。</p>
</blockquote>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89970b2c-fd40-47ea-bd39-18db11b69fe8_1280x1664.gif"></p>
<h3 id="如何选择持久化"><a href="#如何选择持久化" class="headerlink" title="如何选择持久化"></a>如何选择持久化</h3><ul>
<li>如果不关心数据丢失，可以不持久化。</li>
<li>如果可以承受数分钟以内的数据丢失，可以只使用 RDB。</li>
<li>如果不能承受数分钟以内的数据丢失，可以同时使用 RDB 和 AOF。</li>
</ul>
<p>有很多用户都只使用 AOF 持久化， 但并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份，并且快照恢复数据集的速度也要比 AOF 恢复的速度要快，除此之外，使用快照还可以避免之前提到的 AOF 程序的 bug 。</p>
<h3 id="RDB-切换为-AOF"><a href="#RDB-切换为-AOF" class="headerlink" title="RDB 切换为 AOF"></a>RDB 切换为 AOF</h3><p>在 Redis 2.2 或以上版本，可以在不重启的情况下，从 RDB 切换为 AOF ：</p>
<ul>
<li>为最新的 dump.rdb 文件创建一个备份。</li>
<li>将备份放到一个安全的地方。</li>
<li>执行以下两条命令:</li>
<li>redis-cli config set appendonly yes</li>
<li>redis-cli config set save</li>
<li>确保写命令会被正确地追加到 AOF 文件的末尾。</li>
<li>执行的第一条命令开启了 AOF 功能： Redis 会阻塞直到初始 AOF 文件创建完成为止， 之后 Redis 会继续处理命令请求， 并开始将写入命令追加到 AOF 文件末尾。</li>
</ul>
<p>执行的第二条命令用于关闭快照功能。 这一步是可选的， 如果你愿意的话， 也可以同时使用快照和 AOF 这两种持久化功能。</p>
<blockquote>
<p>🔔 重要：别忘了在 <code>redis.conf</code> 中打开 AOF 功能！否则的话，服务器重启之后，之前通过 CONFIG SET 设置的配置就会被遗忘，程序会按原来的配置来启动服务器。</p>
</blockquote>
<h3 id="AOF-和-RDB-的相互作用"><a href="#AOF-和-RDB-的相互作用" class="headerlink" title="AOF 和 RDB 的相互作用"></a>AOF 和 RDB 的相互作用</h3><p><code>BGSAVE</code> 和 <code>BGREWRITEAOF</code> 命令不可以同时执行。这是为了避免两个 Redis 后台进程同时对磁盘进行大量的 I&#x2F;O 操作。</p>
<p>如果 <code>BGSAVE</code> 正在执行，并且用户显示地调用 <code>BGREWRITEAOF</code> 命令，那么服务器将向用户回复一个 OK 状态，并告知用户，<code>BGREWRITEAOF</code> 已经被预定执行。一旦 <code>BGSAVE</code> 执行完毕， <code>BGREWRITEAOF</code> 就会正式开始。</p>
<h3 id="混合持久化"><a href="#混合持久化" class="headerlink" title="混合持久化"></a>混合持久化</h3><p>RDB 优点是数据恢复速度快，但是快照的频率不好把握。频率太低，丢失的数据就会比较多，频率太高，就会影响性能。AOF 优点是丢失数据少，但是数据恢复不快。</p>
<p>为了集成了两者的优点，Redis 4.0 提出了<strong>混合使用 AOF 日志和内存快照</strong>，也叫混合持久化，既保证了 Redis 重启速度，又降低数据丢失风险。</p>
<p>混合持久化工作在 <strong>AOF 日志重写过程</strong>，当开启了混合持久化时，在 AOF 重写日志时，fork 出来的重写子进程会先将与主线程共享的内存数据以 RDB 方式写入到 AOF 文件，然后主线程处理的操作命令会被记录在重写缓冲区里，重写缓冲区里的增量命令会以 AOF 方式写入到 AOF 文件，写入完成后通知主进程将新的含有 RDB 格式和 AOF 格式的 AOF 文件替换旧的的 AOF 文件。</p>
<p>也就是说，使用了混合持久化，AOF 文件的<strong>前半部分是 RDB 格式的全量数据，后半部分是 AOF 格式的增量数据</strong>。</p>
<p>这样的好处在于，重启 Redis 加载数据的时候，由于前半部分是 RDB 内容，这样<strong>加载的时候速度会很快</strong>。</p>
<p>加载完 RDB 的内容后，才会加载后半部分的 AOF 内容，这里的内容是 Redis 后台子进程重写 AOF 期间，主线程处理的操作命令，可以使得<strong>数据更少的丢失</strong>。</p>
<p><strong>混合持久化优点</strong>：</p>
<ul>
<li>混合持久化结合了 RDB 和 AOF 持久化的优点，开头为 RDB 的格式，使得 Redis 可以更快的启动，同时结合 AOF 的优点，有减低了大量数据丢失的风险。</li>
</ul>
<p><strong>混合持久化缺点</strong>：</p>
<ul>
<li>AOF 文件中添加了 RDB 格式的内容，使得 AOF 文件的可读性变得很差；</li>
<li>兼容性差，如果开启混合持久化，那么此混合持久化 AOF 文件，就不能用在 Redis 4.0 之前版本了。</li>
</ul>
<h2 id="Redis-备份"><a href="#Redis-备份" class="headerlink" title="Redis 备份"></a>Redis 备份</h2><p>应该确保 Redis 数据有完整的备份。</p>
<p>备份 Redis 数据建议采用 RDB。</p>
<h3 id="备份过程"><a href="#备份过程" class="headerlink" title="备份过程"></a>备份过程</h3><ol>
<li>创建一个定期任务（cron job），每小时将一个 RDB 文件备份到一个文件夹，并且每天将一个 RDB 文件备份到另一个文件夹。</li>
<li>确保快照的备份都带有相应的日期和时间信息，每次执行定期任务脚本时，使用 find 命令来删除过期的快照：比如说，你可以保留最近 48 小时内的每小时快照，还可以保留最近一两个月的每日快照。</li>
<li>至少每天一次，将 RDB 备份到你的数据中心之外，或者至少是备份到你运行 Redis 服务器的物理机器之外。</li>
</ol>
<h3 id="容灾备份"><a href="#容灾备份" class="headerlink" title="容灾备份"></a>容灾备份</h3><p>Redis 的容灾备份基本上就是对数据进行备份，并将这些备份传送到多个不同的外部数据中心。</p>
<p>容灾备份可以在 Redis 运行并产生快照的主数据中心发生严重的问题时，仍然让数据处于安全状态。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://item.jd.com/11486101.html">《Redis 设计与实现》</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/76331397/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/76331397/" class="post-title-link" itemprop="url">Redis 基本数据类型</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-06-24 10:45:38" itemprop="dateCreated datePublished" datetime="2020-06-24T10:45:38+08:00">2020-06-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">数据库</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/KV%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">KV数据库</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/KV%E6%95%B0%E6%8D%AE%E5%BA%93/redis/" itemprop="url" rel="index"><span itemprop="name">redis</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>18k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>17 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Redis-基本数据类型"><a href="#Redis-基本数据类型" class="headerlink" title="Redis 基本数据类型"></a>Redis 基本数据类型</h1><blockquote>
<p>关键词：<code>String</code>、<code>Hash</code>、<code>List</code>、<code>Set</code>、<code>Zset</code></p>
</blockquote>
<p>Redis 提供了多种数据类型，每种数据类型有丰富的命令支持。</p>
<p>Redis 支持的基本数据类型：STRING、HASH、LIST、SET、ZSET</p>
<p>Redis 支持的高级数据类型：BitMap、HyperLogLog、GEO、Stream</p>
<p>使用 Redis ，不仅要了解其数据类型的特性，还需要根据业务场景，灵活的、高效的使用其数据类型来建模。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202309232155082.png"></p>
<h2 id="String"><a href="#String" class="headerlink" title="String"></a>String</h2><h3 id="String-简介"><a href="#String-简介" class="headerlink" title="String 简介"></a>String 简介</h3><p>String 类型是键值对结构。</p>
<p>String 类型是<strong>二进制安全</strong>的。二进制安全是指，String 类型不仅可以保存文本数据，还可以保存任意格式的二进制数据，如：图片、音频、视频、压缩文件等。</p>
<p>默认情况下，String 类型的值最大可为 <strong>512 MB</strong>。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/cs/database/redis/redis-datatype-string.png"></p>
<h3 id="String-实现"><a href="#String-实现" class="headerlink" title="String 实现"></a>String 实现</h3><p>String 类型的底层的数据结构实现主要是 int 和 SDS（简单动态字符串）。</p>
<p>SDS 和我们认识的 C 字符串不太一样，之所以没有使用 C 语言的字符串表示，因为 SDS 相比于 C 的原生字符串：</p>
<ul>
<li><strong>SDS 不仅可以保存文本数据，还可以保存二进制数据</strong>。因为 <code>SDS</code> 使用 <code>len</code> 属性的值而不是空字符来判断字符串是否结束，并且 SDS 的所有 API 都会以处理二进制的方式来处理 SDS 存放在 <code>buf[]</code> 数组里的数据。所以 SDS 不光能存放文本数据，而且能保存图片、音频、视频、压缩文件这样的二进制数据。</li>
<li>**SDS 获取字符串长度的时间复杂度是 O(1)**。因为 C 语言的字符串并不记录自身长度，所以获取长度的复杂度为 O(n)；而 SDS 结构里用 <code>len</code> 属性记录了字符串长度，所以复杂度为 <code>O(1)</code>。</li>
<li><strong>Redis 的 SDS API 是安全的，拼接字符串不会造成缓冲区溢出</strong>。因为 SDS 在拼接字符串之前会检查 SDS 空间是否满足要求，如果空间不够会自动扩容，所以不会导致缓冲区溢出的问题。</li>
</ul>
<p><strong>字符串对象的编码可以是 <code>int</code> 、 <code>raw</code> 或者 <code>embstr</code></strong> 。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202410100759580.svg"></p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202410100759674.svg"></p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202410100800212.svg"></p>
<p>字符串对象保存各类型值的编码方式：</p>
<table>
<thead>
<tr>
<th align="left">值</th>
<th align="left">编码</th>
</tr>
</thead>
<tbody><tr>
<td align="left">可以用 <code>long</code> 类型保存的整数。</td>
<td align="left"><code>int</code></td>
</tr>
<tr>
<td align="left">可以用 <code>long double</code> 类型保存的浮点数。</td>
<td align="left"><code>embstr</code> 或者 <code>raw</code></td>
</tr>
<tr>
<td align="left">字符串值， 或者因为长度太大而没办法用 <code>long</code> 类型表示的整数， 又或者因为长度太大而没办法用 <code>long double</code> 类型表示的浮点数。</td>
<td align="left"><code>embstr</code> 或者 <code>raw</code></td>
</tr>
</tbody></table>
<p>如果一个字符串对象保存的是整数值， 并且这个整数值可以用 <code>long</code> 类型来表示， 那么字符串对象会将整数值保存在字符串对象结构的 <code>ptr</code> 属性里面（将 <code>void*</code> 转换成 <code>long</code> ）， 并将字符串对象的编码设置为 <code>int</code> 。</p>
<p>【示例】set 整数值</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SET number 10086</span></span><br><span class="line">OK</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">OBJECT ENCODING number</span></span><br><span class="line">&quot;int&quot;</span><br></pre></td></tr></table></figure>

<p>如果字符串对象保存的是一个字符串值， 并且这个字符串值的长度大于 <code>39</code> 字节， 那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串值， 并将对象的编码设置为 <code>raw</code> 。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt; SET story <span class="string">&quot;Long, long, long ago there lived a king ...&quot;</span></span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">&gt; STRLEN <span class="title function_">story</span></span><br><span class="line"><span class="params">(integer)</span> 43</span><br><span class="line"></span><br><span class="line">&gt; OBJECT ENCODING story</span><br><span class="line">&quot;raw&quot;</span><br></pre></td></tr></table></figure>

<p>如果字符串对象保存的是一个字符串值， 并且这个字符串值的长度小于等于 <code>39</code> 字节， 那么字符串对象将使用 <code>embstr</code> 编码的方式来保存这个字符串值。<code>embstr</code> 编码是专门用于保存短字符串的一种优化编码方式。</p>
<p>【示例】set 字符串值</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; SET msg <span class="string">&quot;hello&quot;</span></span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">&gt; OBJECT ENCODING msg</span><br><span class="line"><span class="string">&quot;embstr&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="String-命令"><a href="#String-命令" class="headerlink" title="String 命令"></a>String 命令</h3><table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>SET</code></td>
<td>存储一个字符串值</td>
</tr>
<tr>
<td><code>SETNX</code></td>
<td>仅当键不存在时，才存储字符串值</td>
</tr>
<tr>
<td><code>GET</code></td>
<td>获取指定 key 的值</td>
</tr>
<tr>
<td><code>MGET</code></td>
<td>获取一个或多个指定 key 的值</td>
</tr>
<tr>
<td><code>INCRBY</code></td>
<td>将 key 中储存的数字加上指定的增量值</td>
</tr>
<tr>
<td><code>DECRBY</code></td>
<td>将 key 中储存的数字减去指定的减量值</td>
</tr>
</tbody></table>
<blockquote>
<p>更多命令请参考：<a target="_blank" rel="noopener" href="https://redis.io/commands#string">Redis String 类型官方命令文档</a></p>
</blockquote>
<p>【示例】SET、GET、DEL 操作</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将 key(name) 的 value 保存为 dunwu</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="built_in">set</span> name dunwu</span></span><br><span class="line">OK</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取 key(name) 的 value</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">get name</span></span><br><span class="line">&quot;dunwu&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将 key(name) 的 value 保存为 unknown（覆盖原 value）</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="built_in">set</span> name unknown</span></span><br><span class="line">OK</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">get name</span></span><br><span class="line">&quot;unknown&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">检查 key(name) 是否存在</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">exists name</span></span><br><span class="line">(integer) 1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">删除 key(name)</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">del name</span></span><br><span class="line">(integer) 1</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">exists name</span></span><br><span class="line">(integer) 0</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">get name</span></span><br><span class="line">(nil)</span><br></pre></td></tr></table></figure>

<p>【示例】SETNX 操作</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">检查 key(lock) 是否存在</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">exists lock</span></span><br><span class="line">(integer) 0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将 key(lock) 的 value 保存为 1，保存成功</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">setnx lock 1</span></span><br><span class="line">(integer) 1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将 key(lock) 的 value 保存为 2，由于 key 已存在，保存失败</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">setnx lock 2</span></span><br><span class="line">(integer) 0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取 key(lock) 的 value</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">get lock</span></span><br><span class="line">&quot;1&quot;</span><br></pre></td></tr></table></figure>

<p>【示例】MSET、MGET 操作</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">批量设置 one、two、three 这 3 个 key</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">mset one 1 tow 2 three 3</span></span><br><span class="line">OK</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">批量获取 one、two、three 3 个 key 的 value</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">mget one tow three</span></span><br><span class="line">1) &quot;1&quot;</span><br><span class="line">2) &quot;2&quot;</span><br><span class="line">3) &quot;3&quot;</span><br></pre></td></tr></table></figure>

<p>【示例】INCR、DECR、INCRBY、DECRBY 操作</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将 key(counter) 的 value 保存为 0</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="built_in">set</span> counter 0</span></span><br><span class="line">OK</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将 key(counter) 的 value 加 1</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">incr counter</span></span><br><span class="line">(integer) 1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将 key(counter) 的 value 加 9</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">incrby counter 9</span></span><br><span class="line">(integer) 10</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将 key(counter) 的 value 减 1</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">decr counter</span></span><br><span class="line">(integer) 9</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将 key(counter) 的 value 减 9</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">decrby counter 9</span></span><br><span class="line">(integer) 0</span><br></pre></td></tr></table></figure>

<h3 id="String-应用"><a href="#String-应用" class="headerlink" title="String 应用"></a>String 应用</h3><p><strong>适用场景：缓存、计数器、共享 Session</strong></p>
<h4 id="缓存对象"><a href="#缓存对象" class="headerlink" title="缓存对象"></a>缓存对象</h4><p>使用 String 来缓存对象有两种方式：</p>
<p>（1）缓存对象的 JSON 值</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="built_in">set</span> user:1 &#123;<span class="string">&quot;name&quot;</span>:<span class="string">&quot;dunwu&quot;</span>,<span class="string">&quot;sex&quot;</span>:<span class="string">&quot;man&quot;</span>&#125;</span></span><br></pre></td></tr></table></figure>

<p>（2）将 key 分离为 user:ID: 属性的形式，采用 MSET 存储，用 MGET 获取各属性值</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">mset user:1:name dunwu user:1:sex man</span></span><br><span class="line">OK</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">mget user:1:name user:1:sex</span></span><br><span class="line">1) &quot;dunwu&quot;</span><br><span class="line">2) &quot;man&quot;</span><br></pre></td></tr></table></figure>

<h4 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h4><p>【需求场景】</p>
<p>统计网站某内容的点击量、收藏量、点赞数等等。</p>
<p>【解决方案】</p>
<blockquote>
<p>使用 Redis 的 String 类型存储一个计数器。</p>
</blockquote>
<p>维护计数器的常见操作如下：</p>
<ul>
<li>增加统计值 - 使用 <code>INCR</code>、<code>DECR</code> 命令</li>
<li>减少统计值 - 使用 <code>INCRBY</code>、<code>DECRBY</code> 操作</li>
</ul>
<p>【示例代码】</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">初始化 ID 为 1024 的博文访问量为 0</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="built_in">set</span> blog:view:1024 0</span></span><br><span class="line">OK</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ID 为 1024 的博文访问量加 1</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">incr blog:view:1024</span></span><br><span class="line">(integer) 1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ID 为 1024 的博文访问量加 1</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">incr blog:view:1024</span></span><br><span class="line">(integer) 2</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看 ID 为 1024 的博文访问量</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">get blog:view:1024</span></span><br><span class="line">&quot;2&quot;</span><br></pre></td></tr></table></figure>

<h4 id="分布式锁"><a href="#分布式锁" class="headerlink" title="分布式锁"></a>分布式锁</h4><p>（1）申请锁</p>
<p>SET 命令有个 NX 参数可以实现“key 不存在才插入”，可以用它来实现分布式锁：</p>
<ul>
<li>如果 key 不存在，则显示插入成功，可以用来表示加锁成功；</li>
<li>如果 key 存在，则会显示插入失败，可以用来表示加锁失败。</li>
</ul>
<p>一般而言，还会对分布式锁加上过期时间，分布式锁的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET key value NX PX 30000</span><br></pre></td></tr></table></figure>

<ul>
<li>key - 就是分布式锁的关键字；</li>
<li>value - 是客户端生成的唯一的标识；</li>
<li>NX - 表示只有 <code>key</code> 不存在的时候才会设置成功。（如果此时 redis 中存在这个 key，那么设置失败，返回 <code>nil</code>）</li>
<li>PX 30000 - 表示：30s 后，key 会被删除（这意味着锁被释放了）。设置过期时间，是为了防止出现各种意外，导致锁始终无法释放的情况。</li>
</ul>
<p>（2）释放锁</p>
<p>释放锁就是删除 key ，但是一般可以用 <code>lua</code> 脚本删除，判断 value 一样才删除，这是为了保证释放锁操作和申请所操作是同一个客户端。由于涉及两个操作，为了保证原子性，可以使用 lua 脚本来实现，因为 Redis 执行 Lua 脚本时，是以原子性方式执行的。</p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 删除锁的时候，找到 key 对应的 value，跟自己传过去的 value 做比较，如果是一样的才删除。</span></span><br><span class="line"><span class="keyword">if</span> redis.call(<span class="string">&quot;get&quot;</span>,KEYS[<span class="number">1</span>]) == ARGV[<span class="number">1</span>] <span class="keyword">then</span></span><br><span class="line">    <span class="keyword">return</span> redis.call(<span class="string">&quot;del&quot;</span>,KEYS[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>

<h4 id="共享-Session-信息"><a href="#共享-Session-信息" class="headerlink" title="共享 Session 信息"></a>共享 Session 信息</h4><p>在分布式场景下，一个用户的 Session 如果只存储在一个服务器上，那么当负载均衡器把用户的下一个请求转发到另一个服务器上，该服务器没有用户的 Session，就可能导致用户需要重新进行登录等操作。</p>
<p>分布式 Session 的几种实现策略：</p>
<ol>
<li>粘性 session</li>
<li>应用服务器间的 session 复制共享</li>
<li>基于缓存的 session 共享 ✅</li>
</ol>
<p>基于缓存的 session 共享实现</p>
<blockquote>
<p><strong>使用一个单独的存储服务器存储 Session 数据</strong>，可以存在 MySQL 数据库上，也可以存在 Redis 或者 Memcached 这种内存型数据库。</p>
<p>缺点：需要去实现存取 Session 的代码。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/cs/design/architecture/MultiNode-SpringSession.jpg"></p>
<h2 id="Hash"><a href="#Hash" class="headerlink" title="Hash"></a>Hash</h2><h3 id="Hash-简介"><a href="#Hash-简介" class="headerlink" title="Hash 简介"></a>Hash 简介</h3><p><img src="https://raw.githubusercontent.com/dunwu/images/master/cs/database/redis/redis-datatype-hash.png"></p>
<p>Hash 是一个键值对（key - value）集合，其中 value 的形式如： <code>value=[&#123;field1，value1&#125;，...&#123;fieldN，valueN&#125;]</code>。Hash 特别适合用于存储对象。</p>
<h3 id="Hash-实现"><a href="#Hash-实现" class="headerlink" title="Hash 实现"></a>Hash 实现</h3><p>哈希对象的编码可以是 <code>ziplist</code> 或者 <code>hashtable</code> 。</p>
<p><code>ziplist</code> 编码的哈希对象使用压缩列表作为底层实现，每当有新的键值对要加入到哈希对象时， 程序会先将保存了键的压缩列表节点推入到压缩列表表尾， 然后再将保存了值的压缩列表节点推入到压缩列表表尾。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202410100803215.svg"></p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202410100804441.svg"></p>
<p><code>hashtable</code> 编码的哈希对象使用字典作为底层实现， 哈希对象中的每个键值对都使用一个字典键值对来保存。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202410100805761.svg"></p>
<p>当哈希对象同时满足以下两个条件时， 使用 <code>ziplist</code> 编码；否则，使用 <code>hashtable</code> 编码。</p>
<ol>
<li>哈希对象保存的所有键值对的键和值的字符串长度都小于 <code>64</code> 字节（可由 <code>hash-max-ziplist-value</code> 配置）；</li>
<li>哈希对象保存的键值对数量小于 <code>512</code> 个（可由 <code>hash-max-ziplist-entries</code> 配置）；</li>
</ol>
<blockquote>
<p>注意：这两个条件的上限值是可以修改的， 具体请看配置文件中关于 <code>hash-max-ziplist-value</code> 选项和 <code>hash-max-ziplist-entries</code> 选项的说明。</p>
</blockquote>
<h3 id="Hash-命令"><a href="#Hash-命令" class="headerlink" title="Hash 命令"></a>Hash 命令</h3><table>
<thead>
<tr>
<th>命令</th>
<th>行为</th>
</tr>
</thead>
<tbody><tr>
<td><code>HSET</code></td>
<td>将指定字段的值设为 value</td>
</tr>
<tr>
<td><code>HGET</code></td>
<td>获取指定字段的值</td>
</tr>
<tr>
<td><code>HGETALL</code></td>
<td>获取所有键值对</td>
</tr>
<tr>
<td><code>HMSET</code></td>
<td>设置多个键值对</td>
</tr>
<tr>
<td><code>HMGET</code></td>
<td>获取所有指定字段的值</td>
</tr>
<tr>
<td><code>HDEL</code></td>
<td>删除指定字段</td>
</tr>
<tr>
<td><code>HINCRBY</code></td>
<td>为指定字段的整数值加上增量</td>
</tr>
<tr>
<td><code>HKEYS</code></td>
<td>获取所有字段</td>
</tr>
</tbody></table>
<blockquote>
<p>更多命令请参考：<a target="_blank" rel="noopener" href="https://redis.io/commands#hash">Redis Hash 类型官方命令文档</a></p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">存储一个哈希表 key 的键值</span></span><br><span class="line">HSET key field value</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取哈希表 key 对应的 field 键值</span></span><br><span class="line">HGET key field</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在一个哈希表 key 中存储多个键值对</span></span><br><span class="line">HMSET key field value [field value...]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">批量获取哈希表 key 中多个 field 键值</span></span><br><span class="line">HMGET key field [field ...]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">删除哈希表 key 中的 field 键值</span></span><br><span class="line">HDEL key field [field ...]</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">返回哈希表 key 中 field 的数量</span></span><br><span class="line">HLEN key</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">返回哈希表 key 中所有的键值</span></span><br><span class="line">HGETALL key</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">为哈希表 key 中 field 键的值加上增量 n</span></span><br><span class="line">HINCRBY key field n</span><br></pre></td></tr></table></figure>

<h3 id="Hash-应用"><a href="#Hash-应用" class="headerlink" title="Hash 应用"></a>Hash 应用</h3><blockquote>
<p><strong>Hash 类型适用于存储结构化数据</strong>。</p>
</blockquote>
<h4 id="缓存对象-1"><a href="#缓存对象-1" class="headerlink" title="缓存对象"></a>缓存对象</h4><p>Hash 类型的（key，field，value）的结构与对象的（对象 id，属性，值）的结构相似，也可以用来存储对象。</p>
<p>我们以用户信息为例，它在关系型数据库中的结构是这样的：</p>
<p>我们可以使用如下命令，将用户对象的信息存储到 Hash 类型：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">存储一个哈希表 uid:1 的键值</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">HMSET uid:1 name Tom age 15</span></span><br><span class="line">2</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">存储一个哈希表 uid:2 的键值</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">HMSET uid:2 name Jerry age 13</span></span><br><span class="line">2</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取哈希表用户 <span class="built_in">id</span> 为 1 中所有的键值</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">HGETALL uid:1</span></span><br><span class="line">1) &quot;name&quot;</span><br><span class="line">2) &quot;Tom&quot;</span><br><span class="line">3) &quot;age&quot;</span><br><span class="line">4) &quot;15&quot;</span><br></pre></td></tr></table></figure>

<p>Redis Hash 存储其结构如下图：</p>
<p>在介绍 String 类型的应用场景时有所介绍，String + Json 也是存储对象的一种方式，那么存储对象时，到底用 String + json 还是用 Hash 呢？</p>
<p>一般对象用 String + Json 存储，对象中某些频繁变化的属性可以考虑抽出来用 Hash 类型存储。</p>
<h4 id="购物车"><a href="#购物车" class="headerlink" title="购物车"></a>购物车</h4><p>【需求场景】</p>
<p>用户浏览电商平台，添加商品到购物车，并支持查看购物车。需要考虑未登录的情况。</p>
<p>【解决方案】</p>
<blockquote>
<p>可以使用 HASH 类型来实现购物车功能。</p>
<p>以用户 session 为 key，存储了商品 ID 和商品数量的映射。其中，商品 id 为 field，商品数量为 value。</p>
<p>为什么不使用用户 ID？</p>
<p>因为很多场景下需要支持用户在免登陆的情况下使用购物车的，因为未登录，所以无法知道用户的用户 ID，这种情况下使用用户 session 更合适。并且由于绑定的是 session，可以在清空 session 时，顺便清空购物车缓存，更加方便。</p>
</blockquote>
<p>维护购物车的常见操作如下：</p>
<ul>
<li>添加商品 - <code>HSET cart:&#123;session&#125; &#123;商品 id&#125; 1</code></li>
<li>添加数量 - <code>HINCRBY cart:&#123;session&#125; &#123;商品 id&#125; 1</code></li>
<li>商品总数 - <code>HLEN cart:&#123;session&#125;</code></li>
<li>删除商品 - <code>HDEL cart:&#123;session&#125; &#123;商品 id&#125;</code></li>
<li>获取购物车所有商品 - <code>HGETALL cart:&#123;session&#125;</code></li>
</ul>
<p>当前仅仅是将商品 ID 存储到了 Redis 中，在回显商品具体信息的时候，还需要拿着商品 id 查询一次数据库，获取完整的商品的信息。</p>
<h2 id="List"><a href="#List" class="headerlink" title="List"></a>List</h2><p>Redis 中的 List 类型就是有序列表。</p>
<h3 id="List-简介"><a href="#List-简介" class="headerlink" title="List 简介"></a>List 简介</h3><p><img src="https://raw.githubusercontent.com/dunwu/images/master/cs/database/redis/redis-datatype-list.png"></p>
<p>List 列表是简单的字符串列表，<strong>按照插入顺序排序</strong>，可以从头部或尾部向 List 列表添加元素。</p>
<p>列表的最大长度为 <code>2^32 - 1</code>，也即每个列表支持超过 <code>40 亿</code>个元素。</p>
<h3 id="List-实现"><a href="#List-实现" class="headerlink" title="List 实现"></a>List 实现</h3><p>列表对象的编码可以是 <code>ziplist</code> 或者 <code>linkedlist</code> 。</p>
<p><code>ziplist</code> 编码的列表对象使用压缩列表作为底层实现， 每个压缩列表节点（entry）保存了一个列表元素。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202410100802398.svg"></p>
<p><code>inkedlist</code> 编码的列表对象使用双链表作为底层实现。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202410100802787.svg"></p>
<p>当列表对象可以同时满足以下两个条件时， 列表对象使用 <code>ziplist</code> 编码；否则，使用 <code>linkedlist</code> 编码</p>
<ol>
<li>列表对象保存的所有字符串元素的长度都小于 <code>64</code> 字节；</li>
<li>列表对象保存的元素数量小于 <code>512</code> 个；</li>
</ol>
<blockquote>
<p>注意</p>
<p>以上两个条件的上限值是可以修改的， 具体请看配置文件中关于 <code>list-max-ziplist-value</code> 选项和 <code>list-max-ziplist-entries</code> 选项的说明。</p>
</blockquote>
<h3 id="List-命令"><a href="#List-命令" class="headerlink" title="List 命令"></a>List 命令</h3><table>
<thead>
<tr>
<th>命令</th>
<th>行为</th>
</tr>
</thead>
<tbody><tr>
<td><code>LPUSH</code></td>
<td>将给定值推入列表的右端。</td>
</tr>
<tr>
<td><code>RPUSH</code></td>
<td>将给定值推入列表的右端。</td>
</tr>
<tr>
<td><code>LPOP</code></td>
<td>从列表的左端弹出一个值，并返回被弹出的值。</td>
</tr>
<tr>
<td><code>RPOP</code></td>
<td>从列表的右端弹出一个值，并返回被弹出的值。</td>
</tr>
<tr>
<td><code>LRANGE</code></td>
<td>获取列表在给定范围上的所有值。</td>
</tr>
<tr>
<td><code>LINDEX</code></td>
<td>获取列表在给定位置上的单个元素。</td>
</tr>
<tr>
<td><code>LREM</code></td>
<td>从列表的左端弹出一个值，并返回被弹出的值。</td>
</tr>
<tr>
<td><code>LTRIM</code></td>
<td>只保留指定区间内的元素，删除其他元素。</td>
</tr>
</tbody></table>
<blockquote>
<p>更多命令请参考：<a target="_blank" rel="noopener" href="https://redis.io/commands#list">Redis List 类型官方命令文档</a></p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将一个或多个值 value 插入到 key 列表的表头（最左边），最后的值在最前面</span></span><br><span class="line">LPUSH key value [value ...]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将一个或多个值 value 插入到 key 列表的表尾（最右边）</span></span><br><span class="line">RPUSH key value [value ...]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">移除并返回 key 列表的头元素</span></span><br><span class="line">LPOP key</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">移除并返回 key 列表的尾元素</span></span><br><span class="line">RPOP key</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">返回列表 key 中指定区间内的元素，区间以偏移量 start 和 stop 指定，从 0 开始</span></span><br><span class="line">LRANGE key start stop</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">从 key 列表表头弹出一个元素，没有就阻塞 <span class="built_in">timeout</span> 秒，如果 <span class="built_in">timeout</span>=0 则一直阻塞</span></span><br><span class="line">BLPOP key [key ...] timeout</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">从 key 列表表尾弹出一个元素，没有就阻塞 <span class="built_in">timeout</span> 秒，如果 <span class="built_in">timeout</span>=0 则一直阻塞</span></span><br><span class="line">BRPOP key [key ...] timeout</span><br></pre></td></tr></table></figure>

<h3 id="List-应用"><a href="#List-应用" class="headerlink" title="List 应用"></a>List 应用</h3><h4 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h4><p>消息队列在存取消息时，必须要满足三个需求，分别是<strong>消息保序、处理重复的消息和保证消息可靠性</strong>。</p>
<p>Redis 的 List 和 Stream 两种数据类型，就可以满足消息队列的这三个需求。我们先来了解下基于 List 的消息队列实现方法，后面在介绍 Stream 数据类型时候，在详细说说 Stream。</p>
<p><em>1、如何满足消息保序需求？</em></p>
<p>List 本身就是按先进先出的顺序对数据进行存取的，所以，如果使用 List 作为消息队列保存消息的话，就已经能满足消息保序的需求了。</p>
<p>List 可以使用 LPUSH + RPOP（或者反过来，RPUSH+LPOP）命令实现消息队列。</p>
<ul>
<li><p>生产者使用 <code>LPUSH key value[value...]</code> 将消息插入到队列的头部，如果 key 不存在则会创建一个空的队列再插入消息。</p>
</li>
<li><p>消费者使用 <code>RPOP key</code> 依次读取队列的消息，先进先出。</p>
</li>
</ul>
<p>不过，在消费者读取数据时，有一个潜在的性能风险点。</p>
<p>在生产者往 List 中写入数据时，List 并不会主动地通知消费者有新消息写入，如果消费者想要及时处理消息，就需要在程序中不停地调用 <code>RPOP</code> 命令（比如使用一个 while(1) 循环）。如果有新消息写入，RPOP 命令就会返回结果，否则，RPOP 命令返回空值，再继续循环。</p>
<p>所以，即使没有新消息写入 List，消费者也要不停地调用 RPOP 命令，这就会导致消费者程序的 CPU 一直消耗在执行 RPOP 命令上，带来不必要的性能损失。</p>
<p>为了解决这个问题，Redis 提供了 BRPOP 命令。<strong>BRPOP 命令也称为阻塞式读取，客户端在没有读到队列数据时，自动阻塞，直到有新的数据写入队列，再开始读取新数据</strong>。和消费者程序自己不停地调用 RPOP 命令相比，这种方式能节省 CPU 开销。</p>
<p><em>2、如何处理重复的消息？</em></p>
<p>消费者要实现重复消息的判断，需要 2 个方面的要求：</p>
<ul>
<li>每个消息都有一个全局的 ID。</li>
<li>消费者要记录已经处理过的消息的 ID。当收到一条消息后，消费者程序就可以对比收到的消息 ID 和记录的已处理过的消息 ID，来判断当前收到的消息有没有经过处理。如果已经处理过，那么，消费者程序就不再进行处理了。</li>
</ul>
<p>但是 <strong>List 并不会为每个消息生成 ID 号，所以我们需要自行为每个消息生成一个全局唯一 ID</strong>，生成之后，我们在用 LPUSH 命令把消息插入 List 时，需要在消息中包含这个全局唯一 ID。</p>
<p>例如，我们执行以下命令，就把一条全局 ID 为 111000102、库存量为 99 的消息插入了消息队列：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">LPUSH mq <span class="string">&quot;111000102:stock:99&quot;</span></span></span><br><span class="line">(integer) 1</span><br></pre></td></tr></table></figure>

<p><em>3、如何保证消息可靠性？</em></p>
<p>当消费者程序从 List 中读取一条消息后，List 就不会再留存这条消息了。所以，如果消费者程序在处理消息的过程出现了故障或宕机，就会导致消息没有处理完成，那么，消费者程序再次启动后，就没法再次从 List 中读取消息了。</p>
<p>为了留存消息，List 类型提供了 <code>BRPOPLPUSH</code> 命令，这个命令的<strong>作用是让消费者程序从一个 List 中读取消息，同时，Redis 会把这个消息再插入到另一个 List（可以叫作备份 List）留存</strong>。</p>
<p>这样一来，如果消费者程序读了消息但没能正常处理，等它重启后，就可以从备份 List 中重新读取消息并进行处理了。</p>
<p>好了，到这里可以知道基于 List 类型的消息队列，满足消息队列的三大需求（消息保序、处理重复的消息和保证消息可靠性）。</p>
<ul>
<li>消息保序：使用 LPUSH + RPOP；</li>
<li>阻塞读取：使用 BRPOP；</li>
<li>重复消息处理：生产者自行实现全局唯一 ID；</li>
<li>消息的可靠性：使用 BRPOPLPUSH</li>
</ul>
<blockquote>
<p>List 作为消息队列有什么缺陷？</p>
</blockquote>
<p><strong>List 不支持多个消费者消费同一条消息</strong>，因为一旦消费者拉取一条消息后，这条消息就从 List 中删除了，无法被其它消费者再次消费。</p>
<p>要实现一条消息可以被多个消费者消费，那么就要将多个消费者组成一个消费组，使得多个消费者可以消费同一条消息，但是 <strong>List 类型并不支持消费组的实现</strong>。</p>
<p>这就要说起 Redis 从 5.0 版本开始提供的 Stream 数据类型了，Stream 同样能够满足消息队列的三大需求，而且它还支持“消费组”形式的消息读取。</p>
<h4 id="输入自动补全"><a href="#输入自动补全" class="headerlink" title="输入自动补全"></a>输入自动补全</h4><p>【需求场景】</p>
<p>根据用户输入，自动补全信息，如：联系人、商品名等。</p>
<ul>
<li>典型场景一 - 社交网站后台记录用户最近联系过的 100 个好友，当用户查找好友时，根据输入的关键字自动补全姓名。</li>
<li>典型场景二 - 电商网站后台记录用户最近浏览过的 10 件商品，当用户查找商品是，根据输入的关键字自动补全商品名称。</li>
</ul>
<p>【解决方案】</p>
<blockquote>
<p>使用 Redis 的 List 类型存储一个最近信息列表，然后在需要自动补全信息时展示相应数量的数据。</p>
</blockquote>
<p>维护最近信息列表的常见操作如下：</p>
<ul>
<li>如果指定信息已经存在于最近信息列表里，那么从列表里移除。使用 <code>LREM</code> 命令。</li>
<li>将指定信息添加到最近信息列表的头部。使用 <code>LPUSH</code> 命令。</li>
<li>添加操作完成后，如果最近信息列表中的数量超过上限 N，进行裁剪操作。使用 <code>LTRIM</code> 命令。</li>
</ul>
<h2 id="Set"><a href="#Set" class="headerlink" title="Set"></a>Set</h2><p>Redis 中的 Set 类型就是无序且去重的集合。</p>
<h3 id="Set-简介"><a href="#Set-简介" class="headerlink" title="Set 简介"></a>Set 简介</h3><div align="center">
<img src="https://raw.githubusercontent.com/dunwu/images/master/cs/database/redis/redis-datatype-set.png" width="400"/>
</div>

<p>Set 类型是一个无序并唯一的键值集合，它的存储顺序不会按照插入的先后顺序进行存储。</p>
<p>一个集合最多可以存储 <code>2^32-1</code> 个元素。概念和数学中个的集合基本类似，可以交集，并集，差集等等，所以 Set 类型除了支持集合内的增删改查，同时还支持多个集合取交集、并集、差集。</p>
<p>Set 类型和 List 类型的区别如下：</p>
<ul>
<li>List 可以存储重复元素，Set 只能存储非重复元素；</li>
<li>List 是按照元素的先后顺序存储元素的，而 Set 则是无序方式存储元素的。</li>
</ul>
<h3 id="Set-实现"><a href="#Set-实现" class="headerlink" title="Set 实现"></a>Set 实现</h3><p>集合对象的编码可以是 <code>intset</code> 或者 <code>hashtable</code> 。</p>
<p><code>intset</code> 编码的集合对象使用整数集合作为底层实现， 集合对象包含的所有元素都被保存在整数集合里面。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202410100806680.svg"></p>
<p><code>hashtable</code> 编码的集合对象使用字典作为底层实现， 字典的每个键都是一个字符串对象， 每个字符串对象包含了一个集合元素， 而字典的值则全部被设置为 <code>NULL</code> 。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202410100806732.svg"></p>
<p>当集合对象可以同时满足以下两个条件时，集合对象使用 <code>intset</code> 编码；否则，使用 <code>hashtable</code> 编码：</p>
<ol>
<li>集合对象保存的所有元素都是整数值；</li>
<li>集合对象保存的元素数量不超过 <code>512</code> 个；</li>
</ol>
<blockquote>
<p>注意：第二个条件的上限值是可以修改的， 具体请看配置文件中关于 <code>set-max-intset-entries</code> 选项的说明。</p>
</blockquote>
<h3 id="Set-命令"><a href="#Set-命令" class="headerlink" title="Set 命令"></a>Set 命令</h3><table>
<thead>
<tr>
<th>命令</th>
<th>行为</th>
</tr>
</thead>
<tbody><tr>
<td><code>SADD</code></td>
<td>将给定元素添加到集合。</td>
</tr>
<tr>
<td><code>SMEMBERS</code></td>
<td>返回集合包含的所有元素。</td>
</tr>
<tr>
<td><code>SISMEMBER</code></td>
<td>检查给定元素是否存在于集合中。</td>
</tr>
<tr>
<td><code>SREM</code></td>
<td>如果给定的元素存在于集合中，那么移除这个元素。</td>
</tr>
</tbody></table>
<blockquote>
<p>更多命令请参考：<a target="_blank" rel="noopener" href="https://redis.io/commands#set">Redis Set 类型官方命令文档</a></p>
</blockquote>
<p>Set 常用操作：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">往集合 key 中存入元素，元素存在则忽略，若 key 不存在则新建</span></span><br><span class="line">SADD key member [member ...]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">从集合 key 中删除元素</span></span><br><span class="line">SREM key member [member ...]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取集合 key 中所有元素</span></span><br><span class="line">SMEMBERS key</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取集合 key 中的元素个数</span></span><br><span class="line">SCARD key</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">判断 member 元素是否存在于集合 key 中</span></span><br><span class="line">SISMEMBER key member</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">从集合 key 中随机选出 count 个元素，元素不从 key 中删除</span></span><br><span class="line">SRANDMEMBER key [count]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">从集合 key 中随机选出 count 个元素，元素从 key 中删除</span></span><br><span class="line">SPOP key [count]</span><br></pre></td></tr></table></figure>

<p>Set 运算操作：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">交集运算</span></span><br><span class="line">SINTER key [key ...]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将交集结果存入新集合 destination 中</span></span><br><span class="line">SINTERSTORE destination key [key ...]</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">并集运算</span></span><br><span class="line">SUNION key [key ...]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将并集结果存入新集合 destination 中</span></span><br><span class="line">SUNIONSTORE destination key [key ...]</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">差集运算</span></span><br><span class="line">SDIFF key [key ...]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将差集结果存入新集合 destination 中</span></span><br><span class="line">SDIFFSTORE destination key [key ...]</span><br></pre></td></tr></table></figure>

<h3 id="Set-应用"><a href="#Set-应用" class="headerlink" title="Set 应用"></a>Set 应用</h3><p>集合的主要几个特性，无序、不可重复、支持并交差等操作。</p>
<p>因此 Set 类型比较适合用来数据去重和保障数据的唯一性，还可以用来统计多个集合的交集、错集和并集等，当我们存储的数据是无序并且需要去重的情况下，比较适合使用集合类型进行存储。</p>
<p>但是要提醒你一下，这里有一个潜在的风险。<strong>Set 的差集、并集和交集的计算复杂度较高，在数据量较大的情况下，如果直接执行这些计算，会导致 Redis 实例阻塞</strong>。</p>
<p>在主从集群中，为了避免主库因为 Set 做聚合计算（交集、差集、并集）时导致主库被阻塞，我们可以选择一个从库完成聚合统计，或者把数据返回给客户端，由客户端来完成聚合统计。</p>
<h4 id="点赞"><a href="#点赞" class="headerlink" title="点赞"></a>点赞</h4><p>Set 类型可以保证一个用户只能点一个赞，这里举例子一个场景，key 是文章 id，value 是用户 id。</p>
<p><code>uid:1</code> 、<code>uid:2</code>、<code>uid:3</code> 三个用户分别对 article:1 文章点赞了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">uid:1 用户对文章 article:1 点赞</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SADD article:1 uid:1</span></span><br><span class="line">(integer) 1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">uid:2 用户对文章 article:1 点赞</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SADD article:1 uid:2</span></span><br><span class="line">(integer) 1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">uid:3 用户对文章 article:1 点赞</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SADD article:1 uid:3</span></span><br><span class="line">(integer) 1</span><br></pre></td></tr></table></figure>

<p><code>uid:1</code> 取消了对 article:1 文章点赞。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; SREM article:1 uid:1</span><br><span class="line">(integer) 1</span><br></pre></td></tr></table></figure>

<p>获取 article:1 文章所有点赞用户 :</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SMEMBERS article:1</span></span><br><span class="line">1) &quot;uid:3&quot;</span><br><span class="line">2) &quot;uid:2&quot;</span><br></pre></td></tr></table></figure>

<p>获取 article:1 文章的点赞用户数量：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SCARD article:1</span></span><br><span class="line">(integer) 2</span><br></pre></td></tr></table></figure>

<p>判断用户 <code>uid:1</code> 是否对文章 article:1 点赞了：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SISMEMBER article:1 uid:1</span></span><br><span class="line">(integer) 0  # 返回 0 说明没点赞，返回 1 则说明点赞了</span><br></pre></td></tr></table></figure>

<h4 id="共同关注"><a href="#共同关注" class="headerlink" title="共同关注"></a>共同关注</h4><p>Set 类型支持交集运算，所以可以用来计算共同关注的好友、公众号等。</p>
<p>key 可以是用户 id，value 则是已关注的公众号的 id。</p>
<p><code>uid:1</code> 用户关注公众号 id 为 5、6、7、8、9，<code>uid:2</code> 用户关注公众号 id 为 7、8、9、10、11。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">uid:1 用户关注公众号 <span class="built_in">id</span> 为 5、6、7、8、9</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SADD uid:1 5 6 7 8 9</span></span><br><span class="line">(integer) 5</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">uid:2  用户关注公众号 <span class="built_in">id</span> 为 7、8、9、10、11</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SADD uid:2 7 8 9 10 11</span></span><br><span class="line">(integer) 5</span><br></pre></td></tr></table></figure>

<p><code>uid:1</code> 和 <code>uid:2</code> 共同关注的公众号：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取共同关注</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SINTER uid:1 uid:2</span></span><br><span class="line">1) &quot;7&quot;</span><br><span class="line">2) &quot;8&quot;</span><br><span class="line">3) &quot;9&quot;</span><br></pre></td></tr></table></figure>

<p>给 <code>uid:2</code> 推荐 <code>uid:1</code> 关注的公众号：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SDIFF uid:1 uid:2</span></span><br><span class="line">1) &quot;5&quot;</span><br><span class="line">2) &quot;6&quot;</span><br></pre></td></tr></table></figure>

<p>验证某个公众号是否同时被 <code>uid:1</code> 或 <code>uid:2</code> 关注：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SISMEMBER uid:1 5</span></span><br><span class="line">(integer) 1 # 返回 1，说明关注了</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SISMEMBER uid:2 5</span></span><br><span class="line">(integer) 0 # 返回 0，说明没关注</span><br></pre></td></tr></table></figure>

<h4 id="抽奖活动"><a href="#抽奖活动" class="headerlink" title="抽奖活动"></a>抽奖活动</h4><p>存储某活动中中奖的用户名，Set 类型因为有去重功能，可以保证同一个用户不会中奖两次。</p>
<p>key 为抽奖活动名，value 为员工名称，把所有员工名称放入抽奖箱：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SADD lucky Tom Jerry John Sean Marry Lindy Sary Mark</span></span><br><span class="line">(integer) 5</span><br></pre></td></tr></table></figure>

<p>如果允许重复中奖，可以使用 SRANDMEMBER 命令。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">抽取 1 个一等奖：</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SRANDMEMBER lucky 1</span></span><br><span class="line">1) &quot;Tom&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">抽取 2 个二等奖：</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SRANDMEMBER lucky 2</span></span><br><span class="line">1) &quot;Mark&quot;</span><br><span class="line">2) &quot;Jerry&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">抽取 3 个三等奖：</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SRANDMEMBER lucky 3</span></span><br><span class="line">1) &quot;Sary&quot;</span><br><span class="line">2) &quot;Tom&quot;</span><br><span class="line">3) &quot;Jerry&quot;</span><br></pre></td></tr></table></figure>

<p>如果不允许重复中奖，可以使用 SPOP 命令。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">抽取一等奖 1 个</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SPOP lucky 1</span></span><br><span class="line">1) &quot;Sary&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">抽取二等奖 2 个</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SPOP lucky 2</span></span><br><span class="line">1) &quot;Jerry&quot;</span><br><span class="line">2) &quot;Mark&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">抽取三等奖 3 个</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SPOP lucky 3</span></span><br><span class="line">1) &quot;John&quot;</span><br><span class="line">2) &quot;Sean&quot;</span><br><span class="line">3) &quot;Lindy&quot;</span><br></pre></td></tr></table></figure>

<h2 id="Zset"><a href="#Zset" class="headerlink" title="Zset"></a>Zset</h2><p>Redis 中的 Zset 类型就是有序且去重的集合。</p>
<h3 id="Zset-简介"><a href="#Zset-简介" class="headerlink" title="Zset 简介"></a>Zset 简介</h3><p>Zset 类型（有序集合类型）相比于 Set 类型多了一个排序属性 score（分值），对于有序集合 ZSet 来说，每个存储元素相当于有两个值组成的，一个是有序结合的元素值，一个是排序值。</p>
<p>有序集合保留了集合不能有重复成员的特性（分值可以重复），但不同的是，有序集合中的元素可以排序。</p>
<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/images/master/cs/database/redis/redis-datatype-zset.png" width="400"/>
</div>

<h3 id="Zset-实现"><a href="#Zset-实现" class="headerlink" title="Zset 实现"></a>Zset 实现</h3><p>有序集合的编码可以是 <code>ziplist</code> 或者 <code>skiplist</code> 。</p>
<p><code>ziplist</code> 编码的有序集合对象使用压缩列表作为底层实现， 每个集合元素使用两个紧挨在一起的压缩列表节点来保存， 第一个节点保存元素的成员（member）， 而第二个元素则保存元素的分值（score）。压缩列表内的集合元素按分值从小到大进行排序， 分值较小的元素被放置在靠近表头的方向， 而分值较大的元素则被放置在靠近表尾的方向。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202410100808991.svg"></p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202410100808319.svg"></p>
<p><code>skiplist</code> 编码的有序集合对象使用 <code>zset</code> 结构作为底层实现， 一个 <code>zset</code> 结构同时包含一个字典和一个跳跃表</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">zset</span> &#123;</span></span><br><span class="line"></span><br><span class="line">    zskiplist *zsl;</span><br><span class="line"></span><br><span class="line">    dict *dict;</span><br><span class="line"></span><br><span class="line">&#125; zset;</span><br></pre></td></tr></table></figure>

<p><code>zset</code> 结构中的 <code>zsl</code> 跳跃表按分值从小到大保存了所有集合元素， 每个跳跃表节点都保存了一个集合元素： 跳跃表节点的 <code>object</code> 属性保存了元素的成员， 而跳跃表节点的 <code>score</code> 属性则保存了元素的分值。 通过这个跳跃表， 程序可以对有序集合进行范围型操作， 比如 ZRANK 、 ZRANGE 等命令就是基于跳跃表 API 来实现的。</p>
<p>除此之外， <code>zset</code> 结构中的 <code>dict</code> 字典为有序集合创建了一个从成员到分值的映射， 字典中的每个键值对都保存了一个集合元素： 字典的键保存了元素的成员， 而字典的值则保存了元素的分值。 通过这个字典， 程序可以用 O(1) 复杂度查找给定成员的分值， ZSCORE 命令就是根据这一特性实现的， 而很多其他有序集合命令都在实现的内部用到了这一特性。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202410100810255.svg"></p>
<p>有序集合每个元素的成员都是一个字符串对象， 而每个元素的分值都是一个 <code>double</code> 类型的浮点数。 值得一提的是， 虽然 <code>zset</code> 结构同时使用跳跃表和字典来保存有序集合元素， 但这两种数据结构都会通过指针来共享相同元素的成员和分值， 所以同时使用跳跃表和字典来保存集合元素不会产生任何重复成员或者分值， 也不会因此而浪费额外的内存。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202410100812776.svg"></p>
<p>当有序集合对象可以同时满足以下两个条件时，有序集合对象使用 <code>ziplist</code> 编码；否则，使用 <code>skiplist</code> 编码。</p>
<ul>
<li>有序集合保存的元素数量小于 <code>128</code> 个；</li>
<li>有序集合保存的所有元素成员的长度都小于 <code>64</code> 字节；</li>
</ul>
<blockquote>
<p>注意：以上两个条件的上限值是可以修改的， 具体请看配置文件中关于 <code>zset-max-ziplist-entries</code> 选项和 <code>zset-max-ziplist-value</code> 选项的说明。</p>
</blockquote>
<p><strong>在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了。</strong></p>
<h3 id="Zset-命令"><a href="#Zset-命令" class="headerlink" title="Zset 命令"></a>Zset 命令</h3><table>
<thead>
<tr>
<th>命令</th>
<th>行为</th>
</tr>
</thead>
<tbody><tr>
<td><code>ZADD</code></td>
<td>将一个带有给定分值的成员添加到有序集合里面</td>
</tr>
<tr>
<td><code>ZRANGE</code></td>
<td>顺序排序，并返回指定排名区间的成员</td>
</tr>
<tr>
<td><code>ZREVRANGE</code></td>
<td>反序排序，并返回指定排名区间的成员</td>
</tr>
<tr>
<td><code>ZRANGEBYSCORE</code></td>
<td>顺序排序，并返回指定排名区间的成员及其分值</td>
</tr>
<tr>
<td><code>ZREVRANGEBYSCORE</code></td>
<td>反序排序，并返回指定排名区间的成员及其分值</td>
</tr>
<tr>
<td><code>ZREM</code></td>
<td>移除指定的成员</td>
</tr>
<tr>
<td><code>ZSCORE</code></td>
<td>返回指定成员的分值</td>
</tr>
<tr>
<td><code>ZCARD</code></td>
<td>返回所有成员数</td>
</tr>
</tbody></table>
<blockquote>
<p>更多命令请参考：<a target="_blank" rel="noopener" href="https://redis.io/commands#sorted_set">Redis ZSet 类型官方命令文档</a></p>
</blockquote>
<p>Zset 常用操作：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">往有序集合 key 中加入带分值元素</span></span><br><span class="line">ZADD key score member [[score member]...]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">往有序集合 key 中删除元素</span></span><br><span class="line">ZREM key member [member...]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">返回有序集合 key 中元素 member 的分值</span></span><br><span class="line">ZSCORE key member</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">返回有序集合 key 中元素个数</span></span><br><span class="line">ZCARD key</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">为有序集合 key 中元素 member 的分值加上 increment</span></span><br><span class="line">ZINCRBY key increment member</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">正序获取有序集合 key 从 start 下标到 stop 下标的元素</span></span><br><span class="line">ZRANGE key start stop [WITHSCORES]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">倒序获取有序集合 key 从 start 下标到 stop 下标的元素</span></span><br><span class="line">ZREVRANGE key start stop [WITHSCORES]</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">返回有序集合中指定分数区间内的成员，分数由低到高排序。</span></span><br><span class="line">ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">返回指定成员区间内的成员，按字典正序排列，分数必须相同。</span></span><br><span class="line">ZRANGEBYLEX key min max [LIMIT offset count]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">返回指定成员区间内的成员，按字典倒序排列，分数必须相同</span></span><br><span class="line">ZREVRANGEBYLEX key max min [LIMIT offset count]</span><br></pre></td></tr></table></figure>

<p>Zset 运算操作（相比于 Set 类型，ZSet 类型没有支持差集运算）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">并集计算（相同元素分值相加），numberkeys 一共多少个 key，WEIGHTS 每个 key 对应的分值乘积</span></span><br><span class="line">ZUNIONSTORE destkey numberkeys key [key...]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">交集计算（相同元素分值相加），numberkeys 一共多少个 key，WEIGHTS 每个 key 对应的分值乘积</span></span><br><span class="line">ZINTERSTORE destkey numberkeys key [key...]</span><br></pre></td></tr></table></figure>

<h3 id="Zset-应用"><a href="#Zset-应用" class="headerlink" title="Zset 应用"></a>Zset 应用</h3><p>Zset 类型（Sorted Set，有序集合）可以根据元素的权重来排序，我们可以自己来决定每个元素的权重值。比如说，我们可以根据元素插入 Sorted Set 的时间确定权重值，先插入的元素权重小，后插入的元素权重大。</p>
<p>在面对需要展示最新列表、排行榜等场景时，如果数据更新频繁或者需要分页显示，可以优先考虑使用 Sorted Set。</p>
<h4 id="排行榜"><a href="#排行榜" class="headerlink" title="排行榜"></a>排行榜</h4><p>【需求场景】</p>
<p>各种排行榜，如：内容平台（视频、歌曲、文章）的播放量&#x2F;收藏量&#x2F;评分排行榜；电商网站的销售排行榜；</p>
<p>【解决方案】</p>
<p>有序集合比较典型的使用场景就是排行榜。例如学生成绩的排名榜、游戏积分排行榜、视频播放排名、电商系统中商品的销量排名等。</p>
<p>我们以博文点赞排名为例，dunwu 发表了五篇博文，分别获得赞为 200、40、100、50、150。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">article:1 文章获得了 200 个赞</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">ZADD user:dunwu:ranking 200 article:1</span></span><br><span class="line">(integer) 1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">article:2 文章获得了 40 个赞</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">ZADD user:dunwu:ranking 40 article:2</span></span><br><span class="line">(integer) 1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">article:3 文章获得了 100 个赞</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">ZADD user:dunwu:ranking 100 article:3</span></span><br><span class="line">(integer) 1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">article:4 文章获得了 50 个赞</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">ZADD user:dunwu:ranking 50 article:4</span></span><br><span class="line">(integer) 1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">article:5 文章获得了 150 个赞</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">ZADD user:dunwu:ranking 150 article:5</span></span><br><span class="line">(integer) 1</span><br></pre></td></tr></table></figure>

<p>文章 article:4 新增一个赞，可以使用 ZINCRBY 命令（为有序集合 key 中元素 member 的分值加上 increment）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">ZINCRBY user:dunwu:ranking 1 article:4</span></span><br><span class="line">&quot;51&quot;</span><br></pre></td></tr></table></figure>

<p>查看某篇文章的赞数，可以使用 ZSCORE 命令（返回有序集合 key 中元素个数）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">ZSCORE user:dunwu:ranking article:4</span></span><br><span class="line">&quot;50&quot;</span><br></pre></td></tr></table></figure>

<p>获取 dunwu 文章赞数最多的 3 篇文章，可以使用 ZREVRANGE 命令（倒序获取有序集合 key 从 start 下标到 stop 下标的元素）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">WITHSCORES 表示把 score 也显示出来</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">ZREVRANGE user:dunwu:ranking 0 2 WITHSCORES</span></span><br><span class="line">1) &quot;article:1&quot;</span><br><span class="line">2) &quot;200&quot;</span><br><span class="line">3) &quot;article:5&quot;</span><br><span class="line">4) &quot;150&quot;</span><br><span class="line">5) &quot;article:3&quot;</span><br><span class="line">6) &quot;100&quot;</span><br></pre></td></tr></table></figure>

<p>获取 dunwu 100 赞到 200 赞的文章，可以使用 ZRANGEBYSCORE 命令（返回有序集合中指定分数区间内的成员，分数由低到高排序）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">ZRANGEBYSCORE user:dunwu:ranking 100 200 WITHSCORES</span></span><br><span class="line">1) &quot;article:3&quot;</span><br><span class="line">2) &quot;100&quot;</span><br><span class="line">3) &quot;article:5&quot;</span><br><span class="line">4) &quot;150&quot;</span><br><span class="line">5) &quot;article:1&quot;</span><br><span class="line">6) &quot;200&quot;</span><br></pre></td></tr></table></figure>

<h4 id="前缀排序"><a href="#前缀排序" class="headerlink" title="前缀排序"></a>前缀排序</h4><p>使用有序集合的 <code>ZRANGEBYLEX</code> 或 <code>ZREVRANGEBYLEX</code> 可以帮助我们实现电话号码或姓名的排序，我们以 <code>ZRANGEBYLEX</code> （返回指定成员区间内的成员，按 key 正序排列，分数必须相同）为例。</p>
<p><strong>注意：不要在分数不一致的 SortSet 集合中去使用 ZRANGEBYLEX 和 ZREVRANGEBYLEX 指令，因为获取的结果会不准确。</strong></p>
<p><em>1、电话排序</em></p>
<p>我们可以将电话号码存储到 SortSet 中，然后根据需要来获取号段：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">ZADD phone 0 13100111100 0 13110114300 0 13132110901</span></span><br><span class="line">(integer) 3</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">ZADD phone 0 13200111100 0 13210414300 0 13252110901</span></span><br><span class="line">(integer) 3</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">ZADD phone 0 13300111100 0 13310414300 0 13352110901</span></span><br><span class="line">(integer) 3</span><br></pre></td></tr></table></figure>

<p>获取所有号码：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">ZRANGEBYLEX phone - +</span></span><br><span class="line">1) &quot;13100111100&quot;</span><br><span class="line">2) &quot;13110114300&quot;</span><br><span class="line">3) &quot;13132110901&quot;</span><br><span class="line">4) &quot;13200111100&quot;</span><br><span class="line">5) &quot;13210414300&quot;</span><br><span class="line">6) &quot;13252110901&quot;</span><br><span class="line">7) &quot;13300111100&quot;</span><br><span class="line">8) &quot;13310414300&quot;</span><br><span class="line">9) &quot;13352110901&quot;</span><br></pre></td></tr></table></figure>

<p>获取 132 号段的号码：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">ZRANGEBYLEX phone [132 (133</span></span><br><span class="line">1) &quot;13200111100&quot;</span><br><span class="line">2) &quot;13210414300&quot;</span><br><span class="line">3) &quot;13252110901&quot;</span><br></pre></td></tr></table></figure>

<p>获取 132、133 号段的号码：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">ZRANGEBYLEX phone [132 (134</span></span><br><span class="line">1) &quot;13200111100&quot;</span><br><span class="line">2) &quot;13210414300&quot;</span><br><span class="line">3) &quot;13252110901&quot;</span><br><span class="line">4) &quot;13300111100&quot;</span><br><span class="line">5) &quot;13310414300&quot;</span><br><span class="line">6) &quot;13352110901&quot;</span><br></pre></td></tr></table></figure>

<p><em>2、姓名排序</em></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">zadd names 0 Toumas 0 Jake 0 Bluetuo 0 Gaodeng 0 Aimini 0 Aidehua</span></span><br><span class="line">(integer) 6</span><br></pre></td></tr></table></figure>

<p>获取所有人的名字：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">ZRANGEBYLEX names - +</span></span><br><span class="line">1) &quot;Aidehua&quot;</span><br><span class="line">2) &quot;Aimini&quot;</span><br><span class="line">3) &quot;Bluetuo&quot;</span><br><span class="line">4) &quot;Gaodeng&quot;</span><br><span class="line">5) &quot;Jake&quot;</span><br><span class="line">6) &quot;Toumas&quot;</span><br></pre></td></tr></table></figure>

<p>获取名字中大写字母 A 开头的所有人：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">ZRANGEBYLEX names [A (B</span></span><br><span class="line">1) &quot;Aidehua&quot;</span><br><span class="line">2) &quot;Aimini&quot;</span><br></pre></td></tr></table></figure>

<p>获取名字中大写字母 C 到 Z 的所有人：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">ZRANGEBYLEX names [C [Z</span></span><br><span class="line">1) &quot;Gaodeng&quot;</span><br><span class="line">2) &quot;Jake&quot;</span><br><span class="line">3) &quot;Toumas&quot;</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Redis 常见的五种数据类型：<strong>String（字符串），Hash（哈希），List（列表），Set（集合）及 Zset（sorted set：有序集合）</strong>。</p>
<p>这五种数据类型都由多种数据结构实现的，主要是出于时间和空间的考虑，当数据量小的时候使用更简单的数据结构，有利于节省内存，提高性能。</p>
<p>可以看到，Redis 数据类型的底层数据结构随着版本的更新也有所不同，比如：</p>
<ul>
<li>在 Redis 3.0 版本中 List 对象的底层数据结构由“双向链表”或“压缩表列表”实现，但是在 3.2 版本之后，List 数据类型底层数据结构是由 quicklist 实现的；</li>
<li>在最新的 Redis 代码中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了。</li>
</ul>
<p>Redis 五种数据类型的应用场景：</p>
<ul>
<li>String 类型的应用场景：缓存对象、分布式锁、共享 session、计数器、限流、分布式 ID 等。</li>
<li>List 类型的应用场景：消息队列（有两个问题：1. 生产者需要自行实现全局唯一 ID；2. 不能以消费组形式消费数据）、输入自动补全等。</li>
<li>Hash 类型：缓存对象、购物车等。</li>
<li>Set 类型：聚合计算（并集、交集、差集）场景，比如点赞、共同关注、抽奖活动等。</li>
<li>Zset 类型：排序场景，比如排行榜、电话和姓名排序等。</li>
</ul>
<p>Redis 后续版本又支持四种数据类型，它们的应用场景如下：</p>
<ul>
<li>BitMap（2.2 版新增）：二值状态统计的场景，比如签到、判断用户登陆状态、连续签到用户总数等；</li>
<li>HyperLogLog（2.8 版新增）：海量数据基数统计的场景，比如百万级网页 UV 计数等；</li>
<li>GEO（3.2 版新增）：存储地理位置信息的场景，比如滴滴叫车；</li>
<li>Stream（5.0 版新增）：消息队列，相比于基于 List 类型实现的消息队列，有这两个特有的特性：自动生成全局唯一消息 ID，支持以消费组形式消费数据。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202309232144470.jpg"></p>
<p>针对 Redis 是否适合做消息队列，关键看你的业务场景：</p>
<ul>
<li>如果你的业务场景足够简单，对于数据丢失不敏感，而且消息积压概率比较小的情况下，把 Redis 当作队列是完全可以的。</li>
<li>如果你的业务有海量消息，消息积压的概率比较大，并且不能接受数据丢失，那么还是用专业的消息队列中间件吧。</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://item.jd.com/11791607.html">《Redis 实战》</a></li>
<li><a target="_blank" rel="noopener" href="https://item.jd.com/11486101.html">《Redis 设计与实现》</a></li>
<li><a target="_blank" rel="noopener" href="https://dunwucoding.com/redis/data_struct/command.html#string">Redis 常见数据类型和应用场景</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/9cfc1b29/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/9cfc1b29/" class="post-title-link" itemprop="url">Redis 高级数据类型</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-06-24 10:45:38" itemprop="dateCreated datePublished" datetime="2020-06-24T10:45:38+08:00">2020-06-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">数据库</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/KV%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">KV数据库</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/KV%E6%95%B0%E6%8D%AE%E5%BA%93/redis/" itemprop="url" rel="index"><span itemprop="name">redis</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>12k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Redis-高级数据类型"><a href="#Redis-高级数据类型" class="headerlink" title="Redis 高级数据类型"></a>Redis 高级数据类型</h1><blockquote>
<p>关键词：<code>BitMap</code>、<code>HyperLogLog</code>、<code>Geo</code>、<code>Stream</code></p>
</blockquote>
<p>Redis 支持的高级数据类型：BitMap、HyperLogLog、GEO、Stream</p>
<p>使用 Redis ，不仅要了解其数据类型的特性，还需要根据业务场景，灵活的、高效的使用其数据类型来建模。</p>
<h2 id="BitMap"><a href="#BitMap" class="headerlink" title="BitMap"></a>BitMap</h2><h3 id="BitMap-简介"><a href="#BitMap-简介" class="headerlink" title="BitMap 简介"></a>BitMap 简介</h3><p>Bitmap，<strong>即位图，是一串连续的二进制数组（0 和 1）</strong>，可以通过偏移量（offset）定位元素。由于 bit 是计算机中最小的单位，使用它进行储存将<strong>非常节省空间</strong>，特别适合一些数据量大且使用<strong>二值统计的场景</strong>。例如在一个系统中，不同的用户使用单调递增的用户 ID 表示。40 亿（$$2^{32}$$ &#x3D; $$4<em>1024</em>1024*1024$$ ≈ 40 亿）用户只需要 512M 内存就能记住某种状态，例如用户是否已登录。</p>
<h3 id="BitMap-实现"><a href="#BitMap-实现" class="headerlink" title="BitMap 实现"></a>BitMap 实现</h3><p>实际上，<strong>BitMap 不是真实的数据结构，而是基于 String 实现的一组位操作</strong>。</p>
<p>由于 STRING 是二进制安全的，并且其最大长度是 512 MB，所以 BitMap 能最大设置 $$2^{32}$$ 个不同的 bit。</p>
<h3 id="BitMap-命令"><a href="#BitMap-命令" class="headerlink" title="BitMap 命令"></a>BitMap 命令</h3><table>
<thead>
<tr>
<th>命令</th>
<th>行为</th>
</tr>
</thead>
<tbody><tr>
<td><code>SETBIT</code></td>
<td>对 key 所储存的字符串值，设置或清除指定偏移量上的位(bit)</td>
</tr>
<tr>
<td><code>GETBIT</code></td>
<td>对 key 所储存的字符串值，获取指定偏移量上的位(bit)</td>
</tr>
<tr>
<td><code>BITOP</code></td>
<td>对一个或多个字符串执行位运算</td>
</tr>
</tbody></table>
<p>【示例】SETBIT、GETBIT 操作</p>
<p>假设有 1000 个传感器，标记为 0-999。现在，想要快速确定某传感器是否在一小时内对服务器执行了 ping 操作。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">传感器 123 在 2024 年 1 月 1 日 00:00 内对服务器执行 ping 操作</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">SETBIT pings:2024-01-01-00:00 123 1</span></span><br><span class="line">(integer) 0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">传感器 123 是否在 2024 年 1 月 1 日 00:00 内对服务器执行 ping 操作</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">GETBIT pings:2024-01-01-00:00 123</span></span><br><span class="line">1</span><br><span class="line">What about sensor 456?</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">GETBIT pings:2024-01-01-00:00 456</span></span><br><span class="line">0</span><br></pre></td></tr></table></figure>

<p>【示例】BITOP 操作</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">BitMap间的运算</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">operations 位移操作符，枚举值</span></span><br><span class="line">  AND 与运算 &amp;</span><br><span class="line">  OR 或运算 |</span><br><span class="line">  XOR 异或 ^</span><br><span class="line">  NOT 取反 ~</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">result 计算的结果，会存储在该key中</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">key1 … keyn 参与运算的key，可以有多个，空格分割，not运算只能一个key</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">当 BITOP 处理不同长度的字符串时，较短的那个字符串所缺少的部分会被看作 0。返回值是保存到 destkey 的字符串的长度（以字节byte为单位），和输入 key 中最长的字符串长度相等。</span></span><br><span class="line">BITOP [operations] [result] [key1] [keyn…]</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">返回指定key中第一次出现指定value(0/1)的位置</span></span><br><span class="line">BITPOS [key] [value]</span><br></pre></td></tr></table></figure>

<h3 id="BitMap-应用"><a href="#BitMap-应用" class="headerlink" title="BitMap 应用"></a>BitMap 应用</h3><p>Bitmap 类型非常适合二值状态统计的场景，这里的二值状态就是指集合元素的取值就只有 0 和 1 两种，在记录海量数据时，Bitmap 能够有效地节省内存空间。</p>
<h4 id="签到统计"><a href="#签到统计" class="headerlink" title="签到统计"></a>签到统计</h4><p>在签到打卡的场景中，我们只用记录签到（1）或未签到（0），所以它就是非常典型的二值状态。</p>
<p>签到统计时，每个用户一天的签到用 1 个 bit 位就能表示，一个月（假设是 31 天）的签到情况用 31 个 bit 位就可以，而一年的签到也只需要用 365 个 bit 位，根本不用太复杂的集合类型。</p>
<p>假设我们要统计 ID 100 的用户在 2022 年 6 月份的签到情况，就可以按照下面的步骤进行操作。</p>
<p>第一步，执行下面的命令，记录该用户 6 月 3 号已签到。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SETBIT uid:sign:100:202206 2 1</span><br></pre></td></tr></table></figure>

<p>第二步，检查该用户 6 月 3 日是否签到。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GETBIT uid:sign:100:202206 2</span><br></pre></td></tr></table></figure>

<p>第三步，统计该用户在 6 月份的签到次数。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">BITCOUNT uid:sign:100:202206</span><br></pre></td></tr></table></figure>

<p>这样，我们就知道该用户在 6 月份的签到情况了。</p>
<blockquote>
<p>如何统计这个月首次打卡时间呢？</p>
</blockquote>
<p>Redis 提供了 <code>BITPOS key bitValue [start] [end]</code>指令，返回数据表示 Bitmap 中第一个值为 <code>bitValue</code> 的 offset 位置。</p>
<p>在默认情况下，命令将检测整个位图，用户可以通过可选的 <code>start</code> 参数和 <code>end</code> 参数指定要检测的范围。所以我们可以通过执行这条命令来获取 userID &#x3D; 100 在 2022 年 6 月份<strong>首次打卡</strong>日期：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">BITPOS uid:sign:100:202206 1</span><br></pre></td></tr></table></figure>

<p>需要注意的是，因为 offset 从 0 开始的，所以我们需要将返回的 value + 1。</p>
<h4 id="判断用户是否登录"><a href="#判断用户是否登录" class="headerlink" title="判断用户是否登录"></a>判断用户是否登录</h4><p>Bitmap 提供了 <code>GETBIT、SETBIT</code> 操作，通过一个偏移值 offset 对 bit 数组的 offset 位置的 bit 位进行读写操作，需要注意的是 offset 从 0 开始。</p>
<p>只需要一个 key &#x3D; login_status 表示存储用户登陆状态集合数据，将用户 ID 作为 offset，在线就设置为 1，下线设置 0。通过 <code>GETBIT</code>判断对应的用户是否在线。50000 万 用户只需要 6 MB 的空间。</p>
<p>假如我们要判断 ID &#x3D; 10086 的用户的登陆情况：</p>
<p>第一步，执行以下指令，表示用户已登录。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SETBIT login_status 10086 1</span><br></pre></td></tr></table></figure>

<p>第二步，检查该用户是否登陆，返回值 1 表示已登录。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GETBIT login_status 10086</span><br></pre></td></tr></table></figure>

<p>第三步，登出，将 offset 对应的 value 设置成 0。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SETBIT login_status 10086 0</span><br></pre></td></tr></table></figure>

<h4 id="连续签到用户总数"><a href="#连续签到用户总数" class="headerlink" title="连续签到用户总数"></a>连续签到用户总数</h4><p>如何统计出这连续 7 天连续打卡用户总数呢？</p>
<p>我们把每天的日期作为 Bitmap 的 key，userId 作为 offset，若是打卡则将 offset 位置的 bit 设置成 1。</p>
<p>key 对应的集合的每个 bit 位的数据则是一个用户在该日期的打卡记录。</p>
<p>一共有 7 个这样的 Bitmap，如果我们能对这 7 个 Bitmap 的对应的 bit 位做 “与”运算。同样的 UserID offset 都是一样的，当一个 userID 在 7 个 Bitmap 对应对应的 offset 位置的 bit &#x3D; 1 就说明该用户 7 天连续打卡。</p>
<p>结果保存到一个新 Bitmap 中，我们再通过 <code>BITCOUNT</code> 统计 bit &#x3D; 1 的个数便得到了连续打卡 7 天的用户总数了。</p>
<p>Redis 提供了 <code>BITOP operation destkey key [key ...]</code>这个指令用于对一个或者多个 key 的 Bitmap 进行位元操作。</p>
<ul>
<li><code>operation</code> 可以是 <code>and</code>、<code>OR</code>、<code>NOT</code>、<code>XOR</code>。当 BITOP 处理不同长度的字符串时，较短的那个字符串所缺少的部分会被看作 <code>0</code> 。空的 <code>key</code> 也被看作是包含 <code>0</code> 的字符串序列。</li>
</ul>
<p>假设要统计 3 天连续打卡的用户数，则是将三个 bitmap 进行 AND 操作，并将结果保存到 destmap 中，接着对 destmap 执行 BITCOUNT 统计，如下命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">与操作</span></span><br><span class="line">BITOP AND destmap bitmap:01 bitmap:02 bitmap:03</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">统计 bit 位 =  1 的个数</span></span><br><span class="line">BITCOUNT destmap</span><br></pre></td></tr></table></figure>

<p>即使一天产生一个亿的数据，Bitmap 占用的内存也不大，大约占 12 MB 的内存（10^8&#x2F;8&#x2F;1024&#x2F;1024），7 天的 Bitmap 的内存开销约为 84 MB。同时我们最好给 Bitmap 设置过期时间，让 Redis 删除过期的打卡数据，节省内存。</p>
<h2 id="HyperLogLog"><a href="#HyperLogLog" class="headerlink" title="HyperLogLog"></a>HyperLogLog</h2><h3 id="HyperLogLog-简介"><a href="#HyperLogLog-简介" class="headerlink" title="HyperLogLog 简介"></a>HyperLogLog 简介</h3><p>Redis HyperLogLog 是 Redis 2.8.9 版本新增的数据类型，是一种<strong>用于“统计基数”的数据集合类型</strong>，基数统计就是指统计一个集合中不重复的元素个数。但要注意，**HyperLogLog 是统计规则是基于概率完成的，不是非常准确，标准误算率是 0.81%**。</p>
<p>所以，简单来说 HyperLogLog <strong>提供不精确的去重计数</strong>。</p>
<p>HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的内存空间总是固定的、并且是很小的。</p>
<p>在 Redis 里面，<strong>每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 <code>2^64</code> 个不同元素的基数</strong>，和元素越多就越耗费内存的 Set 和 Hash 类型相比，HyperLogLog 就非常节省空间。</p>
<p>这什么概念？举个例子给大家对比一下。</p>
<p>用 Java 语言来说，一般 long 类型占用 8 字节，而 1 字节有 8 位，即：1 byte &#x3D; 8 bit，即 long 数据类型最大可以表示的数是：<code>2^63-1</code>。对应上面的<code>2^64</code>个数，假设此时有<code>2^63-1</code>这么多个数，从 <code>0 ~ 2^63-1</code>，按照<code>long</code>以及<code>1k = 1024 字节</code>的规则来计算内存总数，就是：<code>((2^63-1) * 8/1024)K</code>，这是很庞大的一个数，存储空间远远超过<code>12K</code>，而 <code>HyperLogLog</code> 却可以用 <code>12K</code> 就能统计完。</p>
<h3 id="HyperLogLog-实现"><a href="#HyperLogLog-实现" class="headerlink" title="HyperLogLog 实现"></a>HyperLogLog 实现</h3><p>HyperLogLog 的实现涉及到很多数学问题，太费脑子了，我也没有搞懂，如果你想了解一下，课下可以看看这个：<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog</a>。</p>
<h3 id="HyperLogLog-命令"><a href="#HyperLogLog-命令" class="headerlink" title="HyperLogLog 命令"></a>HyperLogLog 命令</h3><p>HyperLogLog 命令很少，就三个。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加指定元素到 HyperLogLog 中</span></span><br><span class="line">PFADD key element [element ...]</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">返回给定 HyperLogLog 的基数估算值。</span></span><br><span class="line">PFCOUNT key [key ...]</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将多个 HyperLogLog 合并为一个 HyperLogLog</span></span><br><span class="line">PFMERGE destkey sourcekey [sourcekey ...]</span><br></pre></td></tr></table></figure>

<h3 id="HyperLogLog-应用"><a href="#HyperLogLog-应用" class="headerlink" title="HyperLogLog 应用"></a>HyperLogLog 应用</h3><h4 id="百万级网页-UV-计数"><a href="#百万级网页-UV-计数" class="headerlink" title="百万级网页 UV 计数"></a>百万级网页 UV 计数</h4><p>Redis HyperLogLog 优势在于只需要花费 12 KB 内存，就可以计算接近 2^64 个元素的基数，和元素越多就越耗费内存的 Set 和 Hash 类型相比，HyperLogLog 就非常节省空间。</p>
<p>所以，非常适合统计百万级以上的网页 UV 的场景。</p>
<p>在统计 UV 时，你可以用 PFADD 命令（用于向 HyperLogLog 中添加新元素）把访问页面的每个用户都添加到 HyperLogLog 中。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PFADD page1:uv user1 user2 user3 user4 user5</span><br></pre></td></tr></table></figure>

<p>接下来，就可以用 PFCOUNT 命令直接获得 page1 的 UV 值了，这个命令的作用就是返回 HyperLogLog 的统计结果。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PFCOUNT page1:uv</span><br></pre></td></tr></table></figure>

<p>不过，有一点需要你注意一下，HyperLogLog 的统计规则是基于概率完成的，所以它给出的统计结果是有一定误差的，标准误算率是 0.81%。</p>
<p>这也就意味着，你使用 HyperLogLog 统计的 UV 是 100 万，但实际的 UV 可能是 101 万。虽然误差率不算大，但是，如果你需要精确统计结果的话，最好还是继续用 Set 或 Hash 类型。</p>
<h2 id="GEO"><a href="#GEO" class="headerlink" title="GEO"></a>GEO</h2><h3 id="GEO-简介"><a href="#GEO-简介" class="headerlink" title="GEO 简介"></a>GEO 简介</h3><p>Redis GEO 是 Redis 3.2 版本新增的数据类型，主要用于存储地理位置信息，并对存储的信息进行操作。</p>
<p>在日常生活中，我们越来越依赖搜索“附近的餐馆”、在打车软件上叫车，这些都离不开基于位置信息服务（Location-Based Service，LBS）的应用。LBS 应用访问的数据是和人或物关联的一组经纬度信息，而且要能查询相邻的经纬度范围，GEO 就非常适合应用在 LBS 服务的场景中。</p>
<h3 id="GEO-实现"><a href="#GEO-实现" class="headerlink" title="GEO 实现"></a>GEO 实现</h3><p>GEO 本身并没有设计新的底层数据结构，而是直接使用了 Zset 类型。</p>
<p>GEO 类型使用 GeoHash 编码方法实现了经纬度到 Sorted Set 中元素权重分数的转换，这其中的两个关键机制就是“对二维地图做区间划分”和“对区间进行编码”。一组经纬度落在某个区间后，就用区间的编码值来表示，并把编码值作为 Sorted Set 元素的权重分数。</p>
<p>这样一来，我们就可以把经纬度保存到 Sorted Set 中，利用 Sorted Set 提供的“按权重进行有序范围查找”的特性，实现 LBS 服务中频繁使用的“搜索附近”的需求。</p>
<h3 id="GEO-命令"><a href="#GEO-命令" class="headerlink" title="GEO 命令"></a>GEO 命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">存储指定的地理空间位置，可以将一个或多个经度(longitude)、纬度(latitude)、位置名称(member)添加到指定的 key 中。</span></span><br><span class="line">GEOADD key longitude latitude member [longitude latitude member ...]</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">从给定的 key 里返回所有指定名称(member)的位置（经度和纬度），不存在的返回 nil。</span></span><br><span class="line">GEOPOS key member [member ...]</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">返回两个给定位置之间的距离。</span></span><br><span class="line">GEODIST key member1 member2 [m|km|ft|mi]</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">根据用户给定的经纬度坐标来获取指定范围内的地理位置集合。</span></span><br><span class="line">GEORADIUS key longitude latitude radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key]</span><br></pre></td></tr></table></figure>

<h3 id="GEO-应用"><a href="#GEO-应用" class="headerlink" title="GEO 应用"></a>GEO 应用</h3><h4 id="滴滴叫车"><a href="#滴滴叫车" class="headerlink" title="滴滴叫车"></a>滴滴叫车</h4><p>这里以滴滴叫车的场景为例，介绍下具体如何使用 GEO 命令：GEOADD 和 GEORADIUS 这两个命令。</p>
<p>假设车辆 ID 是 33，经纬度位置是（116.034579，39.030452），我们可以用一个 GEO 集合保存所有车辆的经纬度，集合 key 是 cars:locations。</p>
<p>执行下面的这个命令，就可以把 ID 号为 33 的车辆的当前经纬度位置存入 GEO 集合中：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GEOADD cars:locations 116.034579 39.030452 33</span><br></pre></td></tr></table></figure>

<p>当用户想要寻找自己附近的网约车时，LBS 应用就可以使用 GEORADIUS 命令。</p>
<p>例如，LBS 应用执行下面的命令时，Redis 会根据输入的用户的经纬度信息（116.054579，39.030452），查找以这个经纬度为中心的 5 公里内的车辆信息，并返回给 LBS 应用。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GEORADIUS cars:locations 116.054579 39.030452 5 km ASC COUNT 10</span><br></pre></td></tr></table></figure>

<h2 id="Stream"><a href="#Stream" class="headerlink" title="Stream"></a>Stream</h2><h3 id="Stream-简介"><a href="#Stream-简介" class="headerlink" title="Stream 简介"></a>Stream 简介</h3><p>Redis Stream 是 Redis 5.0 版本新增加的数据类型，Redis 专门为消息队列设计的数据类型。</p>
<p>在 Redis 5.0 Stream 没出来之前，消息队列的实现方式都有着各自的缺陷，例如：</p>
<ul>
<li>发布订阅模式，不能持久化也就无法可靠的保存消息，并且对于离线重连的客户端不能读取历史消息的缺陷；</li>
<li>List 实现消息队列的方式不能重复消费，一个消息消费完就会被删除，而且生产者需要自行实现全局唯一 ID。</li>
</ul>
<p>基于以上问题，Redis 5.0 便推出了 Stream 类型也是此版本最重要的功能，用于完美地实现消息队列，它支持消息的持久化、支持自动生成全局唯一 ID、支持 ack 确认消息的模式、支持消费组模式等，让消息队列更加的稳定和可靠。</p>
<h3 id="Stream-命令"><a href="#Stream-命令" class="headerlink" title="Stream 命令"></a>Stream 命令</h3><p>Stream 消息队列操作命令：</p>
<ul>
<li>XADD：插入消息，保证有序，可以自动生成全局唯一 ID；</li>
<li>XLEN：查询消息长度；</li>
<li>XREAD：用于读取消息，可以按 ID 读取数据；</li>
<li>XDEL：根据消息 ID 删除消息；</li>
<li>DEL：删除整个 Stream；</li>
<li>XRANGE：读取区间消息</li>
<li>XREADGROUP：按消费组形式读取消息；</li>
<li>XPENDING 和 XACK：<ul>
<li>XPENDING 命令可以用来查询每个消费组内所有消费者“已读取、但尚未确认”的消息；</li>
<li>XACK 命令用于向消息队列确认消息处理已完成；</li>
</ul>
</li>
</ul>
<h3 id="Stream-应用"><a href="#Stream-应用" class="headerlink" title="Stream 应用"></a>Stream 应用</h3><h4 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h4><p>生产者通过 XADD 命令插入一条消息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">* 表示让 Redis 为插入的数据自动生成一个全局唯一的 ID</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">往名称为 mymq 的消息队列中插入一条消息，消息的键是 name，值是 xiaolin</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">XADD mymq * name xiaolin</span></span><br><span class="line">&quot;1654254953808-0&quot;</span><br></pre></td></tr></table></figure>

<p>插入成功后会返回全局唯一的 ID：”1654254953808-0”。消息的全局唯一 ID 由两部分组成：</p>
<ul>
<li>第一部分“1654254953808”是数据插入时，以毫秒为单位计算的当前服务器时间；</li>
<li>第二部分表示插入消息在当前毫秒内的消息序号，这是从 0 开始编号的。例如，“1654254953808-0”就表示在“1654254953808”毫秒内的第 1 条消息。</li>
</ul>
<p>消费者通过 XREAD 命令从消息队列中读取消息时，可以指定一个消息 ID，并从这个消息 ID 的下一条消息开始进行读取（注意是输入消息 ID 的下一条信息开始读取，不是查询输入 ID 的消息）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">从 ID 号为 1654254953807-0 的消息开始，读取后续的所有消息（示例中一共 1 条）。</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">XREAD STREAMS mymq 1654254953807-0</span></span><br><span class="line">1) 1) &quot;mymq&quot;</span><br><span class="line">   2) 1) 1) &quot;1654254953808-0&quot;</span><br><span class="line">         2) 1) &quot;name&quot;</span><br><span class="line">            2) &quot;xiaolin&quot;</span><br></pre></td></tr></table></figure>

<p>如果<strong>想要实现阻塞读（当没有数据时，阻塞住），可以调用 XRAED 时设定 BLOCK 配置项</strong>，实现类似于 BRPOP 的阻塞读取操作。</p>
<p>比如，下面这命令，设置了 BLOCK 10000 的配置项，10000 的单位是毫秒，表明 XREAD 在读取最新消息时，如果没有消息到来，XREAD 将阻塞 10000 毫秒（即 10 秒），然后再返回。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">命令最后的“$”符号表示读取最新的消息</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">XREAD BLOCK 10000 STREAMS mymq $</span></span><br><span class="line">(nil)</span><br><span class="line">(10.00s)</span><br></pre></td></tr></table></figure>

<p>Stream 的基础方法，使用 xadd 存入消息和 xread 循环阻塞读取消息的方式可以实现简易版的消息队列，交互流程如下图所示：</p>
<blockquote>
<p>前面介绍的这些操作 List 也支持的，接下来看看 Stream 特有的功能。</p>
</blockquote>
<p>Stream 可以以使用 <strong>XGROUP 创建消费组</strong>，创建消费组之后，Stream 可以使用 XREADGROUP 命令让消费组内的消费者读取消息。</p>
<p>创建两个消费组，这两个消费组消费的消息队列是 mymq，都指定从第一条消息开始读取：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建一个名为 group1 的消费组，0-0 表示从第一条消息开始读取。</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">XGROUP CREATE mymq group1 0-0</span></span><br><span class="line">OK</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建一个名为 group2 的消费组，0-0 表示从第一条消息开始读取。</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">XGROUP CREATE mymq group2 0-0</span></span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<p>消费组 group1 内的消费者 consumer1 从 mymq 消息队列中读取所有消息的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">命令最后的参数“&gt;”，表示从第一条尚未被消费的消息开始读取。</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">XREADGROUP GROUP group1 consumer1 STREAMS mymq &gt;</span></span><br><span class="line">1) 1) &quot;mymq&quot;</span><br><span class="line">   2) 1) 1) &quot;1654254953808-0&quot;</span><br><span class="line">         2) 1) &quot;name&quot;</span><br><span class="line">            2) &quot;xiaolin&quot;</span><br></pre></td></tr></table></figure>

<p><strong>消息队列中的消息一旦被消费组里的一个消费者读取了，就不能再被该消费组内的其他消费者读取了，即同一个消费组里的消费者不能消费同一条消息</strong>。</p>
<p>比如说，我们执行完刚才的 XREADGROUP 命令后，再执行一次同样的命令，此时读到的就是空值了：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">XREADGROUP GROUP group1 consumer1 STREAMS mymq &gt;</span></span><br><span class="line">(nil)</span><br></pre></td></tr></table></figure>

<p>但是，<strong>不同消费组的消费者可以消费同一条消息（但是有前提条件，创建消息组的时候，不同消费组指定了相同位置开始读取消息）</strong>。</p>
<p>比如说，刚才 group1 消费组里的 consumer1 消费者消费了一条 id 为 1654254953808-0 的消息，现在用 group2 消费组里的 consumer1 消费者消费消息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">XREADGROUP GROUP group2 consumer1 STREAMS mymq &gt;</span></span><br><span class="line">1) 1) &quot;mymq&quot;</span><br><span class="line">   2) 1) 1) &quot;1654254953808-0&quot;</span><br><span class="line">         2) 1) &quot;name&quot;</span><br><span class="line">            2) &quot;xiaolin&quot;</span><br></pre></td></tr></table></figure>

<p>因为我创建两组的消费组都是从第一条消息开始读取，所以可以看到第二组的消费者依然可以消费 id 为 1654254953808-0 的这一条消息。因此，不同的消费组的消费者可以消费同一条消息。</p>
<p>使用消费组的目的是让组内的多个消费者共同分担读取消息，所以，我们通常会让每个消费者读取部分消息，从而实现消息读取负载在多个消费者间是均衡分布的。</p>
<p>例如，我们执行下列命令，让 group2 中的 consumer1、2、3 各自读取一条消息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">让 group2 中的 consumer1 从 mymq 消息队列中消费一条消息</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">XREADGROUP GROUP group2 consumer1 COUNT 1 STREAMS mymq &gt;</span></span><br><span class="line">1) 1) &quot;mymq&quot;</span><br><span class="line">   2) 1) 1) &quot;1654254953808-0&quot;</span><br><span class="line">         2) 1) &quot;name&quot;</span><br><span class="line">            2) &quot;xiaolin&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">让 group2 中的 consumer2 从 mymq 消息队列中消费一条消息</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">XREADGROUP GROUP group2 consumer2 COUNT 1 STREAMS mymq &gt;</span></span><br><span class="line">1) 1) &quot;mymq&quot;</span><br><span class="line">   2) 1) 1) &quot;1654256265584-0&quot;</span><br><span class="line">         2) 1) &quot;name&quot;</span><br><span class="line">            2) &quot;xiaolincoding&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">让 group2 中的 consumer3 从 mymq 消息队列中消费一条消息</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">XREADGROUP GROUP group2 consumer3 COUNT 1 STREAMS mymq &gt;</span></span><br><span class="line">1) 1) &quot;mymq&quot;</span><br><span class="line">   2) 1) 1) &quot;1654256271337-0&quot;</span><br><span class="line">         2) 1) &quot;name&quot;</span><br><span class="line">            2) &quot;Tom&quot;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>基于 Stream 实现的消息队列，如何保证消费者在发生故障或宕机再次重启后，仍然可以读取未处理完的消息？</p>
</blockquote>
<p>Streams 会自动使用内部队列（也称为 PENDING List）留存消费组里每个消费者读取的消息，直到消费者使用 XACK 命令通知 Streams“消息已经处理完成”。</p>
<p>消费确认增加了消息的可靠性，一般在业务处理完成之后，需要执行 XACK 命令确认消息已经被消费完成，整个流程的执行如下图所示：</p>
<p>如果消费者没有成功处理消息，它就不会给 Streams 发送 XACK 命令，消息仍然会留存。此时，<strong>消费者可以在重启后，用 XPENDING 命令查看已读取、但尚未确认处理完成的消息</strong>。</p>
<p>例如，我们来查看一下 group2 中各个消费者已读取、但尚未确认的消息个数，命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; XPENDING mymq group2</span><br><span class="line">1) (integer) 3</span><br><span class="line">2) &quot;1654254953808-0&quot;  # 表示 group2 中所有消费者读取的消息最小 ID</span><br><span class="line">3) &quot;1654256271337-0&quot;  # 表示 group2 中所有消费者读取的消息最大 ID</span><br><span class="line">4) 1) 1) &quot;consumer1&quot;</span><br><span class="line">      2) &quot;1&quot;</span><br><span class="line">   2) 1) &quot;consumer2&quot;</span><br><span class="line">      2) &quot;1&quot;</span><br><span class="line">   3) 1) &quot;consumer3&quot;</span><br><span class="line">      2) &quot;1&quot;</span><br></pre></td></tr></table></figure>

<p>如果想查看某个消费者具体读取了哪些数据，可以执行下面的命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看 group2 里 consumer2 已从 mymq 消息队列中读取了哪些消息</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">XPENDING mymq group2 - + 10 consumer2</span></span><br><span class="line">1) 1) &quot;1654256265584-0&quot;</span><br><span class="line">   2) &quot;consumer2&quot;</span><br><span class="line">   3) (integer) 410700</span><br><span class="line">   4) (integer) 1</span><br></pre></td></tr></table></figure>

<p>可以看到，consumer2 已读取的消息的 ID 是 1654256265584-0。</p>
<p><strong>一旦消息 1654256265584-0 被 consumer2 处理了，consumer2 就可以使用 XACK 命令通知 Streams，然后这条消息就会被删除</strong>。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">XACK mymq group2 1654256265584-0</span></span><br><span class="line">(integer) 1</span><br></pre></td></tr></table></figure>

<p>当我们再使用 XPENDING 命令查看时，就可以看到，consumer2 已经没有已读取、但尚未确认处理的消息了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">XPENDING mymq group2 - + 10 consumer2</span></span><br><span class="line">(empty array)</span><br></pre></td></tr></table></figure>

<p>好了，基于 Stream 实现的消息队列就说到这里了，小结一下：</p>
<ul>
<li>消息保序：XADD&#x2F;XREAD</li>
<li>阻塞读取：XREAD block</li>
<li>重复消息处理：Stream 在使用 XADD 命令，会自动生成全局唯一 ID；</li>
<li>消息可靠性：内部使用 PENDING List 自动保存消息，使用 XPENDING 命令查看消费组已经读取但是未被确认的消息，消费者使用 XACK 确认消息；</li>
<li>支持消费组形式消费数据</li>
</ul>
<blockquote>
<p>Redis 基于 Stream 消息队列与专业的消息队列有哪些差距？</p>
</blockquote>
<p>一个专业的消息队列，必须要做到两大块：</p>
<ul>
<li>消息不丢。</li>
<li>消息可堆积。</li>
</ul>
<p><em>1、Redis Stream 消息会丢失吗？</em></p>
<p>使用一个消息队列，其实就分为三大块：<strong>生产者、队列中间件、消费者</strong>，所以要保证消息就是保证三个环节都不能丢失数据。</p>
<p>Redis Stream 消息队列能不能保证三个环节都不丢失数据？</p>
<ul>
<li>Redis 生产者会不会丢消息？生产者会不会丢消息，取决于生产者对于异常情况的处理是否合理。从消息被生产出来，然后提交给 MQ 的过程中，只要能正常收到（MQ 中间件）的 ack 确认响应，就表示发送成功，所以只要处理好返回值和异常，如果返回异常则进行消息重发，那么这个阶段是不会出现消息丢失的。</li>
<li>Redis 消费者会不会丢消息？不会，因为 Stream（MQ 中间件）会自动使用内部队列（也称为 PENDING List）留存消费组里每个消费者读取的消息，但是未被确认的消息。消费者可以在重启后，用 XPENDING 命令查看已读取、但尚未确认处理完成的消息。等到消费者执行完业务逻辑后，再发送消费确认 XACK 命令，也能保证消息的不丢失。</li>
<li>Redis 消息中间件会不会丢消息？<strong>会</strong>，Redis 在以下 2 个场景下，都会导致数据丢失：<ul>
<li>AOF 持久化配置为每秒写盘，但这个写盘过程是异步的，Redis 宕机时会存在数据丢失的可能</li>
<li>主从复制也是异步的，<a target="_blank" rel="noopener" href="https://xiaolincoding.com/redis/cluster/master_slave_replication.html#redis-%E4%B8%BB%E4%BB%8E%E5%88%87%E6%8D%A2%E5%A6%82%E4%BD%95%E5%87%8F%E5%B0%91%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1">主从切换时，也存在丢失数据的可能</a>。</li>
</ul>
</li>
</ul>
<p>可以看到，Redis 在队列中间件环节无法保证消息不丢。像 RabbitMQ 或 Kafka 这类专业的队列中间件，在使用时是部署一个集群，生产者在发布消息时，队列中间件通常会写“多个节点”，也就是有多个副本，这样一来，即便其中一个节点挂了，也能保证集群的数据不丢失。</p>
<p><em>2、Redis Stream 消息可堆积吗？</em></p>
<p>Redis 的数据都存储在内存中，这就意味着一旦发生消息积压，则会导致 Redis 的内存持续增长，如果超过机器内存上限，就会面临被 OOM 的风险。</p>
<p>所以 Redis 的 Stream 提供了可以指定队列最大长度的功能，就是为了避免这种情况发生。</p>
<p>当指定队列最大长度时，队列长度超过上限后，旧消息会被删除，只保留固定长度的新消息。这么来看，Stream 在消息积压时，如果指定了最大长度，还是有可能丢失消息的。</p>
<p>但 Kafka、RabbitMQ 专业的消息队列它们的数据都是存储在磁盘上，当消息积压时，无非就是多占用一些磁盘空间。</p>
<p>因此，把 Redis 当作队列来使用时，会面临的 2 个问题：</p>
<ul>
<li>Redis 本身可能会丢数据；</li>
<li>面对消息挤压，内存资源会紧张；</li>
</ul>
<p>所以，能不能将 Redis 作为消息队列来使用，关键看你的业务场景：</p>
<ul>
<li>如果你的业务场景足够简单，对于数据丢失不敏感，而且消息积压概率比较小的情况下，把 Redis 当作队列是完全可以的。</li>
<li>如果你的业务有海量消息，消息积压的概率比较大，并且不能接受数据丢失，那么还是用专业的消息队列中间件吧。</li>
</ul>
<blockquote>
<p>补充：Redis 发布&#x2F;订阅机制为什么不可以作为消息队列？</p>
</blockquote>
<p>发布订阅机制存在以下缺点，都是跟丢失数据有关：</p>
<ol>
<li>发布&#x2F;订阅机制没有基于任何数据类型实现，所以不具备“数据持久化”的能力，也就是发布&#x2F;订阅机制的相关操作，不会写入到 RDB 和 AOF 中，当 Redis 宕机重启，发布&#x2F;订阅机制的数据也会全部丢失。</li>
<li>发布订阅模式是“发后既忘”的工作模式，如果有订阅者离线重连之后不能消费之前的历史消息。</li>
<li>当消费端有一定的消息积压时，也就是生产者发送的消息，消费者消费不过来时，如果超过 32M 或者是 60s 内持续保持在 8M 以上，消费端会被强行断开，这个参数是在配置文件中设置的，默认值是 <code>client-output-buffer-limit pubsub 32mb 8mb 60</code>。</li>
</ol>
<p>所以，发布&#x2F;订阅机制只适合即时通讯的场景，比如<a target="_blank" rel="noopener" href="https://xiaolincoding.com/redis/cluster/sentinel.html#%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4%E6%98%AF%E5%A6%82%E4%BD%95%E7%BB%84%E6%88%90%E7%9A%84">构建哨兵集群</a>的场景采用了发布&#x2F;订阅机制。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Redis 后续版本又支持四种数据类型，它们的应用场景如下：</p>
<ul>
<li>BitMap（2.2 版新增）：二值状态统计的场景，比如签到、判断用户登陆状态、连续签到用户总数等；</li>
<li>HyperLogLog（2.8 版新增）：海量数据基数统计的场景，比如百万级网页 UV 计数等；</li>
<li>GEO（3.2 版新增）：存储地理位置信息的场景，比如滴滴叫车；</li>
<li>Stream（5.0 版新增）：消息队列，相比于基于 List 类型实现的消息队列，有这两个特有的特性：自动生成全局唯一消息 ID，支持以消费组形式消费数据。</li>
</ul>
<p>针对 Redis 是否适合做消息队列，关键看你的业务场景：</p>
<ul>
<li>如果你的业务场景足够简单，对于数据丢失不敏感，而且消息积压概率比较小的情况下，把 Redis 当作队列是完全可以的。</li>
<li>如果你的业务有海量消息，消息积压的概率比较大，并且不能接受数据丢失，那么还是用专业的消息队列中间件吧。</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://redis.io/">Redis 官网</a></li>
<li><a target="_blank" rel="noopener" href="http://redis.cn/">Redis 官方文档中文版</a></li>
<li><a target="_blank" rel="noopener" href="https://item.jd.com/11791607.html">《Redis 实战》</a></li>
<li><a target="_blank" rel="noopener" href="https://item.jd.com/11486101.html">《Redis 设计与实现》</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/u011957758/article/details/74783347">一看就懂系列之 详解 redis 的 bitmap 在亿级项目中的应用</a></li>
<li><a target="_blank" rel="noopener" href="http://blog.getspool.com/2011/11/29/fast-easy-realtime-metrics-using-redis-BitMap/">Fast, easy, realtime metrics using Redis BitMap</a></li>
<li><a target="_blank" rel="noopener" href="https://xiaolincoding.com/redis/data_struct/command.html#string">Redis 常见数据类型和应用场景</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/7fdb1c11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/7fdb1c11/" class="post-title-link" itemprop="url">Redis 集群</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-06-24 10:45:38" itemprop="dateCreated datePublished" datetime="2020-06-24T10:45:38+08:00">2020-06-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">数据库</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/KV%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">KV数据库</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/KV%E6%95%B0%E6%8D%AE%E5%BA%93/redis/" itemprop="url" rel="index"><span itemprop="name">redis</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Redis-集群"><a href="#Redis-集群" class="headerlink" title="Redis 集群"></a>Redis 集群</h1><blockquote>
<p><strong><a target="_blank" rel="noopener" href="https://redis.io/topics/cluster-tutorial">Redis 集群（Redis Cluster）</a> 是 Redis 官方提供的“分布式数据库”方案</strong>。</p>
<p>Redis Cluster 既然被设计分布式系统，自然需要具备分布式系统的基本特性：伸缩性、高可用、一致性。</p>
<ul>
<li><strong>伸缩性</strong> - Redis Cluster 通过划分虚拟 hash 槽来进行“分区”，以实现集群的伸缩性。</li>
<li><strong>高可用</strong> - Redis Cluster 采用主从架构，支持“复制”和“自动故障转移”，以保证 Redis Cluster 的高可用。</li>
<li><strong>一致性</strong> - 根据 CAP 理论，Consistency、Availability、Partition tolerance 三者不可兼得。而 Redis Cluster 的选择是 AP，即不保证“强一致性”，尽力达到“最终一致性”。</li>
</ul>
<p>Redis Cluster 应用了 <a target="_blank" rel="noopener" href="https://ramcloud.atlassian.net/wiki/download/attachments/6586375/raft.pdf">Raft 协议</a> 协议和 Gossip 协议。</p>
<p>关键词：<code>高可用</code>、<code>监控</code>、<code>选主</code>、<code>故障转移</code>、<code>分区</code>、<code>Raft</code>、<code>Gossip</code></p>
</blockquote>
<h2 id="Redis-Cluster-分区"><a href="#Redis-Cluster-分区" class="headerlink" title="Redis Cluster 分区"></a>Redis Cluster 分区</h2><h3 id="集群节点"><a href="#集群节点" class="headerlink" title="集群节点"></a>集群节点</h3><p>Redis Cluster 由多个节点组成，节点刚启动时，彼此是相互独立的。<strong>节点通过握手（ <a target="_blank" rel="noopener" href="https://redis.io/commands/cluster-meet/"><code>CLUSTER MEET</code></a> 命令）来将其他节点添加到自己所处的集群中</strong>。</p>
<p>向一个节点发送 <code>CLUSTER MEET</code> 命令，可以让当前节点与指定 IP、PORT 的节点进行三次握手，握手成功时，当前节点会将指定节点加入所在集群。</p>
<p><strong>集群节点保存键值对以及过期时间的方式与单机 Redis 服务完全相同</strong>。</p>
<p>Redis Cluster 节点分为主节点（master）和从节点（slave）：</p>
<ul>
<li>主节点用于处理槽。</li>
<li>从节点用于复制主节点， 并在主节点下线时， 代替主节点继续处理命令请求。</li>
</ul>
<h3 id="分配-Hash-槽"><a href="#分配-Hash-槽" class="headerlink" title="分配 Hash 槽"></a>分配 Hash 槽</h3><p>分布式存储需要解决的首要问题是把整个数据集按照“<strong>分区规则</strong>” 到<strong>多个节点</strong>，即每个节点负责整体数据的一个 <strong>子集</strong>。</p>
<p><strong>Redis Cluster 将整个数据库规划为 “16384” 个虚拟的哈希槽</strong>，数据库中的每个键都属于其中一个槽。<strong>每个节点都会记录哪些槽指派给了自己， 而哪些槽又被指派给了其他节点</strong>。</p>
<p><strong>如果数据库中有任何一个槽没有得到分配，那么集群处于“下线”状态</strong>。</p>
<p>通过向节点发送 <a target="_blank" rel="noopener" href="https://redis.io/commands/cluster-addslots"><code>CLUSTER ADDSLOTS</code></a> 命令，可以将一个或多个槽指派给节点负责。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">CLUSTER ADDSLOTS 1 2 3</span></span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<p>集群中的每个节点负责一部分哈希槽，比如集群中有３个节点，则：</p>
<ul>
<li>节点Ａ存储的哈希槽范围是：0 – 5500</li>
<li>节点Ｂ存储的哈希槽范围是：5501 – 11000</li>
<li>节点Ｃ存储的哈希槽范围是：11001 – 16384</li>
</ul>
<h3 id="路由"><a href="#路由" class="headerlink" title="路由"></a>路由</h3><p>当客户端向节点发送与数据库键有关的命令时，接受命令的节点会<strong>计算出命令要处理的数据库属于哪个槽</strong>，并<strong>检查这个槽是否指派给了自己</strong>：</p>
<ul>
<li>如果键所在的槽正好指派给了当前节点，那么当前节点直接执行命令。</li>
<li>如果键所在的槽没有指派给当前节点，那么节点会向客户端返回一个 <code>MOVED</code> 错误，指引客户端重定向至正确的节点。</li>
</ul>
<h4 id="计算键属于哪个槽"><a href="#计算键属于哪个槽" class="headerlink" title="计算键属于哪个槽"></a>计算键属于哪个槽</h4><p>决定一个 key 应该分配到那个槽的算法是：<strong>计算该 key 的 CRC16 结果再模 16834</strong>。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">HASH_SLOT</span> = CRC16(KEY) mod <span class="number">16384</span></span><br></pre></td></tr></table></figure>

<p>当节点计算出 key 所属的槽为 i 之后，节点会根据以下条件判断槽是否由自己负责：</p>
<figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clusterState.slots[i] <span class="operator">=</span><span class="operator">=</span> clusterState.myself</span><br></pre></td></tr></table></figure>

<h4 id="MOVED-错误"><a href="#MOVED-错误" class="headerlink" title="MOVED 错误"></a>MOVED 错误</h4><p>节点在接到一个命令请求时，会先检查这个命令请求要处理的键所在的槽是否由自己负责， 如果不是的话， 节点将向客户端返回一个 <code>MOVED</code> 错误， <code>MOVED</code> 错误携带的信息可以指引客户端转向至正在负责相关槽的节点。</p>
<p><code>MOVED</code> 错误的格式为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MOVED &lt;slot&gt; &lt;ip&gt;:&lt;port&gt;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>提示：<code>MOVED</code> 命令的作用有点类似 HTTP 协议中的重定向。</p>
</blockquote>
<h3 id="重新分区"><a href="#重新分区" class="headerlink" title="重新分区"></a>重新分区</h3><p>对 Redis Cluster 的重新分片工作是由客户端（redis-trib）执行的， <strong>重新分片的关键是将属于某个槽的所有键值对从一个节点转移至另一个节点</strong>。</p>
<p>重新分区操作可以“<strong>在线</strong>”进行，在重新分区的过程中，集群不需要下线，并且源节点和目标节点都可以继续处理命令请求。</p>
<p>重新分区的实现原理如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/cs/database/redis/redis-cluster-trib.png" alt="img"></p>
<h3 id="ASK-错误"><a href="#ASK-错误" class="headerlink" title="ASK 错误"></a>ASK 错误</h3><p>如果节点 A 正在迁移槽 <code>i</code> 至节点 B ， 那么当节点 A 没能在自己的数据库中找到命令指定的数据库键时， 节点 A 会向客户端返回一个 <code>ASK</code> 错误， 指引客户端到节点 B 继续查找指定的数据库键。</p>
<p><code>ASK</code> 错误与 <code>MOVED</code> 的区别在于：</p>
<ul>
<li><code>MOVED</code> 错误表示槽的负责权已经从一个节点转移到了另一个节点；</li>
<li>而 <code>ASK</code> 错误只是两个节点在迁移槽的过程中使用的一种临时措施。</li>
</ul>
<p>判断 ASK 错误的过程如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/cs/database/redis/redis-ask.png" alt="img"></p>
<h2 id="Redis-Cluster-复制"><a href="#Redis-Cluster-复制" class="headerlink" title="Redis Cluster 复制"></a>Redis Cluster 复制</h2><p>Redis Cluster 中的节点分为主节点和从节点，其中主节点用于处理槽，而从节点则用于复制某个主节点，并在被复制的主节点下线时，代替下线主节点继续处理命令请求。</p>
<p>向一个节点发送命令 <code>CLUSTER REPLICATE &lt;node_id&gt;</code> 可以让接收命令的节点成为 node_id 所指定节点的从节点，并开始对主节点进行复制。</p>
<p>Redis Cluster 节点间的复制是“异步”的。</p>
<h2 id="Redis-Cluster-故障转移"><a href="#Redis-Cluster-故障转移" class="headerlink" title="Redis Cluster 故障转移"></a>Redis Cluster 故障转移</h2><h3 id="故障检测"><a href="#故障检测" class="headerlink" title="故障检测"></a>故障检测</h3><p><strong>集群中每个节点都会定期向集群中的其他节点发送 <code>PING</code> 消息，以此来检测对方是否在线</strong>。</p>
<p>节点的状态信息可以分为：</p>
<ul>
<li>在线状态；</li>
<li>疑似下线状态（<code>PFAIL</code>） - 即在规定的时间内，没有应答 <code>PING</code> 消息</li>
<li>已下线状态（<code>FAIL</code>） - 半数以上负责处理槽的主节点都将某个主节点视为“疑似下线”，则这个主节点将被标记为“已下线”</li>
</ul>
<h3 id="故障转移"><a href="#故障转移" class="headerlink" title="故障转移"></a>故障转移</h3><ol>
<li>下线主节点的所有从节点中，会有一个从节点被选中。</li>
<li>被选中的从节点会执行 <code>SLAVEOF no one</code> 命令，成为新的主节点。</li>
<li>新的主节点会撤销所有对已下线主节点的槽指派，并将这些槽全部指派给自己。</li>
<li>新的主节点向集群广播一条 <code>PONG</code> 消息，告知其他节点这个从节点已变成主节点。</li>
<li>新的主节点开始接收和自己负责处理的槽有关的命令请求，故障转移完成。</li>
</ol>
<h3 id="选主"><a href="#选主" class="headerlink" title="选主"></a>选主</h3><blockquote>
<p>Redis Sentinel 和 Redis Cluster 的选主流程非常相似，二者都基于<a target="_blank" rel="noopener" href="https://ramcloud.atlassian.net/wiki/download/attachments/6586375/raft.pdf">Raft 协议</a> 实现。</p>
</blockquote>
<ol>
<li>从节点发现自己的主节点状态为 <code>FAIL</code>。</li>
<li>从节点将自己记录的纪元（<code>epoch</code>）加 1，并广播消息，要求所有收到消息且有投票权的主节点都为自己投票。——这里的纪元（<code>epoch</code>），相当于 Raft 协议中的选期（<code>term</code>）。因个人习惯，后面统一将纪元描述为选期。</li>
<li>如果某主节点具有投票权（它正在负责处理槽），并且这个主节点尚未投票，那么主节点就返回一条确认消息，表示支持该从节点成为新的主节点。</li>
<li>每个参与选举的从节点都会根据收到的确认消息，统计自己所得的选票。</li>
<li>假设集群中存在 N 个具有投票权的主节点，那么<strong>当某从节点得到“半数以上”（<code>N / 2 + 1</code>）的选票，则该从节点当选为新的主节点</strong>。</li>
<li>由于每个选期中，任意具有投票权的主节点“只能投一票”，所以获得“半数以上”选票的从节点只能有一个。</li>
<li>如果在一个选期中，没有从节点能获得“半数以上”投票，则本次选期作废，开始进入下一个选期，直到选出新的主节点为止。</li>
</ol>
<h2 id="Redis-Cluster-通信"><a href="#Redis-Cluster-通信" class="headerlink" title="Redis Cluster 通信"></a>Redis Cluster 通信</h2><p><strong>集群中的节点通过发送和接收消息来进行通信</strong>。Redis Cluster 各实例之间的通信方式采用 <a target="_blank" rel="noopener" href="http://publicatio.bibl.u-szeged.hu/1529/1/gossip11.pdf">Gossip 协议</a>来实现。</p>
<p>Redis Cluster 采用 Gossip 协议基于两个主要目标：<strong>去中心化</strong>以及<strong>失败检测</strong>。</p>
<p>Redis Cluster 中，每个节点之间都会同步信息，但是每个节点的信息不保证实时的，即无法保证数据强一致性，但是保证“<strong>数据最终一致性</strong>”——当集群中发生节点增减、故障、主从关系变化、槽信息变更等事件时，通过不断的通信，在经过一段时间后，所有的节点都会同步集群全部节点的最新状态。</p>
<p>Redis Cluster 节点发送的消息主要有以下五种：</p>
<ul>
<li><code>MEET</code> - 请求接收方加入发送方所在的集群。</li>
<li><code>PING</code> - 集群中每个节点每隔一段时间（默认为一秒）从已知节点列表中随机选出五个节点，然后对这五个节点中最久没联系的节点发送 <code>PING</code> 消息，以此检测被选中的节点是否在线。</li>
<li><code>PONG</code> - 当接收方收到发送方发来的 <code>MEET</code> 消息或 <code>PING</code> 消息时，会返回一条 <code>PONG</code> 消息作为应答。</li>
<li><code>FAIL</code> - 当一个主节点 A 判断另一个主节点 B 已经进入 <code>FAIL</code> 状态时，节点 A 会向集群广播一条关于节点 B 的 <code>FAIL</code> 消息，所有收到这条消息的节点都会立即将节点 B 标记为已下线。</li>
<li><code>PUBLISH</code> - 当节点收到一个 <code>PUBLISH</code> 命令时，节点会执行这个命令，并向集群广播一条 <code>PUBLISH</code> 消息，所有接受到这条消息的节点都会执行相同的 <code>PUBLISH</code> 命令。</li>
</ul>
<h2 id="Redis-Cluster-应用"><a href="#Redis-Cluster-应用" class="headerlink" title="Redis Cluster 应用"></a>Redis Cluster 应用</h2><h3 id="集群功能限制"><a href="#集群功能限制" class="headerlink" title="集群功能限制"></a>集群功能限制</h3><p>Redis Cluster 相对 <strong>单机</strong>，存在一些功能限制，需要 <strong>开发人员</strong> 提前了解，在使用时做好规避。</p>
<ul>
<li><code>key</code> <strong>批量操作</strong> 支持有限：类似 <code>mset</code>、<code>mget</code> 操作，目前只支持对具有相同 <code>slot</code> 值的 <code>key</code> 执行 <strong>批量操作</strong>。对于 <strong>映射为不同</strong> <code>slot</code> 值的 <code>key</code> 由于执行 <code>mget</code>、<code>mget</code> 等操作可能存在于多个节点上，因此不被支持。</li>
<li><code>key</code> <strong>事务操作</strong> 支持有限：只支持 <strong>多</strong> <code>key</code> 在 <strong>同一节点上</strong> 的 <strong>事务操作</strong>，当多个 <code>key</code> 分布在 <strong>不同</strong> 的节点上时 <strong>无法</strong> 使用事务功能。</li>
<li><code>key</code> 作为 <strong>数据分区</strong> 的最小粒度，不能将一个 <strong>大的键值</strong> 对象如 <code>hash</code>、<code>list</code> 等映射到 <strong>不同的节点</strong>。</li>
<li>不支持 <strong>多数据库空间</strong>：<strong>单机</strong> 下的 Redis 可以支持 <code>16</code> 个数据库（<code>db0 ~ db15</code>），<strong>集群模式</strong> 下只能使用 <strong>一个</strong> 数据库空间，即 <code>db0</code>。</li>
<li><strong>复制结构</strong> 只支持一层：<strong>从节点</strong> 只能复制 <strong>主节点</strong>，不支持 <strong>嵌套树状复制</strong> 结构。</li>
</ul>
<h3 id="集群规模限制"><a href="#集群规模限制" class="headerlink" title="集群规模限制"></a>集群规模限制</h3><p>Redis Cluster 的优点是易于使用。分区、主从复制、弹性扩容这些功能都可以做到自动化，通过简单的部署就可以获得一个大容量、高可靠、高可用的 Redis 集群，并且对于应用来说，近乎于是透明的。</p>
<p>所以，<strong>Redis Cluster 非常适合构建中小规模 Redis 集群</strong>，这里的中小规模指的是，大概几个到几十个节点这样规模的 Redis 集群。</p>
<p>但是 Redis Cluster 不太适合构建超大规模集群，主要原因是，它采用了去中心化的设计。</p>
<p>Redis 的每个节点上，都保存了所有槽和节点的映射关系表，客户端可以访问任意一个节点，再通过重定向命令，找到数据所在的那个节点。那么，这个映射关系表是如何更新的呢？Redis Cluster 采用了一种去中心化的流言 (Gossip) 协议来传播集群配置的变化。</p>
<p>Gossip 协议的优点是去中心化；缺点是传播速度慢，并且是集群规模越大，传播的越慢。</p>
<h3 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h3><p>我们后面会部署一个 Redis Cluster 作为例子，在那之前，先介绍一下集群在 redis.conf 中的参数。</p>
<ul>
<li><strong>cluster-enabled</strong> <code>&lt;yes/no&gt;</code> - 如果配置”yes”则开启集群功能，此 redis 实例作为集群的一个节点，否则，它是一个普通的单一的 redis 实例。</li>
<li><strong>cluster-config-file</strong> <code>&lt;filename&gt;</code> - 注意：虽然此配置的名字叫“集群配置文件”，但是此配置文件不能人工编辑，它是集群节点自动维护的文件，主要用于记录集群中有哪些节点、他们的状态以及一些持久化参数等，方便在重启时恢复这些状态。通常是在收到请求之后这个文件就会被更新。</li>
<li><strong>cluster-node-timeout</strong> <code>&lt;milliseconds&gt;</code> - 这是集群中的节点能够失联的最大时间，超过这个时间，该节点就会被认为故障。如果主节点超过这个时间还是不可达，则用它的从节点将启动故障迁移，升级成主节点。注意，任何一个节点在这个时间之内如果还是没有连上大部分的主节点，则此节点将停止接收任何请求。</li>
<li><strong>cluster-slave-validity-factor</strong> <code>&lt;factor&gt;</code> - 如果设置成０，则无论从节点与主节点失联多久，从节点都会尝试升级成主节点。如果设置成正数，则 cluster-node-timeout 乘以 cluster-slave-validity-factor 得到的时间，是从节点与主节点失联后，此从节点数据有效的最长时间，超过这个时间，从节点不会启动故障迁移。假设 cluster-node-timeout&#x3D;5，cluster-slave-validity-factor&#x3D;10，则如果从节点跟主节点失联超过 50 秒，此从节点不能成为主节点。注意，如果此参数配置为非 0，将可能出现由于某主节点失联却没有从节点能顶上的情况，从而导致集群不能正常工作，在这种情况下，只有等到原来的主节点重新回归到集群，集群才恢复运作。</li>
<li><strong>cluster-migration-barrier</strong> <code>&lt;count&gt;</code> - 主节点需要的最小从节点数，只有达到这个数，主节点失败时，它从节点才会进行迁移。更详细介绍可以看本教程后面关于副本迁移到部分。</li>
<li><strong>cluster-require-full-coverage</strong> <code>&lt;yes/no&gt;</code> - 在部分 key 所在的节点不可用时，如果此参数设置为”yes”(默认值), 则整个集群停止接受操作；如果此参数设置为”no”，则集群依然为可达节点上的 key 提供读操作。</li>
</ul>
<h2 id="其他-Redis-集群方案"><a href="#其他-Redis-集群方案" class="headerlink" title="其他 Redis 集群方案"></a>其他 Redis 集群方案</h2><p>Redis Cluster 不太适合用于大规模集群，所以，如果要构建超大 Redis 集群，需要选择替代方案。一般有三种方案类型：</p>
<ul>
<li>客户端分区方案</li>
<li>代理分区方案</li>
<li>查询路由方案</li>
</ul>
<h3 id="客户端分区方案"><a href="#客户端分区方案" class="headerlink" title="客户端分区方案"></a>客户端分区方案</h3><p><strong>客户端</strong> 就已经决定数据会被 <strong>存储</strong> 到哪个 Redis 节点或者从哪个 Redis 节点 <strong>读取数据</strong>。其主要思想是采用 <strong>哈希算法</strong> 将 Redis 数据的 <code>key</code> 进行散列，通过 <code>hash</code> 函数，特定的 <code>key</code>会 <strong>映射</strong> 到特定的 Redis 节点上。</p>
<p><strong>客户端分区方案</strong> 的代表为 Redis Sharding，Redis Sharding 是 Redis Cluster 出来之前，业界普遍使用的 Redis <strong>多实例集群</strong> 方法。Java 的 Redis 客户端驱动库 <a target="_blank" rel="noopener" href="https://github.com/redis/jedis"><strong>Jedis</strong></a>，支持 Redis Sharding 功能，即 ShardedJedis 以及 <strong>结合缓存池</strong> 的 ShardedJedisPool。</p>
<ul>
<li><strong>优点</strong>：不使用 <strong>第三方中间件</strong>，<strong>分区逻辑</strong> 可控，<strong>配置</strong> 简单，节点之间无关联，容易 <strong>线性扩展</strong>，灵活性强。</li>
<li><strong>缺点</strong>：<strong>客户端</strong> 无法 <strong>动态增删</strong> 服务节点，客户端需要自行维护 <strong>分发逻辑</strong>，客户端之间 <strong>无连接共享</strong>，会造成 <strong>连接浪费</strong>。</li>
</ul>
<h3 id="代理分区方案"><a href="#代理分区方案" class="headerlink" title="代理分区方案"></a>代理分区方案</h3><p><strong>客户端</strong> 发送请求到一个 <strong>代理组件</strong>，<strong>代理</strong> 解析 <strong>客户端</strong> 的数据，并将请求转发至正确的节点，最后将结果回复给客户端。</p>
<ul>
<li><strong>优点</strong>：简化 <strong>客户端</strong> 的分布式逻辑，<strong>客户端</strong> 透明接入，切换成本低，代理的 <strong>转发</strong> 和 <strong>存储</strong> 分离。</li>
<li><strong>缺点</strong>：多了一层 <strong>代理层</strong>，加重了 <strong>架构部署复杂度</strong> 和 <strong>性能损耗</strong>。</li>
</ul>
<p><strong>代理分区</strong> 主流实现的有方案有 <strong><a target="_blank" rel="noopener" href="https://github.com/twitter/twemproxy">Twemproxy</a></strong> 和 <a target="_blank" rel="noopener" href="https://github.com/CodisLabs/codis"><strong>Codis</strong></a>。</p>
<h4 id="Twemproxy"><a href="#Twemproxy" class="headerlink" title="Twemproxy"></a>Twemproxy</h4><p><strong><a target="_blank" rel="noopener" href="https://github.com/twitter/twemproxy">Twemproxy</a></strong> 也叫 <code>nutcraker</code>，是 Twitter 开源的一个 Redis 和 Memcache 的 <strong>中间代理服务器</strong> 程序。</p>
<p><strong><a target="_blank" rel="noopener" href="https://github.com/twitter/twemproxy">Twemproxy</a></strong> 作为 <strong>代理</strong>，可接受来自多个程序的访问，按照 <strong>路由规则</strong>，转发给后台的各个 Redis 服务器，再原路返回。**<a target="_blank" rel="noopener" href="https://github.com/twitter/twemproxy">Twemproxy</a>** 存在 <strong>单点故障</strong> 问题，需要结合 Lvs 和 Keepalived 做 <strong>高可用方案</strong>。</p>
<ul>
<li><strong>优点</strong>：应用范围广，稳定性较高，中间代理层 <strong>高可用</strong>。</li>
<li><strong>缺点</strong>：无法平滑地 <strong>水平扩容&#x2F;缩容</strong>，无 <strong>可视化管理界面</strong>，运维不友好，出现故障，不能 <strong>自动转移</strong>。</li>
</ul>
<h4 id="Codis"><a href="#Codis" class="headerlink" title="Codis"></a>Codis</h4><p><a target="_blank" rel="noopener" href="https://github.com/CodisLabs/codis"><strong>Codis</strong></a> 是一个 <strong>分布式</strong> Redis 解决方案，对于上层应用来说，连接 Codis-Proxy 和直接连接 <strong>原生的</strong> Redis-Server 没有的区别。<a target="_blank" rel="noopener" href="https://github.com/CodisLabs/codis"><strong>Codis</strong></a> 底层会 <strong>处理请求的转发</strong>，不停机的进行 <strong>数据迁移</strong> 等工作。<a target="_blank" rel="noopener" href="https://github.com/CodisLabs/codis"><strong>Codis</strong></a> 采用了无状态的 <strong>代理层</strong>，对于 <strong>客户端</strong> 来说，一切都是透明的。</p>
<ul>
<li><strong>优点</strong>：实现了上层 Proxy 和底层 Redis 的 <strong>高可用</strong>，<strong>数据分区</strong> 和 <strong>自动平衡</strong>，提供 <strong>命令行接口</strong> 和 RESTful API，提供 <strong>监控</strong> 和 <strong>管理</strong> 界面，可以动态 <strong>添加</strong> 和 <strong>删除</strong> Redis 节点。</li>
<li><strong>缺点</strong>：<strong>部署架构</strong> 和 <strong>配置</strong> 复杂，不支持 <strong>跨机房</strong> 和 <strong>多租户</strong>，不支持 <strong>鉴权管理</strong>。</li>
</ul>
<h3 id="查询路由方案"><a href="#查询路由方案" class="headerlink" title="查询路由方案"></a>查询路由方案</h3><p><strong>客户端随机地</strong> 请求任意一个 Redis 实例，然后由 Redis 将请求 <strong>转发</strong> 给 <strong>正确</strong> 的 Redis 节点。Redis Cluster 实现了一种 <strong>混合形式</strong> 的 <strong>查询路由</strong>，但并不是 <strong>直接</strong> 将请求从一个 Redis 节点 <strong>转发</strong> 到另一个 Redis 节点，而是在 <strong>客户端</strong> 的帮助下直接 <strong>重定向</strong>（ <code>redirected</code>）到正确的 Redis 节点。</p>
<ul>
<li><strong>优点</strong>：<strong>去中心化</strong>，数据按照 <strong>槽</strong> 存储分布在多个 Redis 实例上，可以平滑的进行节点 <strong>扩容&#x2F;缩容</strong>，支持 <strong>高可用</strong> 和 <strong>自动故障转移</strong>，运维成本低。</li>
<li><strong>缺点</strong>：重度依赖 Redis-trib 工具，缺乏 <strong>监控管理</strong>，需要依赖 Smart Client (<strong>维护连接</strong>，<strong>缓存路由表</strong>，<code>MultiOp</code> 和 <code>Pipeline</code> 支持)。Failover 节点的 <strong>检测过慢</strong>，不如有 <strong>中心节点</strong> 的集群及时（如 ZooKeeper）。Gossip 消息采用广播方式，集群规模越大，开销越大。无法根据统计区分 <strong>冷热数据</strong>。</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://item.jd.com/11486101.html">《Redis 设计与实现》</a></li>
<li><a target="_blank" rel="noopener" href="https://juejin.im/post/5b8fc5536fb9a05d2d01fb11">深入剖析 Redis 系列(三) - Redis 集群模式搭建与原理详解</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/7cad568e/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/7cad568e/" class="post-title-link" itemprop="url">Redis 运维</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-06-24 10:45:38" itemprop="dateCreated datePublished" datetime="2020-06-24T10:45:38+08:00">2020-06-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">数据库</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/KV%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">KV数据库</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/KV%E6%95%B0%E6%8D%AE%E5%BA%93/redis/" itemprop="url" rel="index"><span itemprop="name">redis</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>18k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>16 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Redis-运维"><a href="#Redis-运维" class="headerlink" title="Redis 运维"></a>Redis 运维</h1><blockquote>
<p><strong>Redis</strong> 是一个高性能的 key-value 数据库。</p>
<p>SET 操作每秒钟 110000 次；GET 操作每秒钟 81000 次。</p>
</blockquote>
<h2 id="Redis-安装"><a href="#Redis-安装" class="headerlink" title="Redis 安装"></a>Redis 安装</h2><h3 id="Window-下安装"><a href="#Window-下安装" class="headerlink" title="Window 下安装"></a>Window 下安装</h3><p><strong>下载地址：</strong><a target="_blank" rel="noopener" href="https://github.com/MSOpenTech/redis/releases">https://github.com/MSOpenTech/redis/releases</a>。</p>
<p>Redis 支持 32 位和 64 位。这个需要根据你系统平台的实际情况选择，这里我们下载 <strong>Redis-x64-xxx.zip</strong>压缩包到 C 盘，解压后，将文件夹重新命名为 <strong>redis</strong>。</p>
<p>打开一个 <strong>cmd</strong> 窗口 使用 cd 命令切换目录到 <strong>C:\redis</strong> 运行 <strong>redis-server.exe redis.windows.conf</strong> 。</p>
<p>如果想方便的话，可以把 redis 的路径加到系统的环境变量里，这样就省得再输路径了，后面的那个 redis.windows.conf 可以省略，如果省略，会启用默认的。</p>
<p>这时候另启一个 cmd 窗口，原来的不要关闭，不然就无法访问服务端了。</p>
<p>切换到 redis 目录下运行 <strong>redis-cli.exe -h 127.0.0.1 -p 6379</strong> 。</p>
<h3 id="Linux-下安装"><a href="#Linux-下安装" class="headerlink" title="Linux 下安装"></a>Linux 下安装</h3><p><strong>下载地址：</strong> <a target="_blank" rel="noopener" href="http://redis.io/download%EF%BC%8C%E4%B8%8B%E8%BD%BD%E6%9C%80%E6%96%B0%E6%96%87%E6%A1%A3%E7%89%88%E6%9C%AC%E3%80%82">http://redis.io/download，下载最新文档版本。</a></p>
<p>下载、解压、编译 Redis</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget http://download.redis.io/releases/redis-5.0.4.tar.gz</span><br><span class="line">tar xzf redis-5.0.4.tar.gz</span><br><span class="line">cd redis-5.0.4</span><br><span class="line">make</span><br></pre></td></tr></table></figure>

<p>为了编译 Redis 源码，你需要 gcc-c++和 tcl。如果你的系统是 CentOS，可以直接执行命令：<code>yum install -y gcc-c++ tcl</code> 来安装。</p>
<p>进入到解压后的 <code>src</code> 目录，通过如下命令启动 Redis:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">src/redis-server</span><br></pre></td></tr></table></figure>

<p>您可以使用内置的客户端与 Redis 进行交互:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">src/redis-cli</span></span><br><span class="line"><span class="meta prompt_">redis&gt; </span><span class="language-bash"><span class="built_in">set</span> foo bar</span></span><br><span class="line">OK</span><br><span class="line"><span class="meta prompt_">redis&gt; </span><span class="language-bash">get foo</span></span><br><span class="line">&quot;bar&quot;</span><br></pre></td></tr></table></figure>

<h3 id="Ubuntu-下安装"><a href="#Ubuntu-下安装" class="headerlink" title="Ubuntu 下安装"></a>Ubuntu 下安装</h3><p>在 Ubuntu 系统安装 Redis 可以使用以下命令:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install redis-server</span><br></pre></td></tr></table></figure>

<h3 id="开机启动"><a href="#开机启动" class="headerlink" title="开机启动"></a>开机启动</h3><ul>
<li>开机启动配置：<code>echo &quot;/usr/local/bin/redis-server /etc/redis.conf&quot; &gt;&gt; /etc/rc.local</code></li>
</ul>
<h3 id="开放防火墙端口"><a href="#开放防火墙端口" class="headerlink" title="开放防火墙端口"></a>开放防火墙端口</h3><ul>
<li>添加规则：<code>iptables -I INPUT -p tcp -m tcp --dport 6379 -j ACCEPT</code></li>
<li>保存规则：<code>service iptables save</code></li>
<li>重启 iptables：<code>service iptables restart</code></li>
</ul>
<h3 id="Redis-安装脚本"><a href="#Redis-安装脚本" class="headerlink" title="Redis 安装脚本"></a>Redis 安装脚本</h3><blockquote>
<p>CentOS7 环境安装脚本：<a target="_blank" rel="noopener" href="https://github.com/dunwu/linux-tutorial/tree/master/codes/linux/soft">软件运维配置脚本集合</a></p>
</blockquote>
<p><strong>安装说明</strong></p>
<ul>
<li>采用编译方式安装 Redis, 并将其注册为 systemd 服务</li>
<li>安装路径为：<code>/usr/local/redis</code></li>
<li>默认下载安装 <code>5.0.4</code> 版本，端口号为：<code>6379</code>，密码为空</li>
</ul>
<p><strong>使用方法</strong></p>
<ul>
<li>默认安装 - 执行以下任意命令即可：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -o- https://gitee.com/turnon/linux-tutorial/raw/master/codes/linux/soft/redis-install.sh | bash</span><br><span class="line">wget -qO- https://gitee.com/turnon/linux-tutorial/raw/master/codes/linux/soft/redis-install.sh | bash</span><br></pre></td></tr></table></figure>

<ul>
<li>自定义安装 - 下载脚本到本地，并按照以下格式执行：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh redis-install.sh [version] [port] [password]</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<ul>
<li><code>version</code> - redis 版本号</li>
<li><code>port</code> - redis 服务端口号</li>
<li><code>password</code> - 访问密码</li>
</ul>
<h2 id="Redis-单机使用和配置"><a href="#Redis-单机使用和配置" class="headerlink" title="Redis 单机使用和配置"></a>Redis 单机使用和配置</h2><h3 id="启动-Redis"><a href="#启动-Redis" class="headerlink" title="启动 Redis"></a>启动 Redis</h3><p><strong>启动 redis 服务</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/redis/src</span><br><span class="line">./redis-server</span><br></pre></td></tr></table></figure>

<p><strong>启动 redis 客户端</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/redis/src</span><br><span class="line">./redis-cli</span><br></pre></td></tr></table></figure>

<p><strong>查看 redis 是否启动</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-cli</span><br></pre></td></tr></table></figure>

<p>以上命令将打开以下终端：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis 127.0.0.1:6379&gt;</span><br></pre></td></tr></table></figure>

<p>127.0.0.1 是本机 IP ，6379 是 redis 服务端口。现在我们输入 PING 命令。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">redis 127.0.0.1:6379&gt; ping</span><br><span class="line">PONG</span><br></pre></td></tr></table></figure>

<p>以上说明我们已经成功启动了 redis。</p>
<h3 id="Redis-常见配置"><a href="#Redis-常见配置" class="headerlink" title="Redis 常见配置"></a>Redis 常见配置</h3><blockquote>
<p>Redis 默认的配置文件是根目录下的 <code>redis.conf</code> 文件。</p>
<p>如果需要指定特定文件作为配置文件，需要使用命令： <code>./redis-server -c xxx.conf</code></p>
<p>每次修改配置后，需要重启才能生效。</p>
<p>Redis 官方默认配置：</p>
<ul>
<li>自描述文档 <a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/antirez/redis/2.8/redis.conf">redis.conf for Redis 2.8</a></li>
<li>自描述文档 <a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/antirez/redis/2.6/redis.conf">redis.conf for Redis 2.6</a>.</li>
<li>自描述文档 <a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/antirez/redis/2.4/redis.conf">redis.conf for Redis 2.4</a>.</li>
</ul>
<p>自 Redis2.6 起就可以直接通过命令行传递 Redis 配置参数。这种方法可以用于测试。自 Redis2.6 起就可以直接通过命令行传递 Redis 配置参数。这种方法可以用于测试。</p>
</blockquote>
<h3 id="设为守护进程"><a href="#设为守护进程" class="headerlink" title="设为守护进程"></a>设为守护进程</h3><p>Redis 默认以非守护进程方式启动，而通常我们会将 Redis 设为守护进程启动方式，配置：<code>daemonize yes</code></p>
<h4 id="远程访问"><a href="#远程访问" class="headerlink" title="远程访问"></a>远程访问</h4><p>Redis 默认绑定 127.0.0.1，这样就只能本机才能访问，若要 Redis 允许远程访问，需要配置：<code>bind 0.0.0.0</code></p>
<h4 id="设置密码"><a href="#设置密码" class="headerlink" title="设置密码"></a>设置密码</h4><p>Redis 默认访问不需要密码，如果需要设置密码，需要如下配置：</p>
<ul>
<li><code>protected-mode yes</code></li>
<li><code>requirepass &lt;密码&gt;</code></li>
</ul>
<h4 id="配置参数表"><a href="#配置参数表" class="headerlink" title="配置参数表"></a>配置参数表</h4><table>
<thead>
<tr>
<th align="left">配置项</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>daemonize no</code></td>
<td align="left">Redis 默认不是以守护进程的方式运行，可以通过该配置项修改，使用 yes 启用守护进程（Windows 不支持守护线程的配置为 no ）</td>
</tr>
<tr>
<td align="left"><code>pidfile /var/run/redis.pid</code></td>
<td align="left">当 Redis 以守护进程方式运行时，Redis 默认会把 pid 写入 &#x2F;var&#x2F;run&#x2F;redis.pid 文件，可以通过 pidfile 指定</td>
</tr>
<tr>
<td align="left"><code>port 6379</code></td>
<td align="left">指定 Redis 监听端口，默认端口为 6379，作者在自己的一篇博文中解释了为什么选用 6379 作为默认端口，因为 6379 在手机按键上 MERZ 对应的号码，而 MERZ 取自意大利歌女 Alessia Merz 的名字</td>
</tr>
<tr>
<td align="left"><code>bind 127.0.0.1</code></td>
<td align="left">绑定的主机地址</td>
</tr>
<tr>
<td align="left"><code>timeout 300</code></td>
<td align="left">当客户端闲置多长时间后关闭连接，如果指定为 0，表示关闭该功能</td>
</tr>
<tr>
<td align="left"><code>loglevel notice</code></td>
<td align="left">指定日志记录级别，Redis 总共支持四个级别：debug、verbose、notice、warning，默认为 notice</td>
</tr>
<tr>
<td align="left"><code>logfile stdout</code></td>
<td align="left">日志记录方式，默认为标准输出，如果配置 Redis 为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给 &#x2F;dev&#x2F;null</td>
</tr>
<tr>
<td align="left"><code>databases 16</code></td>
<td align="left">设置数据库的数量，默认数据库为 0，可以使用 SELECT 命令在连接上指定数据库 id</td>
</tr>
<tr>
<td align="left"><code>save &lt;seconds&gt; &lt;changes&gt;</code> Redis 默认配置文件中提供了三个条件：<strong>save 900 1</strong>、<strong>save 300 10</strong>、<strong>save 60 10000</strong> 分别表示 900 秒（15 分钟）内有 1 个更改，300 秒（5 分钟）内有 10 个更改以及 60 秒内有 10000 个更改。</td>
<td align="left">指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合</td>
</tr>
<tr>
<td align="left"><code>rdbcompression yes</code></td>
<td align="left">指定存储至本地数据库时是否压缩数据，默认为 yes，Redis 采用 LZF 压缩，如果为了节省 CPU 时间，可以关闭该选项，但会导致数据库文件变的巨大</td>
</tr>
<tr>
<td align="left"><code>dbfilename dump.rdb</code></td>
<td align="left">指定本地数据库文件名，默认值为 dump.rdb</td>
</tr>
<tr>
<td align="left"><code>dir ./</code></td>
<td align="left">指定本地数据库存放目录</td>
</tr>
<tr>
<td align="left"><code>slaveof &lt;masterip&gt; &lt;masterport&gt;</code></td>
<td align="left">设置当本机为 slav 服务时，设置 master 服务的 IP 地址及端口，在 Redis 启动时，它会自动从 master 进行数据同步</td>
</tr>
<tr>
<td align="left"><code>masterauth &lt;master-password&gt;</code></td>
<td align="left">当 master 服务设置了密码保护时，slav 服务连接 master 的密码</td>
</tr>
<tr>
<td align="left"><code>requirepass foobared</code></td>
<td align="left">设置 Redis 连接密码，如果配置了连接密码，客户端在连接 Redis 时需要通过 <code>AUTH &lt;password&gt;</code> 命令提供密码，默认关闭</td>
</tr>
<tr>
<td align="left"><code>maxclients 128</code></td>
<td align="left">设置同一时间最大客户端连接数，默认无限制，Redis 可以同时打开的客户端连接数为 Redis 进程可以打开的最大文件描述符数，如果设置 maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis 会关闭新的连接并向客户端返回 max number of clients reached 错误信息</td>
</tr>
<tr>
<td align="left"><code>maxmemory &lt;bytes&gt;</code></td>
<td align="left">指定 Redis 最大内存限制，Redis 在启动时会把数据加载到内存中，达到最大内存后，Redis 会先尝试清除已到期或即将到期的 Key，当此方法处理 后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。Redis 新的 vm 机制，会把 Key 存放内存，Value 会存放在 swap 区</td>
</tr>
<tr>
<td align="left"><code>appendonly no</code></td>
<td align="left">指定是否在每次更新操作后进行日志记录，Redis 在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis 本身同步数据文件是按上面 save 条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为 no</td>
</tr>
<tr>
<td align="left"><code>appendfilename appendonly.aof</code></td>
<td align="left">指定更新日志文件名，默认为 appendonly.aof</td>
</tr>
<tr>
<td align="left"><code>appendfsync everysec</code></td>
<td align="left">指定更新日志条件，共有 3 个可选值：<strong>no</strong>：表示等操作系统进行数据缓存同步到磁盘（快）<strong>always</strong>：表示每次更新操作后手动调用 fsync() 将数据写到磁盘（慢，安全）<strong>everysec</strong>：表示每秒同步一次（折中，默认值）</td>
</tr>
<tr>
<td align="left"><code>vm-enabled no</code></td>
<td align="left">指定是否启用虚拟内存机制，默认值为 no，简单的介绍一下，VM 机制将数据分页存放，由 Redis 将访问量较少的页即冷数据 swap 到磁盘上，访问多的页面由磁盘自动换出到内存中（在后面的文章我会仔细分析 Redis 的 VM 机制）</td>
</tr>
<tr>
<td align="left"><code>vm-swap-file /tmp/redis.swap</code></td>
<td align="left">虚拟内存文件路径，默认值为 &#x2F;tmp&#x2F;redis.swap，不可多个 Redis 实例共享</td>
</tr>
<tr>
<td align="left"><code>vm-max-memory 0</code></td>
<td align="left">将所有大于 vm-max-memory 的数据存入虚拟内存，无论 vm-max-memory 设置多小，所有索引数据都是内存存储的(Redis 的索引数据 就是 keys)，也就是说，当 vm-max-memory 设置为 0 的时候，其实是所有 value 都存在于磁盘。默认值为 0</td>
</tr>
<tr>
<td align="left"><code>vm-page-size 32</code></td>
<td align="left">Redis swap 文件分成了很多的 page，一个对象可以保存在多个 page 上面，但一个 page 上不能被多个对象共享，vm-page-size 是要根据存储的 数据大小来设定的，作者建议如果存储很多小对象，page 大小最好设置为 32 或者 64bytes；如果存储很大大对象，则可以使用更大的 page，如果不确定，就使用默认值</td>
</tr>
<tr>
<td align="left"><code>vm-pages 134217728</code></td>
<td align="left">设置 swap 文件中的 page 数量，由于页表（一种表示页面空闲或使用的 bitmap）是在放在内存中的，，在磁盘上每 8 个 pages 将消耗 1byte 的内存。</td>
</tr>
<tr>
<td align="left"><code>vm-max-threads 4</code></td>
<td align="left">设置访问 swap 文件的线程数,最好不要超过机器的核数,如果设置为 0,那么所有对 swap 文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为 4</td>
</tr>
<tr>
<td align="left"><code>glueoutputbuf yes</code></td>
<td align="left">设置在向客户端应答时，是否把较小的包合并为一个包发送，默认为开启</td>
</tr>
<tr>
<td align="left"><code>hash-max-zipmap-entries 64 hash-max-zipmap-value 512</code></td>
<td align="left">指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法</td>
</tr>
<tr>
<td align="left"><code>activerehashing yes</code></td>
<td align="left">指定是否激活重置哈希，默认为开启（后面在介绍 Redis 的哈希算法时具体介绍）</td>
</tr>
<tr>
<td align="left"><code>include /path/to/local.conf</code></td>
<td align="left">指定包含其它的配置文件，可以在同一主机上多个 Redis 实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件</td>
</tr>
</tbody></table>
<h3 id="压力测试"><a href="#压力测试" class="headerlink" title="压力测试"></a>压力测试</h3><blockquote>
<p>参考官方文档：<a target="_blank" rel="noopener" href="https://redis.io/topics/benchmarks">How fast is Redis?</a></p>
</blockquote>
<p>Redis 自带了一个性能测试工具：<code>redis-benchmark</code></p>
<p><strong>（1）基本测试</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-benchmark -q -n 100000</span><br></pre></td></tr></table></figure>

<ul>
<li><code>-q</code> 表示静默（quiet）执行</li>
<li><code>-n 100000</code> 请求 10 万次</li>
</ul>
<p><strong>（2）测试指定读写指令</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">redis-benchmark -t <span class="built_in">set</span>,lpush -n 100000 -q</span></span><br><span class="line">SET: 74239.05 requests per second</span><br><span class="line">LPUSH: 79239.30 requests per second</span><br></pre></td></tr></table></figure>

<p><strong>（3）测试 pipeline 模式下指定读写指令</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">redis-benchmark -n 1000000 -t set,get -P 16 -q</span><br><span class="line">SET: 403063.28 requests per second</span><br><span class="line">GET: 508388.41 requests per second</span><br></pre></td></tr></table></figure>

<h2 id="Redis-集群使用和配置"><a href="#Redis-集群使用和配置" class="headerlink" title="Redis 集群使用和配置"></a>Redis 集群使用和配置</h2><p>Redis 3.0 后支持集群模式。</p>
<h3 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h3><p><code>Redis</code> 集群一般由 <strong>多个节点</strong> 组成，节点数量至少为 <code>6</code> 个，才能保证组成 <strong>完整高可用</strong> 的集群。</p>
<p>理想情况当然是所有节点各自在不同的机器上，首先于资源，本人在部署 Redis 集群时，只得到 3 台服务器。所以，我计划每台服务器部署 2 个 Redis 节点。</p>
<p>【示例】最简高可用 Redis 集群规划</p>
<p>机器配置：16G 内存 + 8 核 CPU + 1T 磁盘</p>
<p>Redis 进程分配 10 G 内存。一般线上生产环境，Redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。</p>
<p>集群拓扑：三主三从；三哨兵，每个哨兵监听所有主节点。</p>
<p>估算性能：</p>
<ul>
<li>容量：三主，占用 30 G 内存，所以最大存储容量为 30 G。假设每条数据记录平均 大小为 10 K，则最大能存储 300 万条数据。</li>
<li>吞吐量：单机一般 TPS&#x2F;QPS 为 五万到八万左右。假设为五万，那么三主三从架构理论上能达到 TPS 15 万，QPS 30 万。</li>
</ul>
<h3 id="部署集群"><a href="#部署集群" class="headerlink" title="部署集群"></a>部署集群</h3><blockquote>
<p>Redis 集群节点的安装与单节点服务相同，差异仅在于部署方式。</p>
<p>注意：为了演示方便，本示例将所有 Redis 集群节点都部署在一台机器上，实际生产环境中，基本都会将节点部署在不同机器上。要求更高的，可能还要考虑多机房部署。</p>
</blockquote>
<p>（1）创建节点目录</p>
<p>我个人偏好将软件放在 <code>/opt</code> 目录下，在我的机器中，Redis 都安装在 <code>/usr/local/redis</code> 目录下。所以，下面的命令和配置都假设 Redis 安装目录为 <code>/usr/local/redis</code> 。</p>
<p>确保机器上已经安装了 Redis 后，执行以下命令，创建 Redis 集群节点实例目录：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -p /usr/local/redis/conf/7001</span><br><span class="line">sudo mkdir -p /usr/local/redis/conf/7002</span><br><span class="line">sudo mkdir -p /usr/local/redis/conf/7003</span><br><span class="line">sudo mkdir -p /usr/local/redis/conf/7004</span><br><span class="line">sudo mkdir -p /usr/local/redis/conf/7005</span><br><span class="line">sudo mkdir -p /usr/local/redis/conf/7006</span><br></pre></td></tr></table></figure>

<p>（2）配置集群节点</p>
<p>每个实例目录下，新建 <code>redis.conf</code> 配置文件。</p>
<p>实例配置模板以 7001 节点为例（其他节点，完全替换配置中的端口号 7001 即可），如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">端口号</span></span><br><span class="line">port 7001</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">绑定的主机端口（0.0.0.0 表示允许远程访问）</span></span><br><span class="line">bind 0.0.0.0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">以守护进程方式启动</span></span><br><span class="line">daemonize yes</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">开启集群模式</span></span><br><span class="line">cluster-enabled yes</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">集群的配置，配置文件首次启动自动生成</span></span><br><span class="line">cluster-config-file /usr/local/redis/conf/7001/7001.conf</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">请求超时时间，设置 10 秒</span></span><br><span class="line">cluster-node-timeout 10000</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">开启 AOF 持久化</span></span><br><span class="line">appendonly yes</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">数据存放目录</span></span><br><span class="line">dir /usr/local/redis/conf/7001</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进程文件</span></span><br><span class="line">pidfile /usr/local/redis/conf/7001/7001.pid</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">日志文件</span></span><br><span class="line">logfile /usr/local/redis/conf/7001/7001.log</span><br></pre></td></tr></table></figure>

<p>（3）批量启动 Redis 节点</p>
<p>Redis 的 utils&#x2F;create-cluster 目录下自带了一个名为 create-cluster 的脚本工具，可以利用它来新建、启动、停止、重启 Redis 节点。</p>
<p>脚本中有几个关键参数：</p>
<ul>
<li><code>PORT</code>&#x3D;30000 - 初始端口号</li>
<li><code>TIMEOUT</code>&#x3D;2000 - 超时时间</li>
<li><code>NODES</code>&#x3D;6 - 节点数</li>
<li><code>REPLICAS</code>&#x3D;1 - 备份数</li>
</ul>
<p>脚本中的每个命令项会根据初始端口号，以及设置的节点数，遍历的去执行操作。</p>
<p>由于前面的规划中，节点端口是从 7001 ~ 7006，所以需要将 PORT 变量设为 7000。</p>
<p>脚本中启动每个 Redis 节点是通过指定命令行参数来配置属性。所以，我们需要改一下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">PORT=7000</span><br><span class="line">TIMEOUT=2000</span><br><span class="line">NODES=6</span><br><span class="line">ENDPORT=$((PORT+NODES))</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">...</span></span><br><span class="line"></span><br><span class="line">if [ &quot;$1&quot; == &quot;start&quot; ]</span><br><span class="line">then</span><br><span class="line">    while [ $((PORT &lt; ENDPORT)) != &quot;0&quot; ]; do</span><br><span class="line">        PORT=$((PORT+1))</span><br><span class="line">        echo &quot;Starting $PORT&quot;</span><br><span class="line">        /usr/local/redis/src/redis-server /usr/local/redis/conf/$&#123;PORT&#125;/redis.conf</span><br><span class="line">    done</span><br><span class="line">    exit 0</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>好了，在每台服务器上，都执行 <code>./create-cluster start</code> 来启动节点。</p>
<p>然后，通过 ps 命令来确认 Redis 进程是否已经工作：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">root @ dbClusterDev01 <span class="keyword">in</span> /usr/local/redis/conf [11:07:55]</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ps -ef | grep redis</span></span><br><span class="line">root      4604     1  0 11:07 ?        00:00:00 /opt/redis/src/redis-server 0.0.0.0:7001 [cluster]</span><br><span class="line">root      4609     1  0 11:07 ?        00:00:00 /opt/redis/src/redis-server 0.0.0.0:7002 [cluster]</span><br><span class="line">root      4614     1  0 11:07 ?        00:00:00 /opt/redis/src/redis-server 0.0.0.0:7003 [cluster]</span><br><span class="line">root      4619     1  0 11:07 ?        00:00:00 /opt/redis/src/redis-server 0.0.0.0:7004 [cluster]</span><br><span class="line">root      4624     1  0 11:07 ?        00:00:00 /opt/redis/src/redis-server 0.0.0.0:7005 [cluster]</span><br><span class="line">root      4629     1  0 11:07 ?        00:00:00 /opt/redis/src/redis-server 0.0.0.0:7006 [cluster]</span><br></pre></td></tr></table></figure>

<p>（4）启动集群</p>
<p>通过 <code>redis-cli --cluster create</code> 命令可以自动配置集群，如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./redis-cli --cluster create 127.0.0.1:7001 127.0.0.1:7002 127.0.0.2:7003 127.0.0.2:7004 127.0.0.3:7005 127.0.0.3:7006 --cluster-replicas 1</span><br></pre></td></tr></table></figure>

<p>redis-cluster 会根据设置的节点数和副本数自动分片（分配 Hash 虚拟槽 slot），如果满意，输入 yes ，直接开始分片。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; Performing <span class="built_in">hash</span> slots allocation on 6 nodes...</span></span><br><span class="line">Master[0] -&gt; Slots 0 - 5460</span><br><span class="line">Master[1] -&gt; Slots 5461 - 10922</span><br><span class="line">Master[2] -&gt; Slots 10923 - 16383</span><br><span class="line">Adding replica 127.0.0.2:7004 to 127.0.0.1:7001</span><br><span class="line">Adding replica 127.0.0.3:7006 to 127.0.0.2:7003</span><br><span class="line">Adding replica 127.0.0.1:7002 to 127.0.0.3:7005</span><br><span class="line">M: b721235997deb6b9a7a2be690b5b9663db8057c6 127.0.0.1:7001</span><br><span class="line">   slots:[0-5460] (5461 slots) master</span><br><span class="line">S: bda9b7036df0bbefe601bda4ce45d3787a2e9bd9 127.0.0.1:7002</span><br><span class="line">   replicates 3623fff69b5243ed18c02a2fbb6f53069b0f1505</span><br><span class="line">M: 91523c0391a044da6cc9f53bb965aabe89502187 127.0.0.2:7003</span><br><span class="line">   slots:[5461-10922] (5462 slots) master</span><br><span class="line">S: 9d899cbe49dead7b8c4f769920cdb75714a441ae 127.0.0.2:7004</span><br><span class="line">   replicates b721235997deb6b9a7a2be690b5b9663db8057c6</span><br><span class="line">M: 3623fff69b5243ed18c02a2fbb6f53069b0f1505 127.0.0.3:7005</span><br><span class="line">   slots:[10923-16383] (5461 slots) master</span><br><span class="line">S: a2869dc153ea4977ca790b76483574a5d56cb40e 127.0.0.3:7006</span><br><span class="line">   replicates 91523c0391a044da6cc9f53bb965aabe89502187</span><br><span class="line">Can I set the above configuration? (type &#x27;yes&#x27; to accept): yes</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; Nodes configuration updated</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; Assign a different config epoch to each node</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; Sending CLUSTER MEET messages to <span class="built_in">join</span> the cluster</span></span><br><span class="line">Waiting for the cluster to join</span><br><span class="line">....</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7001)</span></span><br><span class="line">M: b721235997deb6b9a7a2be690b5b9663db8057c6 127.0.0.1:7001</span><br><span class="line">   slots:[0-5460] (5461 slots) master</span><br><span class="line">   1 additional replica(s)</span><br><span class="line">S: a2869dc153ea4977ca790b76483574a5d56cb40e 127.0.0.1:7006</span><br><span class="line">   slots: (0 slots) slave</span><br><span class="line">   replicates 91523c0391a044da6cc9f53bb965aabe89502187</span><br><span class="line">M: 91523c0391a044da6cc9f53bb965aabe89502187 127.0.0.1:7003</span><br><span class="line">   slots:[5461-10922] (5462 slots) master</span><br><span class="line">   1 additional replica(s)</span><br><span class="line">M: 3623fff69b5243ed18c02a2fbb6f53069b0f1505 127.0.0.1:7005</span><br><span class="line">   slots:[10923-16383] (5461 slots) master</span><br><span class="line">   1 additional replica(s)</span><br><span class="line">S: 9d899cbe49dead7b8c4f769920cdb75714a441ae 127.0.0.1:7004</span><br><span class="line">   slots: (0 slots) slave</span><br><span class="line">   replicates b721235997deb6b9a7a2be690b5b9663db8057c6</span><br><span class="line">S: bda9b7036df0bbefe601bda4ce45d3787a2e9bd9 127.0.0.1:7002</span><br><span class="line">   slots: (0 slots) slave</span><br><span class="line">   replicates 3623fff69b5243ed18c02a2fbb6f53069b0f1505</span><br><span class="line">[OK] All nodes agree about slots configuration.</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; Check <span class="keyword">for</span> open slots...</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; Check slots coverage...</span></span><br><span class="line">[OK] All 16384 slots covered.</span><br></pre></td></tr></table></figure>

<p>（5）日常维护操作</p>
<ul>
<li>关闭集群 - <code>./create-cluster stop</code></li>
<li>检查集群是否健康（指定任意节点即可）：<code>./redis-cli --cluster check &lt;ip:port&gt;</code></li>
<li>尝试修复集群节点：<code>./redis-cli --cluster fix &lt;ip:port&gt;</code></li>
</ul>
<h3 id="部署哨兵"><a href="#部署哨兵" class="headerlink" title="部署哨兵"></a>部署哨兵</h3><p>redis-cluster 实现了 Redis 的分片、复制。</p>
<p>但 redis-cluster 没有解决故障转移问题，一旦任意分片的 Master 节点宕机、网络不通，就会导致 redis-cluster 的集群不能工作。为了解决高可用的问题，Redis 提供了 Redis 哨兵来监控 Redis 节点状态，并且会在 Master 宕机时，发起选举，将这个 Master 的一个 Slave 节点选举为 Master。</p>
<p>（1）创建节点目录</p>
<p>我个人偏好将软件放在 <code>/opt</code> 目录下，在我的机器中，Redis 都安装在 <code>/usr/local/redis</code> 目录下。所以，下面的命令和配置都假设 Redis 安装目录为 <code>/usr/local/redis</code> 。</p>
<p>确保机器上已经安装了 Redis 后，执行以下命令，创建 Redis 集群节点实例目录：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -p /usr/local/redis/conf/27001</span><br><span class="line">sudo mkdir -p /usr/local/redis/conf/27002</span><br><span class="line">sudo mkdir -p /usr/local/redis/conf/27003</span><br></pre></td></tr></table></figure>

<p>（2）配置集群节点</p>
<p>每个实例目录下，新建 <code>redis.conf</code> 配置文件。</p>
<p>实例配置模板以 7001 节点为例（其他节点，完全替换配置中的端口号 7001 即可），如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">port 27001</span><br><span class="line">daemonize yes</span><br><span class="line">sentinel monitor redis-master 172.22.6.3 7001 2</span><br><span class="line">sentinel down-after-milliseconds redis-master 5000</span><br><span class="line">sentinel failover-timeout redis-master 900000</span><br><span class="line">sentinel parallel-syncs redis-master 1</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">sentinel auth-pass redis-master 123456</span></span><br><span class="line">logfile /usr/local/redis/conf/27001/27001.log</span><br></pre></td></tr></table></figure>

<p>（3）批量启动哨兵节点</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/opt/</span>redis<span class="regexp">/src/</span>redis-sentinel <span class="regexp">/usr/</span>local<span class="regexp">/redis/</span>conf<span class="regexp">/27001/</span>sentinel.conf</span><br><span class="line"><span class="regexp">/opt/</span>redis<span class="regexp">/src/</span>redis-sentinel <span class="regexp">/usr/</span>local<span class="regexp">/redis/</span>conf<span class="regexp">/27002/</span>sentinel.conf</span><br><span class="line"><span class="regexp">/opt/</span>redis<span class="regexp">/src/</span>redis-sentinel <span class="regexp">/usr/</span>local<span class="regexp">/redis/</span>conf<span class="regexp">/27003/</span>sentinel.conf</span><br></pre></td></tr></table></figure>

<h3 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h3><p>（1）查看信息</p>
<p>进入任意节点</p>
<figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./redis-<span class="keyword">cli</span> -h <span class="number">172.22</span><span class="number">.6</span><span class="number">.3</span> -p <span class="number">7001</span></span><br></pre></td></tr></table></figure>

<p>cluster info 查看集群节点状态</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">172</span>.<span class="number">22</span>.<span class="number">6</span>.<span class="number">3</span>:<span class="number">7001</span>&gt; cluster nodes</span><br><span class="line"><span class="attribute">f158bf70bb2767cac271ce4efcfc14ba0b7ca98b</span> <span class="number">172.22.6.3:7006</span>@<span class="number">17006</span> slave e7aa182e756b76ec85b471797db9b66e4b2da725 <span class="number">0</span> <span class="number">1594528179000</span> <span class="number">6</span> connected</span><br><span class="line"><span class="attribute">f348e67648460c7a800120d69b4977bf2e4524cb</span> <span class="number">172.22.6.3:7001</span>@<span class="number">17001</span> myself,master - <span class="number">0</span> <span class="number">1594528179000</span> <span class="number">1</span> connected <span class="number">0</span>-<span class="number">5460</span></span><br><span class="line"><span class="attribute">52601e2d4af0e64b83f4cc6d20e8316d0ac38b99</span> <span class="number">172.22.6.3:7004</span>@<span class="number">17004</span> slave <span class="number">4802</span>fafe897160c46392c6e569d6f5e466cca696 <span class="number">0</span> <span class="number">1594528178000</span> <span class="number">4</span> connected</span><br><span class="line"><span class="attribute">c6c6a68674ae8aac3c6ec792c8af4dc1228c6c31</span> <span class="number">172.22.6.3:7005</span>@<span class="number">17005</span> slave f348e67648460c7a800120d69b4977bf2e4524cb <span class="number">0</span> <span class="number">1594528179852</span> <span class="number">5</span> connected</span><br><span class="line"><span class="attribute">e7aa182e756b76ec85b471797db9b66e4b2da725</span> <span class="number">172.22.6.3:7002</span>@<span class="number">17002</span> master - <span class="number">0</span> <span class="number">1594528178000</span> <span class="number">2</span> connected <span class="number">5461</span>-<span class="number">10922</span></span><br><span class="line"><span class="attribute">4802fafe897160c46392c6e569d6f5e466cca696</span> <span class="number">172.22.6.3:7003</span>@<span class="number">17003</span> master - <span class="number">0</span> <span class="number">1594528178000</span> <span class="number">3</span> connected <span class="number">10923</span>-<span class="number">16383</span></span><br></pre></td></tr></table></figure>

<p>cluster info 查看集群信息</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">172</span>.<span class="number">22</span>.<span class="number">6</span>.<span class="number">3</span>:<span class="number">7001</span>&gt; cluster info</span><br><span class="line"><span class="attribute">cluster_state</span>:ok</span><br><span class="line"><span class="attribute">cluster_slots_assigned</span>:<span class="number">16384</span></span><br><span class="line"><span class="attribute">cluster_slots_ok</span>:<span class="number">16384</span></span><br><span class="line"><span class="attribute">cluster_slots_pfail</span>:<span class="number">0</span></span><br><span class="line"><span class="attribute">cluster_slots_fail</span>:<span class="number">0</span></span><br><span class="line"><span class="attribute">cluster_known_nodes</span>:<span class="number">6</span></span><br><span class="line"><span class="attribute">cluster_size</span>:<span class="number">3</span></span><br><span class="line"><span class="attribute">cluster_current_epoch</span>:<span class="number">6</span></span><br><span class="line"><span class="attribute">cluster_my_epoch</span>:<span class="number">1</span></span><br><span class="line"><span class="attribute">cluster_stats_messages_ping_sent</span>:<span class="number">3406</span></span><br><span class="line"><span class="attribute">cluster_stats_messages_pong_sent</span>:<span class="number">3569</span></span><br><span class="line"><span class="attribute">cluster_stats_messages_publish_sent</span>:<span class="number">5035</span></span><br><span class="line"><span class="attribute">cluster_stats_messages_sent</span>:<span class="number">12010</span></span><br><span class="line"><span class="attribute">cluster_stats_messages_ping_received</span>:<span class="number">3564</span></span><br><span class="line"><span class="attribute">cluster_stats_messages_pong_received</span>:<span class="number">3406</span></span><br><span class="line"><span class="attribute">cluster_stats_messages_meet_received</span>:<span class="number">5</span></span><br><span class="line"><span class="attribute">cluster_stats_messages_publish_received</span>:<span class="number">5033</span></span><br><span class="line"><span class="attribute">cluster_stats_messages_received</span>:<span class="number">12008</span></span><br></pre></td></tr></table></figure>

<p>（2）添加节点到集群</p>
<p>将已启动的节点实例添加到集群中</p>
<figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-cli --cluster<span class="built_in"> add-node </span>127.0.0.1:7007 127.0.0.1:7008</span><br></pre></td></tr></table></figure>

<p><strong>添加主节点</strong></p>
<p>添加一组主节点</p>
<figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">.</span>/redis-cli --cluster<span class="built_in"> add-node </span>172.22.6.3:7007 172.22.6.3:7001</span><br><span class="line"><span class="keyword">.</span>/redis-cli --cluster<span class="built_in"> add-node </span>172.22.6.3:7008 172.22.6.3:7001</span><br><span class="line"><span class="keyword">.</span>/redis-cli --cluster<span class="built_in"> add-node </span>172.22.6.3:7009 172.22.6.3:7001</span><br></pre></td></tr></table></figure>

<p>查看节点状态</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">172</span>.<span class="number">22</span>.<span class="number">6</span>.<span class="number">3</span>:<span class="number">7001</span>&gt; cluster nodes</span><br><span class="line"><span class="attribute">f158bf70bb2767cac271ce4efcfc14ba0b7ca98b</span> <span class="number">172.22.6.3:7006</span>@<span class="number">17006</span> slave e7aa182e756b76ec85b471797db9b66e4b2da725 <span class="number">0</span> <span class="number">1594529342575</span> <span class="number">6</span> connected</span><br><span class="line"><span class="attribute">f348e67648460c7a800120d69b4977bf2e4524cb</span> <span class="number">172.22.6.3:7001</span>@<span class="number">17001</span> myself,master - <span class="number">0</span> <span class="number">1594529340000</span> <span class="number">1</span> connected <span class="number">0</span>-<span class="number">5460</span></span><br><span class="line"><span class="attribute">55cacf121662833a4a19dbeb4a5df712cfedf77f</span> <span class="number">172.22.6.3:7009</span>@<span class="number">17009</span> master - <span class="number">0</span> <span class="number">1594529342000</span> <span class="number">0</span> connected</span><br><span class="line"><span class="attribute">c6c6a68674ae8aac3c6ec792c8af4dc1228c6c31</span> <span class="number">172.22.6.3:7005</span>@<span class="number">17005</span> slave f348e67648460c7a800120d69b4977bf2e4524cb <span class="number">0</span> <span class="number">1594529341573</span> <span class="number">5</span> connected</span><br><span class="line"><span class="attribute">4802fafe897160c46392c6e569d6f5e466cca696</span> <span class="number">172.22.6.3:7003</span>@<span class="number">17003</span> master - <span class="number">0</span> <span class="number">1594529343577</span> <span class="number">3</span> connected <span class="number">10923</span>-<span class="number">16383</span></span><br><span class="line"><span class="attribute">e7aa182e756b76ec85b471797db9b66e4b2da725</span> <span class="number">172.22.6.3:7002</span>@<span class="number">17002</span> master - <span class="number">0</span> <span class="number">1594529342000</span> <span class="number">2</span> connected <span class="number">5461</span>-<span class="number">10922</span></span><br><span class="line"><span class="attribute">e5ba78fe629115977a74fbbe1478caf8868d6d55</span> <span class="number">172.22.6.3:7007</span>@<span class="number">17007</span> master - <span class="number">0</span> <span class="number">1594529341000</span> <span class="number">0</span> connected</span><br><span class="line"><span class="attribute">52601e2d4af0e64b83f4cc6d20e8316d0ac38b99</span> <span class="number">172.22.6.3:7004</span>@<span class="number">17004</span> slave <span class="number">4802</span>fafe897160c46392c6e569d6f5e466cca696 <span class="number">0</span> <span class="number">1594529340000</span> <span class="number">4</span> connected</span><br><span class="line"><span class="attribute">79d4fffc2cec210556c3b4c44e63ab506e87eda3</span> <span class="number">172.22.6.3:7008</span>@<span class="number">17008</span> master - <span class="number">0</span> <span class="number">1594529340000</span> <span class="number">7</span> connected</span><br></pre></td></tr></table></figure>

<p>可以发现，新加入的三个主节点，还没有分配哈希槽，所以，暂时还无法访问。</p>
<p><strong>添加从节点</strong></p>
<p>–slave：设置该参数，则新节点以 slave 的角色加入集群<br>–master-id：这个参数需要设置了–slave 才能生效，–master-id 用来指定新节点的 master 节点。如果不设置该参数，则会随机为节点选择 master 节点。</p>
<p>语法</p>
<figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">redis-cli --cluster<span class="built_in"> add-node </span> 新节点IP地址：端口    存在节点IP：端口 --cluster-slave （从节点） --cluster-master-id （master节点的ID）</span><br><span class="line">redis-cli --cluster<span class="built_in"> add-node </span>  10.42.141.119:6379  10.42.166.105:6379  --cluster-slave   --cluster-master-id  dfa238fff8a7a49230cff7eb74f573f5645c8ec5</span><br></pre></td></tr></table></figure>

<p>示例</p>
<figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">.</span>/redis-cli --cluster<span class="built_in"> add-node </span>172.22.6.3:7010 172.22.6.3:7007 --cluster-slave</span><br><span class="line"><span class="keyword">.</span>/redis-cli --cluster<span class="built_in"> add-node </span>172.22.6.3:7011 172.22.6.3:7008 --cluster-slave</span><br><span class="line"><span class="keyword">.</span>/redis-cli --cluster<span class="built_in"> add-node </span>172.22.6.3:7012 172.22.6.3:7009 --cluster-slave</span><br></pre></td></tr></table></figure>

<p>查看状态</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">172</span>.<span class="number">22</span>.<span class="number">6</span>.<span class="number">3</span>:<span class="number">7001</span>&gt; cluster nodes</span><br><span class="line"><span class="attribute">ef5c1b9ce4cc795dc12b2c1e8736a572647b4c3e</span> <span class="number">172.22.6.3:7011</span>@<span class="number">17011</span> slave <span class="number">79</span>d4fffc2cec210556c3b4c44e63ab506e87eda3 <span class="number">0</span> <span class="number">1594529492043</span> <span class="number">7</span> connected</span><br><span class="line"><span class="attribute">f158bf70bb2767cac271ce4efcfc14ba0b7ca98b</span> <span class="number">172.22.6.3:7006</span>@<span class="number">17006</span> slave e7aa182e756b76ec85b471797db9b66e4b2da725 <span class="number">0</span> <span class="number">1594529491943</span> <span class="number">6</span> connected</span><br><span class="line"><span class="attribute">f348e67648460c7a800120d69b4977bf2e4524cb</span> <span class="number">172.22.6.3:7001</span>@<span class="number">17001</span> myself,master - <span class="number">0</span> <span class="number">1594529488000</span> <span class="number">1</span> connected <span class="number">0</span>-<span class="number">5460</span></span><br><span class="line"><span class="attribute">5140d1129ed850df59c51cf818c4eb74545d9959</span> <span class="number">172.22.6.3:7010</span>@<span class="number">17010</span> slave e5ba78fe629115977a74fbbe1478caf8868d6d55 <span class="number">0</span> <span class="number">1594529488000</span> <span class="number">0</span> connected</span><br><span class="line"><span class="attribute">55cacf121662833a4a19dbeb4a5df712cfedf77f</span> <span class="number">172.22.6.3:7009</span>@<span class="number">17009</span> master - <span class="number">0</span> <span class="number">1594529488000</span> <span class="number">8</span> connected</span><br><span class="line"><span class="attribute">c6c6a68674ae8aac3c6ec792c8af4dc1228c6c31</span> <span class="number">172.22.6.3:7005</span>@<span class="number">17005</span> slave f348e67648460c7a800120d69b4977bf2e4524cb <span class="number">0</span> <span class="number">1594529490000</span> <span class="number">5</span> connected</span><br><span class="line"><span class="attribute">4802fafe897160c46392c6e569d6f5e466cca696</span> <span class="number">172.22.6.3:7003</span>@<span class="number">17003</span> master - <span class="number">0</span> <span class="number">1594529489939</span> <span class="number">3</span> connected <span class="number">10923</span>-<span class="number">16383</span></span><br><span class="line"><span class="attribute">e7aa182e756b76ec85b471797db9b66e4b2da725</span> <span class="number">172.22.6.3:7002</span>@<span class="number">17002</span> master - <span class="number">0</span> <span class="number">1594529491000</span> <span class="number">2</span> connected <span class="number">5461</span>-<span class="number">10922</span></span><br><span class="line"><span class="attribute">e5ba78fe629115977a74fbbe1478caf8868d6d55</span> <span class="number">172.22.6.3:7007</span>@<span class="number">17007</span> master - <span class="number">0</span> <span class="number">1594529490942</span> <span class="number">0</span> connected</span><br><span class="line"><span class="attribute">52601e2d4af0e64b83f4cc6d20e8316d0ac38b99</span> <span class="number">172.22.6.3:7004</span>@<span class="number">17004</span> slave <span class="number">4802</span>fafe897160c46392c6e569d6f5e466cca696 <span class="number">0</span> <span class="number">1594529491000</span> <span class="number">4</span> connected</span><br><span class="line"><span class="attribute">02e9f57b5b45c350dc57acf1c8efa8db136db7b7</span> <span class="number">172.22.6.3:7012</span>@<span class="number">17012</span> master - <span class="number">0</span> <span class="number">1594529489000</span> <span class="number">0</span> connected</span><br><span class="line"><span class="attribute">79d4fffc2cec210556c3b4c44e63ab506e87eda3</span> <span class="number">172.22.6.3:7008</span>@<span class="number">17008</span> master - <span class="number">0</span> <span class="number">1594529489000</span> <span class="number">7</span> connected</span><br></pre></td></tr></table></figure>

<p>分配哈希槽</p>
<p>执行 <code>./redis-cli --cluster rebalance 172.22.6.3:7001 --cluster-threshold 1 --cluster-use-empty-masters</code></p>
<p>参数说明：</p>
<p>rebalance：表明让 Redis 自动根据节点数进行均衡哈希槽分配。</p>
<p>–cluster-use-empty-masters：表明</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20200712125827.png" alt="img"></p>
<p>执行结束后，查看状态：</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20200712130234.png" alt="img"></p>
<h2 id="Redis-命令"><a href="#Redis-命令" class="headerlink" title="Redis 命令"></a>Redis 命令</h2><h3 id="通用命令"><a href="#通用命令" class="headerlink" title="通用命令"></a>通用命令</h3><blockquote>
<p>命令详细用法，请参考 <a target="_blank" rel="noopener" href="https://redis.io/commands"><strong>Redis 命令官方文档</strong></a></p>
<p>搬迁两张 cheat sheet 图，原址：<a target="_blank" rel="noopener" href="https://www.cheatography.com/tasjaevan/cheat-sheets/redis/">https://www.cheatography.com/tasjaevan/cheat-sheets/redis/</a></p>
</blockquote>
<h3 id="集群命令"><a href="#集群命令" class="headerlink" title="集群命令"></a>集群命令</h3><ul>
<li><strong>集群</strong><ul>
<li><code>cluster info</code> - 打印集群的信息</li>
<li><code>cluster nodes</code> - 列出集群当前已知的所有节点（ node），以及这些节点的相关信息。</li>
</ul>
</li>
<li><strong>节点</strong><ul>
<li><code>cluster meet &lt;ip&gt; &lt;port&gt;</code> - 将 ip 和 port 所指定的节点添加到集群当中，让它成为集群的一份子。</li>
<li><code>cluster forget &lt;node_id&gt;</code> - 从集群中移除 node_id 指定的节点。</li>
<li><code>cluster replicate &lt;node_id&gt;</code> - 将当前节点设置为 node_id 指定的节点的从节点。</li>
<li><code>cluster saveconfig</code> - 将节点的配置文件保存到硬盘里面。</li>
</ul>
</li>
<li><strong>槽(slot)</strong><ul>
<li><code>cluster addslots &lt;slot&gt; [slot ...]</code> - 将一个或多个槽（ slot）指派（ assign）给当前节点。</li>
<li><code>cluster delslots &lt;slot&gt; [slot ...]</code> - 移除一个或多个槽对当前节点的指派。</li>
<li><code>cluster flushslots</code> - 移除指派给当前节点的所有槽，让当前节点变成一个没有指派任何槽的节点。</li>
<li><code>cluster setslot &lt;slot&gt; node &lt;node_id&gt;</code> - 将槽 slot 指派给 node_id 指定的节点，如果槽已经指派给另一个节点，那么先让另一个节点删除该槽&gt;，然后再进行指派。</li>
<li><code>cluster setslot &lt;slot&gt; migrating &lt;node_id&gt;</code> - 将本节点的槽 slot 迁移到 node_id 指定的节点中。</li>
<li><code>cluster setslot &lt;slot&gt; importing &lt;node_id&gt;</code> - 从 node_id 指定的节点中导入槽 slot 到本节点。</li>
<li><code>cluster setslot &lt;slot&gt; stable</code> - 取消对槽 slot 的导入（ import）或者迁移（ migrate）。</li>
</ul>
</li>
<li><strong>键</strong><ul>
<li><code>cluster keyslot &lt;key&gt;</code> - 计算键 key 应该被放置在哪个槽上。</li>
<li><code>cluster countkeysinslot &lt;slot&gt;</code> - 返回槽 slot 目前包含的键值对数量。</li>
<li><code>cluster getkeysinslot &lt;slot&gt; &lt;count&gt;</code> - 返回 count 个 slot 槽中的键。</li>
</ul>
</li>
</ul>
<h3 id="重新分片"><a href="#重新分片" class="headerlink" title="重新分片"></a>重新分片</h3><p>添加节点：.&#x2F;redis-cli –cluster add-node 192.168.1.136:7007 192.168.1.136:7001 –cluster-slave</p>
<p>redis-cli –cluster reshard 172.22.6.3 7001</p>
<h2 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h2><p>推荐使用 <a target="_blank" rel="noopener" href="https://github.com/uglide/RedisDesktopManager"><strong>RedisDesktopManager</strong></a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><strong>官网</strong><ul>
<li><a target="_blank" rel="noopener" href="https://redis.io/">Redis 官网</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/antirez/redis">Redis Github</a></li>
<li><a target="_blank" rel="noopener" href="http://redis.cn/">Redis 官方文档中文版</a></li>
</ul>
</li>
<li><strong>书籍</strong><ul>
<li><a target="_blank" rel="noopener" href="https://item.jd.com/11791607.html">《Redis 实战》</a></li>
<li><a target="_blank" rel="noopener" href="https://item.jd.com/11486101.html">《Redis 设计与实现》</a></li>
</ul>
</li>
<li><strong>教程</strong><ul>
<li><a target="_blank" rel="noopener" href="http://redisdoc.com/">Redis 命令参考</a></li>
</ul>
</li>
<li><strong>文章</strong><ul>
<li><a target="_blank" rel="noopener" href="https://juejin.im/post/5b8fc5536fb9a05d2d01fb11">深入剖析 Redis 系列(三) - Redis 集群模式搭建与原理详解</a></li>
</ul>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/ce65c23b/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/ce65c23b/" class="post-title-link" itemprop="url">Flink 入门</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-06-22 00:22:25" itemprop="dateCreated datePublished" datetime="2020-06-22T00:22:25+08:00">2020-06-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/" itemprop="url" rel="index"><span itemprop="name">flink</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Flink-入门"><a href="#Flink-入门" class="headerlink" title="Flink 入门"></a>Flink 入门</h1><blockquote>
<p>Apache Flink 是一个框架和分布式处理引擎，用于在<em>无边界和有边界</em>数据流上进行有状态的计算。Flink 能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。</p>
</blockquote>
<p><img src="https://flink.apache.org/img/flink-home-graphic.png" alt="img"></p>
<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><h3 id="处理无界和有界数据"><a href="#处理无界和有界数据" class="headerlink" title="处理无界和有界数据"></a>处理无界和有界数据</h3><p>任何类型的数据都可以形成一种事件流。</p>
<p>数据可以被作为 <em>无界</em> 或者 <em>有界</em> 流来处理。</p>
<ul>
<li><strong>无界流</strong> 有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。</li>
<li><strong>有界流</strong> 有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20200603134807.png" alt="img"></p>
<h3 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h3><h4 id="流"><a href="#流" class="headerlink" title="流"></a>流</h4><p>显而易见，（数据）流是流处理的基本要素。然而，流也拥有着多种特征。这些特征决定了流如何以及何时被处理。Flink 是一个能够处理任何类型数据流的强大处理框架。</p>
<ul>
<li><strong>有界</strong> 和 <strong>无界</strong> 的数据流：流可以是无界的；也可以是有界的，例如固定大小的数据集。Flink 在无界的数据流处理上拥有诸多功能强大的特性，同时也针对有界的数据流开发了专用的高效算子。</li>
<li><strong>实时</strong> 和 <strong>历史记录</strong> 的数据流：所有的数据都是以流的方式产生，但用户通常会使用两种截然不同的方法处理数据。或是在数据生成时进行实时的处理；亦或是先将数据流持久化到存储系统中——例如文件系统或对象存储，然后再进行批处理。Flink 的应用能够同时支持处理实时以及历史记录数据流。</li>
</ul>
<h4 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h4><p>只有在每一个单独的事件上进行转换操作的应用才不需要状态，换言之，每一个具有一定复杂度的流处理应用都是有状态的。任何运行基本业务逻辑的流处理应用都需要在一定时间内存储所接收的事件或中间结果，以供后续的某个时间点（例如收到下一个事件或者经过一段特定时间）进行访问并进行后续处理。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20200603135129.png" alt="img"></p>
<p>应用状态是 Flink 中的一等公民，Flink 提供了许多状态管理相关的特性支持，其中包括：</p>
<ul>
<li><strong>多种状态基础类型</strong>：Flink 为多种不同的数据结构提供了相对应的状态基础类型，例如原子值（value），列表（list）以及映射（map）。开发者可以基于处理函数对状态的访问方式，选择最高效、最适合的状态基础类型。</li>
<li><strong>插件化的 State Backend</strong>：State Backend 负责管理应用程序状态，并在需要的时候进行 checkpoint。Flink 支持多种 state backend，可以将状态存在内存或者 <a target="_blank" rel="noopener" href="https://rocksdb.org/">RocksDB</a>。RocksDB 是一种高效的嵌入式、持久化键值存储引擎。Flink 也支持插件式的自定义 state backend 进行状态存储。</li>
<li><strong>精确一次语义</strong>：Flink 的 checkpoint 和故障恢复算法保证了故障发生后应用状态的一致性。因此，Flink 能够在应用程序发生故障时，对应用程序透明，不造成正确性的影响。</li>
<li><strong>超大数据量状态</strong>：Flink 能够利用其异步以及增量式的 checkpoint 算法，存储数 TB 级别的应用状态。</li>
<li><strong>可弹性伸缩的应用</strong>：Flink 能够通过在更多或更少的工作节点上对状态进行重新分布，支持有状态应用的分布式的横向伸缩。</li>
</ul>
<h4 id="时间"><a href="#时间" class="headerlink" title="时间"></a>时间</h4><p>时间是流处理应用另一个重要的组成部分。因为事件总是在特定时间点发生，所以大多数的事件流都拥有事件本身所固有的时间语义。进一步而言，许多常见的流计算都基于时间语义，例如窗口聚合、会话计算、模式检测和基于时间的 join。流处理的一个重要方面是应用程序如何衡量时间，即区分事件时间（event-time）和处理时间（processing-time）。</p>
<p>Flink 提供了丰富的时间语义支持。</p>
<ul>
<li><strong>事件时间模式</strong>：使用事件时间语义的流处理应用根据事件本身自带的时间戳进行结果的计算。因此，无论处理的是历史记录的事件还是实时的事件，事件时间模式的处理总能保证结果的准确性和一致性。</li>
<li><strong>Watermark 支持</strong>：Flink 引入了 watermark 的概念，用以衡量事件时间进展。Watermark 也是一种平衡处理延时和完整性的灵活机制。</li>
<li><strong>迟到数据处理</strong>：当以带有 watermark 的事件时间模式处理数据流时，在计算完成之后仍会有相关数据到达。这样的事件被称为迟到事件。Flink 提供了多种处理迟到数据的选项，例如将这些数据重定向到旁路输出（side output）或者更新之前完成计算的结果。</li>
<li><strong>处理时间模式</strong>：除了事件时间模式，Flink 还支持处理时间语义。处理时间模式根据处理引擎的机器时钟触发计算，一般适用于有着严格的低延迟需求，并且能够容忍近似结果的流处理应用。</li>
</ul>
<h3 id="Flink-特性"><a href="#Flink-特性" class="headerlink" title="Flink 特性"></a>Flink 特性</h3><ul>
<li>Flink 是<strong>基于事件驱动</strong> (Event-driven) 的应用，能够<strong>同时支持流处理和批处理</strong>；</li>
<li><strong>基于内存计算</strong>，能够保证高吞吐和低延迟，具有优越的性能表现；</li>
<li>支持精确一次 (Exactly-once) 语意，能够完美地保证一致性和正确性；</li>
<li>分层 API ，能够满足各个层次的开发需求；</li>
<li>支持高可用配置，支持保存点机制，能够提供安全性和稳定性上的保证；</li>
<li>多样化的部署方式，支持本地，远端，云端等多种部署方案；</li>
<li>具有横向扩展架构，能够按照用户的需求进行动态扩容；</li>
<li>活跃度极高的社区和完善的生态圈的支持。</li>
</ul>
<h2 id="Flink-架构"><a href="#Flink-架构" class="headerlink" title="Flink 架构"></a>Flink 架构</h2><h3 id="分层架构"><a href="#分层架构" class="headerlink" title="分层架构"></a>分层架构</h3><p>Flink 采用分层的架构设计，从而保证各层在功能和职责上的清晰。</p>
<p>自上而下，分别是 API &amp; Libraries 层、Runtime 核心层以及物理部署层：</p>
<h4 id="API-Libraries-层"><a href="#API-Libraries-层" class="headerlink" title="API &amp; Libraries 层"></a>API &amp; Libraries 层</h4><p>这一层主要提供了编程 API 和 顶层类库：</p>
<ul>
<li><strong>编程 API</strong> : 用于进行流处理的 DataStream API 和用于进行批处理的 DataSet API；<ul>
<li><strong>SQL &amp; Table API</strong> - SQL &amp; Table API 同时适用于批处理和流处理，这意味着你可以对有界数据流和无界数据流以相同的语义进行查询，并产生相同的结果。除了基本查询外， 它还支持自定义的标量函数，聚合函数以及表值函数，可以满足多样化的查询需求。</li>
<li><strong>DataStream &amp; DataSet API</strong> - DataStream &amp; DataSet API 是 Flink 数据处理的核心 API，支持使用 Java 语言或 Scala 语言进行调用，提供了数据读取，数据转换和数据输出等一系列常用操作的封装。</li>
<li><strong>Stateful Stream Processing</strong> - Stateful Stream Processing 是最低级别的抽象，它通过 Process Function 函数内嵌到 DataStream API 中。 Process Function 是 Flink 提供的最底层 API，具有最大的灵活性，允许开发者对于时间和状态进行细粒度的控制。</li>
</ul>
</li>
<li><strong>顶层类库</strong><ul>
<li>用于复杂事件处理的 CEP 库；</li>
<li>用于结构化数据查询的 SQL &amp; Table 库；</li>
<li>基于批处理的机器学习库 FlinkML</li>
<li>图形处理库 Gelly。</li>
</ul>
</li>
</ul>
<h4 id="Runtime-核心层"><a href="#Runtime-核心层" class="headerlink" title="Runtime 核心层"></a>Runtime 核心层</h4><p>这一层是 Flink 分布式计算框架的核心实现层，包括作业转换，任务调度，资源分配，任务执行等功能，基于这一层的实现，可以在流式引擎下同时运行流处理程序和批处理程序。</p>
<h4 id="物理部署层"><a href="#物理部署层" class="headerlink" title="物理部署层"></a>物理部署层</h4><p>Flink 的物理部署层，用于支持在不同平台上部署运行 Flink 应用。</p>
<h3 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h3><h4 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h4><p>按照上面的介绍，Flink 核心架构的第二层是 Runtime 层， 该层采用标准的 Master - Slave 结构， 其中，Master 部分又包含了三个核心组件：Dispatcher、ResourceManager 和 JobManager，而 Slave 则主要是 TaskManager 进程。它们的功能分别如下：</p>
<ul>
<li><strong>JobManagers</strong> (也称为 <em>masters</em>) ：JobManagers <strong>接收由 Dispatcher 传递过来的执行程序，该执行程序包含了作业图 (JobGraph)，逻辑数据流图 (logical dataflow graph) 及其所有的 classes 文件以及第三方类库 (libraries) 等等</strong> 。紧接着 JobManagers 会<strong>将 JobGraph 转换为执行图 (ExecutionGraph)，然后向 ResourceManager 申请资源来执行该任务，一旦申请到资源，就将执行图分发给对应的 TaskManagers</strong> 。因此每个作业 (Job) 至少有一个 JobManager；高可用部署下可以有多个 JobManagers，其中一个作为 _leader_，其余的则处于 <em>standby</em> 状态。</li>
<li><strong>TaskManagers</strong> (也称为 <em>workers</em>) : TaskManagers **负责实际的子任务 (subtasks) 的执行，每个 TaskManagers 都拥有一定数量的 slots。Slot 是一组固定大小的资源的合集 (如计算能力，存储空间)**。TaskManagers 启动后，会将其所拥有的 slots 注册到 ResourceManager 上，由 ResourceManager 进行统一管理。</li>
<li><strong>Dispatcher</strong>：负责接收客户端提交的执行程序，并传递给 JobManager 。除此之外，它还提供了一个 WEB UI 界面，用于监控作业的执行情况。</li>
<li><strong>ResourceManager</strong> ：负责管理 slots 并协调集群资源。ResourceManager 接收来自 JobManager 的资源请求，并将存在空闲 slots 的 TaskManagers 分配给 JobManager 执行任务。Flink 基于不同的部署平台，如 YARN , Mesos，K8s 等提供了不同的资源管理器，当 TaskManagers 没有足够的 slots 来执行任务时，它会向第三方平台发起会话来请求额外的资源。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ed20fbb9587516111f11818092687cab984c48a0/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d6170706c69636174696f6e2d7375626d697373696f6e2e706e67"><img src="https://camo.githubusercontent.com/ed20fbb9587516111f11818092687cab984c48a0/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d6170706c69636174696f6e2d7375626d697373696f6e2e706e67" alt="img"></a></p>
<h4 id="Task-SubTask"><a href="#Task-SubTask" class="headerlink" title="Task &amp; SubTask"></a>Task &amp; SubTask</h4><p>上面我们提到：TaskManagers 实际执行的是 SubTask，而不是 Task，这里解释一下两者的区别：</p>
<p>在执行分布式计算时，Flink 将可以链接的操作 (operators) 链接到一起，这就是 Task。之所以这样做， 是为了减少线程间切换和缓冲而导致的开销，在降低延迟的同时可以提高整体的吞吐量。 但不是所有的 operator 都可以被链接，如下 keyBy 等操作会导致网络 shuffle 和重分区，因此其就不能被链接，只能被单独作为一个 Task。 简单来说，一个 Task 就是一个可以链接的最小的操作链 (Operator Chains) 。如下图，source 和 map 算子被链接到一块，因此整个作业就只有三个 Task：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/1c4380f4018996ae3df4a4f9fd2e7d910a9e4b23/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d7461736b2d7375627461736b2e706e67"><img src="https://camo.githubusercontent.com/1c4380f4018996ae3df4a4f9fd2e7d910a9e4b23/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d7461736b2d7375627461736b2e706e67" alt="img"></a></p>
<p>解释完 Task ，我们在解释一下什么是 SubTask，其准确的翻译是： _A subtask is one parallel slice of a task_，即一个 Task 可以按照其并行度拆分为多个 SubTask。如上图，source &amp; map 具有两个并行度，KeyBy 具有两个并行度，Sink 具有一个并行度，因此整个虽然只有 3 个 Task，但是却有 5 个 SubTask。Jobmanager 负责定义和拆分这些 SubTask，并将其交给 Taskmanagers 来执行，每个 SubTask 都是一个单独的线程。</p>
<h4 id="4-3-资源管理"><a href="#4-3-资源管理" class="headerlink" title="4.3 资源管理"></a>4.3 资源管理</h4><p>理解了 SubTasks ，我们再来看看其与 Slots 的对应情况。一种可能的分配情况如下：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/65dd3fb991b3cc14fe54f6c0c9248f274daf2565/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d7461736b732d736c6f74732e706e67"><img src="https://camo.githubusercontent.com/65dd3fb991b3cc14fe54f6c0c9248f274daf2565/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d7461736b732d736c6f74732e706e67" alt="img"></a></p>
<p>这时每个 SubTask 线程运行在一个独立的 TaskSlot， 它们共享所属的 TaskManager 进程的 TCP 连接（通过多路复用技术）和心跳信息 (heartbeat messages)，从而可以降低整体的性能开销。此时看似是最好的情况，但是每个操作需要的资源都是不尽相同的，这里假设该作业 keyBy 操作所需资源的数量比 Sink 多很多 ，那么此时 Sink 所在 Slot 的资源就没有得到有效的利用。</p>
<p>基于这个原因，Flink 允许多个 subtasks 共享 slots，即使它们是不同 tasks 的 subtasks，但只要它们来自同一个 Job 就可以。假设上面 souce &amp; map 和 keyBy 的并行度调整为 6，而 Slot 的数量不变，此时情况如下：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/d642d9225d82a2bae8a7369e7c45ca1ac80fc490/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d7375627461736b2d736c6f74732e706e67"><img src="https://camo.githubusercontent.com/d642d9225d82a2bae8a7369e7c45ca1ac80fc490/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d7375627461736b2d736c6f74732e706e67" alt="img"></a></p>
<p>可以看到一个 Task Slot 中运行了多个 SubTask 子任务，此时每个子任务仍然在一个独立的线程中执行，只不过共享一组 Sot 资源而已。那么 Flink 到底如何确定一个 Job 至少需要多少个 Slot 呢？Flink 对于这个问题的处理很简单，默认情况一个 Job 所需要的 Slot 的数量就等于其 Operation 操作的最高并行度。如下， A，B，D 操作的并行度为 4，而 C，E 操作的并行度为 2，那么此时整个 Job 就需要至少四个 Slots 来完成。通过这个机制，Flink 就可以不必去关心一个 Job 到底会被拆分为多少个 Tasks 和 SubTasks。</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/f9378f52a789629176ba58d333356071c3f9d9d8/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d7461736b2d706172616c6c656c69736d2e706e67"><img src="https://camo.githubusercontent.com/f9378f52a789629176ba58d333356071c3f9d9d8/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d7461736b2d706172616c6c656c69736d2e706e67" alt="img"></a></p>
<h4 id="4-4-组件通讯"><a href="#4-4-组件通讯" class="headerlink" title="4.4 组件通讯"></a>4.4 组件通讯</h4><p>Flink 的所有组件都基于 Actor System 来进行通讯。Actor system 是多种角色的 actor 的容器，它提供调度，配置，日志记录等多种服务，并包含一个可以启动所有 actor 的线程池，如果 actor 是本地的，则消息通过共享内存进行共享，但如果 actor 是远程的，则通过 RPC 的调用来传递消息。</p>
<h2 id="Flink-窗口"><a href="#Flink-窗口" class="headerlink" title="Flink 窗口"></a>Flink 窗口</h2><p>按照统计维度的不同，Flink 中的窗口可以分为 时间窗口 (Time Windows) 和 计数窗口 (Count Windows) 。</p>
<h3 id="时间窗口"><a href="#时间窗口" class="headerlink" title="时间窗口"></a>时间窗口</h3><p>时间窗口 (Time Windows) 用于以时间为维度来进行数据聚合，具体分为以下四类：</p>
<h4 id="Tumbling-Windows"><a href="#Tumbling-Windows" class="headerlink" title="Tumbling Windows"></a>Tumbling Windows</h4><p>滚动窗口 (Tumbling Windows) 是指彼此之间没有重叠的窗口。例如：每隔 1 小时统计过去 1 小时内的商品点击量，那么 1 天就只能分为 24 个窗口，每个窗口彼此之间是不存在重叠的，具体如下：</p>
<p><img src="https://camo.githubusercontent.com/00d13cef349b16d9c6bd251bd2dbf07a2aa386a8/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d74756d626c696e672d77696e646f77732e706e67" alt="img"></p>
<h4 id="Sliding-Windows"><a href="#Sliding-Windows" class="headerlink" title="Sliding Windows"></a>Sliding Windows</h4><p>滑动窗口（Sliding Windows）用于滚动进行聚合分析，例如：每隔 6 分钟统计一次过去一小时内所有商品的点击量，那么统计窗口彼此之间就是存在重叠的，即 1 天可以分为 240 个窗口。图示如下：</p>
<p><img src="https://camo.githubusercontent.com/998158d5517bf3f4e8af218adc233d76826d4678/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d736c6964696e672d77696e646f77732e706e67" alt="img"></p>
<p>可以看到 window 1 - 4 这四个窗口彼此之间都存在着时间相等的重叠部分。想要实现滑动窗口，只需要在使用 timeWindow 方法时额外传递第二个参数作为滚动时间即可，具体如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 每隔3秒统计一次过去1分钟内的数据</span></span><br><span class="line"><span class="function"><span class="title">timeWindow</span><span class="params">(Time.minutes(<span class="number">1</span>)</span></span>,Time<span class="selector-class">.seconds</span>(<span class="number">3</span>))</span><br></pre></td></tr></table></figure>

<h4 id="Session-Windows"><a href="#Session-Windows" class="headerlink" title="Session Windows"></a>Session Windows</h4><p>当用户在进行持续浏览时，可能每时每刻都会有点击数据，例如在活动区间内，用户可能频繁的将某类商品加入和移除购物车，而你只想知道用户本次浏览最终的购物车情况，此时就可以在用户持有的会话结束后再进行统计。想要实现这类统计，可以通过会话窗口（Session Windows） 来进行实现。</p>
<p><img src="https://camo.githubusercontent.com/e379f3c0c9a3c48114b94a05e934a97e2f5876d5/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d73657373696f6e2d77696e646f77732e706e67" alt="img"></p>
<p>具体的实现代码如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 以处理时间为衡量标准，如果10秒内没有任何数据输入，就认为会话已经关闭，此时触发统计</span></span><br><span class="line"><span class="function"><span class="title">window</span><span class="params">(ProcessingTimeSessionWindows.withGap(Time.seconds(<span class="number">10</span>)</span></span>))</span><br><span class="line"><span class="comment">// 以事件时间为衡量标准</span></span><br><span class="line"><span class="function"><span class="title">window</span><span class="params">(EventTimeSessionWindows.withGap(Time.seconds(<span class="number">10</span>)</span></span>))</span><br></pre></td></tr></table></figure>

<h4 id="Global-Windows"><a href="#Global-Windows" class="headerlink" title="Global Windows"></a>Global Windows</h4><p>最后一个窗口是全局窗口（Global Windows）， 全局窗口会将所有 key 相同的元素分配到同一个窗口中，其通常配合触发器 (trigger) 进行使用。如果没有相应触发器，则计算将不会被执行。</p>
<p><img src="https://camo.githubusercontent.com/36f3b90755c9a95d9bd9fd998d40dc3d752f25e0/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d6e6f6e2d77696e646f7765642e706e67" alt="img"></p>
<p>这里继续以上面词频统计的案例为例，示例代码如下：</p>
<figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 当单词累计出现的次数每达到10次时，则触发计算，计算整个窗口内该单词出现的总数</span></span><br><span class="line"><span class="built_in">window</span>(GlobalWindows.create())<span class="selector-class">.trigger</span>(CountTrigger.of(<span class="number">10</span>))<span class="selector-class">.sum</span>(<span class="number">1</span>)<span class="selector-class">.print</span>();</span><br></pre></td></tr></table></figure>

<h3 id="统计窗口"><a href="#统计窗口" class="headerlink" title="统计窗口"></a>统计窗口</h3><p>统计窗口（Count Windows）用于以数量为维度来进行数据聚合，同样也分为滚动窗口和滑动窗口，实现方式也和时间窗口完全一致，只是调用的 API 不同，具体如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 滚动计数窗口，每1000次点击则计算一次</span></span><br><span class="line"><span class="function"><span class="title">countWindow</span><span class="params">(<span class="number">1000</span>)</span></span></span><br><span class="line"><span class="comment">// 滑动计数窗口，每10次点击发生后，则计算过去1000次点击的情况</span></span><br><span class="line"><span class="function"><span class="title">countWindow</span><span class="params">(<span class="number">1000</span>,<span class="number">10</span>)</span></span></span><br></pre></td></tr></table></figure>

<p>实际上计数窗口内部就是调用的我们上一部分介绍的全局窗口来实现的，其源码如下：</p>
<figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title">countWindow</span><span class="params">(<span class="type">long</span> size)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">window</span>(GlobalWindows.<span class="built_in">create</span>()).<span class="built_in">trigger</span>(PurgingTrigger.<span class="built_in">of</span>(CountTrigger.<span class="built_in">of</span>(size)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title">countWindow</span><span class="params">(<span class="type">long</span> size, <span class="type">long</span> slide)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">window</span>(GlobalWindows.<span class="built_in">create</span>())</span><br><span class="line">        .<span class="built_in">evictor</span>(CountEvictor.<span class="built_in">of</span>(size))</span><br><span class="line">        .<span class="built_in">trigger</span>(CountTrigger.<span class="built_in">of</span>(slide));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Flink-状态"><a href="#Flink-状态" class="headerlink" title="Flink 状态"></a>Flink 状态</h2><p>Flink 一个比较重要的特性就是其支持有状态计算。即你可以将中间的计算结果进行保存，并提供给后续的计算使用：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/2a22dceb9623a0e9e07515478552423445cdacef/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d737461746566756c2d73747265616d2e706e67"><br><img src="https://camo.githubusercontent.com/2a22dceb9623a0e9e07515478552423445cdacef/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d737461746566756c2d73747265616d2e706e67" alt="img"></a></p>
<p>具体而言，Flink 又将状态 (State) 分为 Keyed State 与 Operator State：</p>
<h3 id="2-1-算子状态"><a href="#2-1-算子状态" class="headerlink" title="2.1 算子状态"></a>2.1 算子状态</h3><p>算子状态 (Operator State)：顾名思义，状态是和算子进行绑定的，一个算子的状态不能被其他算子所访问到。官方文档上对 Operator State 的解释是：_each operator state is bound to one parallel operator instance_，所以更为确切的说一个算子状态是与一个并发的算子实例所绑定的，即假设算子的并行度是 2，那么其应有两个对应的算子状态：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/f1fe8ecd3503df42f7e16612fad6b0561627e4b0/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d6f70657261746f722d73746174652e706e67"><img src="https://camo.githubusercontent.com/f1fe8ecd3503df42f7e16612fad6b0561627e4b0/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d6f70657261746f722d73746174652e706e67" alt="img"></a></p>
<h3 id="2-2-键控状态"><a href="#2-2-键控状态" class="headerlink" title="2.2 键控状态"></a>2.2 键控状态</h3><p>键控状态 (Keyed State) ：是一种特殊的算子状态，即状态是根据 key 值进行区分的，Flink 会为每类键值维护一个状态实例。如下图所示，每个颜色代表不同 key 值，对应四个不同的状态实例。需要注意的是键控状态只能在 <code>KeyedStream</code> 上进行使用，我们可以通过 <code>stream.keyBy(...)</code> 来得到 <code>KeyedStream</code> 。</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/39b04c6e1fb1f2d8a88058e14c4bf04ea0967a3b/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d6b657965642d73746174652e706e67"><img src="https://camo.githubusercontent.com/39b04c6e1fb1f2d8a88058e14c4bf04ea0967a3b/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f666c696e6b2d6b657965642d73746174652e706e67" alt="img"></a></p>
<h2 id="Flink-应用场景"><a href="#Flink-应用场景" class="headerlink" title="Flink 应用场景"></a>Flink 应用场景</h2><h3 id="事件驱动型应用"><a href="#事件驱动型应用" class="headerlink" title="事件驱动型应用"></a>事件驱动型应用</h3><h4 id="什么是事件驱动型应用？"><a href="#什么是事件驱动型应用？" class="headerlink" title="什么是事件驱动型应用？"></a>什么是事件驱动型应用？</h4><p>事件驱动型应用是一类具有状态的应用，它从一个或多个事件流提取数据，并根据到来的事件触发计算、状态更新或其他外部动作。</p>
<p>事件驱动型应用是在计算存储分离的传统应用基础上进化而来。在传统架构中，应用需要读写远程事务型数据库。</p>
<p>相反，事件驱动型应用是基于状态化流处理来完成。在该设计中，数据和计算不会分离，应用只需访问本地（内存或磁盘）即可获取数据。系统容错性的实现依赖于定期向远程持久化存储写入 checkpoint。下图描述了传统应用和事件驱动型应用架构的区别。</p>
<p><img src="https://flink.apache.org/img/usecases-eventdrivenapps.png" alt="img"></p>
<h4 id="事件驱动型应用的优势？"><a href="#事件驱动型应用的优势？" class="headerlink" title="事件驱动型应用的优势？"></a>事件驱动型应用的优势？</h4><p>事件驱动型应用无须查询远程数据库，本地数据访问使得它具有更高的吞吐和更低的延迟。而由于定期向远程持久化存储的 checkpoint 工作可以异步、增量式完成，因此对于正常事件处理的影响甚微。事件驱动型应用的优势不仅限于本地数据访问。传统分层架构下，通常多个应用会共享同一个数据库，因而任何对数据库自身的更改（例如：由应用更新或服务扩容导致数据布局发生改变）都需要谨慎协调。反观事件驱动型应用，由于只需考虑自身数据，因此在更改数据表示或服务扩容时所需的协调工作将大大减少。</p>
<h4 id="Flink-如何支持事件驱动型应用？"><a href="#Flink-如何支持事件驱动型应用？" class="headerlink" title="Flink 如何支持事件驱动型应用？"></a>Flink 如何支持事件驱动型应用？</h4><p>事件驱动型应用会受制于底层流处理系统对时间和状态的把控能力，Flink 诸多优秀特质都是围绕这些方面来设计的。它提供了一系列丰富的状态操作原语，允许以精确一次的一致性语义合并海量规模（TB 级别）的状态数据。此外，Flink 还支持事件时间和自由度极高的定制化窗口逻辑，而且它内置的 <code>ProcessFunction</code> 支持细粒度时间控制，方便实现一些高级业务逻辑。同时，Flink 还拥有一个复杂事件处理（CEP）类库，可以用来检测数据流中的模式。</p>
<p>Flink 中针对事件驱动应用的明星特性当属 savepoint。Savepoint 是一个一致性的状态映像，它可以用来初始化任意状态兼容的应用。在完成一次 savepoint 后，即可放心对应用升级或扩容，还可以启动多个版本的应用来完成 A&#x2F;B 测试。</p>
<h4 id="典型的事件驱动型应用实例"><a href="#典型的事件驱动型应用实例" class="headerlink" title="典型的事件驱动型应用实例"></a>典型的事件驱动型应用实例</h4><ul>
<li><a target="_blank" rel="noopener" href="https://sf-2017.flink-forward.org/kb_sessions/streaming-models-how-ing-adds-models-at-runtime-to-catch-fraudsters/">反欺诈</a></li>
<li><a target="_blank" rel="noopener" href="https://sf-2017.flink-forward.org/kb_sessions/building-a-real-time-anomaly-detection-system-with-flink-mux/">异常检测</a></li>
<li><a target="_blank" rel="noopener" href="https://sf-2017.flink-forward.org/kb_sessions/dynamically-configured-stream-processing-using-flink-kafka/">基于规则的报警</a></li>
<li><a target="_blank" rel="noopener" href="https://jobs.zalando.com/tech/blog/complex-event-generation-for-business-process-monitoring-using-apache-flink/">业务流程监控</a></li>
<li><a target="_blank" rel="noopener" href="https://berlin-2017.flink-forward.org/kb_sessions/drivetribes-kappa-architecture-with-apache-flink/">（社交网络）Web 应用</a></li>
</ul>
<h3 id="数据分析应用"><a href="#数据分析应用" class="headerlink" title="数据分析应用"></a>数据分析应用</h3><h4 id="什么是数据分析应用？"><a href="#什么是数据分析应用？" class="headerlink" title="什么是数据分析应用？"></a>什么是数据分析应用？</h4><p>数据分析任务需要从原始数据中提取有价值的信息和指标。传统的分析方式通常是利用批查询，或将事件记录下来并基于此有限数据集构建应用来完成。为了得到最新数据的分析结果，必须先将它们加入分析数据集并重新执行查询或运行应用，随后将结果写入存储系统或生成报告。</p>
<p>借助一些先进的流处理引擎，还可以实时地进行数据分析。和传统模式下读取有限数据集不同，流式查询或应用会接入实时事件流，并随着事件消费持续产生和更新结果。这些结果数据可能会写入外部数据库系统或以内部状态的形式维护。仪表展示应用可以相应地从外部数据库读取数据或直接查询应用的内部状态。</p>
<p>如下图所示，Apache Flink 同时支持流式及批量分析应用。</p>
<p><img src="https://flink.apache.org/img/usecases-analytics.png" alt="img"></p>
<h4 id="流式分析应用的优势？"><a href="#流式分析应用的优势？" class="headerlink" title="流式分析应用的优势？"></a>流式分析应用的优势？</h4><p>和批量分析相比，由于流式分析省掉了周期性的数据导入和查询过程，因此从事件中获取指标的延迟更低。不仅如此，批量查询必须处理那些由定期导入和输入有界性导致的人工数据边界，而流式查询则无须考虑该问题。</p>
<p>另一方面，流式分析会简化应用抽象。批量查询的流水线通常由多个独立部件组成，需要周期性地调度提取数据和执行查询。如此复杂的流水线操作起来并不容易，一旦某个组件出错将会影响流水线的后续步骤。而流式分析应用整体运行在 Flink 之类的高端流处理系统之上，涵盖了从数据接入到连续结果计算的所有步骤，因此可以依赖底层引擎提供的故障恢复机制。</p>
<h4 id="Flink-如何支持数据分析类应用？"><a href="#Flink-如何支持数据分析类应用？" class="headerlink" title="Flink 如何支持数据分析类应用？"></a>Flink 如何支持数据分析类应用？</h4><p>Flink 为持续流式分析和批量分析都提供了良好的支持。具体而言，它内置了一个符合 ANSI 标准的 SQL 接口，将批、流查询的语义统一起来。无论是在记录事件的静态数据集上还是实时事件流上，相同 SQL 查询都会得到一致的结果。同时 Flink 还支持丰富的用户自定义函数，允许在 SQL 中执行定制化代码。如果还需进一步定制逻辑，可以利用 Flink DataStream API 和 DataSet API 进行更低层次的控制。此外，Flink 的 Gelly 库为基于批量数据集的大规模高性能图分析提供了算法和构建模块支持。</p>
<h4 id="典型的数据分析应用实例"><a href="#典型的数据分析应用实例" class="headerlink" title="典型的数据分析应用实例"></a>典型的数据分析应用实例</h4><ul>
<li><a target="_blank" rel="noopener" href="http://2016.flink-forward.org/kb_sessions/a-brief-history-of-time-with-apache-flink-real-time-monitoring-and-analysis-with-flink-kafka-hb/">电信网络质量监控</a></li>
<li>移动应用中的<a target="_blank" rel="noopener" href="https://techblog.king.com/rbea-scalable-real-time-analytics-king/">产品更新及实验评估分析</a></li>
<li>消费者技术中的<a target="_blank" rel="noopener" href="https://eng.uber.com/athenax/">实时数据即席分析</a></li>
<li>大规模图分析</li>
</ul>
<h3 id="数据管道应用"><a href="#数据管道应用" class="headerlink" title="数据管道应用"></a>数据管道应用</h3><h4 id="什么是数据管道？"><a href="#什么是数据管道？" class="headerlink" title="什么是数据管道？"></a>什么是数据管道？</h4><p>提取-转换-加载（ETL）是一种在存储系统之间进行数据转换和迁移的常用方法。ETL 作业通常会周期性地触发，将数据从事务型数据库拷贝到分析型数据库或数据仓库。</p>
<p>数据管道和 ETL 作业的用途相似，都可以转换、丰富数据，并将其从某个存储系统移动到另一个。但数据管道是以持续流模式运行，而非周期性触发。因此它支持从一个不断生成数据的源头读取记录，并将它们以低延迟移动到终点。例如：数据管道可以用来监控文件系统目录中的新文件，并将其数据写入事件日志；另一个应用可能会将事件流物化到数据库或增量构建和优化查询索引。</p>
<p>下图描述了周期性 ETL 作业和持续数据管道的差异。</p>
<p><img src="https://flink.apache.org/img/usecases-datapipelines.png" alt="img"></p>
<h4 id="数据管道的优势？"><a href="#数据管道的优势？" class="headerlink" title="数据管道的优势？"></a>数据管道的优势？</h4><p>和周期性 ETL 作业相比，持续数据管道可以明显降低将数据移动到目的端的延迟。此外，由于它能够持续消费和发送数据，因此用途更广，支持用例更多。</p>
<h4 id="Flink-如何支持数据管道应用？"><a href="#Flink-如何支持数据管道应用？" class="headerlink" title="Flink 如何支持数据管道应用？"></a>Flink 如何支持数据管道应用？</h4><p>很多常见的数据转换和增强操作可以利用 Flink 的 SQL 接口（或 Table API）及用户自定义函数解决。如果数据管道有更高级的需求，可以选择更通用的 DataStream API 来实现。Flink 为多种数据存储系统（如：Kafka、Kinesis、Elasticsearch、JDBC 数据库系统等）内置了连接器。同时它还提供了文件系统的连续型数据源及数据汇，可用来监控目录变化和以时间分区的方式写入文件。</p>
<h4 id="典型的数据管道应用实例"><a href="#典型的数据管道应用实例" class="headerlink" title="典型的数据管道应用实例"></a>典型的数据管道应用实例</h4><ul>
<li>电子商务中的<a target="_blank" rel="noopener" href="https://ververica.com/blog/blink-flink-alibaba-search">实时查询索引构建</a></li>
<li>电子商务中的<a target="_blank" rel="noopener" href="https://jobs.zalando.com/tech/blog/apache-showdown-flink-vs.-spark/">持续 ETL</a></li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://flink.apache.org/zh/">Flink 官网</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/18d7f671/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/18d7f671/" class="post-title-link" itemprop="url">MapReduce</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-06-22 00:22:25" itemprop="dateCreated datePublished" datetime="2020-06-22T00:22:25+08:00">2020-06-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/" itemprop="url" rel="index"><span itemprop="name">hadoop</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h1><h2 id="MapReduce-简介"><a href="#MapReduce-简介" class="headerlink" title="MapReduce 简介"></a>MapReduce 简介</h2><p>MapReduce 是 Hadoop 项目中的分布式计算框架。它降低了分布式计算的门槛，可以让用户轻松编写程序，让其以可靠、容错的方式运行在大型集群上并行处理海量数据（TB 级）。</p>
<p>MapReduce 的设计思路是：</p>
<ul>
<li>分而治之，并行计算</li>
<li>移动计算，而非移动数据</li>
</ul>
<p>MapReduce 作业通过将输入的数据集拆分为独立的块，这些块由 <code>map</code> 任务以并行的方式处理。框架对 <code>map</code> 的输出进行排序，然后将其输入到 <code>reduce</code> 任务中。作业的输入和输出都存储在文件系统中。该框架负责调度任务、监控任务并重新执行失败的任务。</p>
<p>通常，计算节点和存储节点是相同的，即 MapReduce 框架和 HDFS 在同一组节点上运行。此配置允许框架在已存在数据的节点上有效地调度任务，从而在整个集群中实现非常高的聚合带宽。</p>
<p>MapReduce 框架由一个主 <code>ResourceManager</code>、每个集群节点一个工作程序 <code>NodeManager</code> 和每个应用程序的 <code>MRAppMaster</code> （YARN 组件） 组成。</p>
<p>MapReduce 框架仅对 <code>&lt;key、value&gt;</code> 对进行作，也就是说，框架将作业的输入视为一组 <code>&lt;key、value&gt;</code> 对，并生成一组 <code>&lt;key、value&gt;</code> 对作为作业的输出，可以想象是不同的类型。<code>键</code>和<code>值</code>类必须可由框架序列化，因此需要实现 <a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Writable.html">Writable</a> 接口。此外，关键类必须实现 <a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/WritableComparable.html">WritableComparable</a> 接口，以便于按框架进行排序。</p>
<p>MapReduce 作业的 Input 和 Output 类型：</p>
<figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="function"><span class="title">input</span>) &lt;k1, v1&gt; -&gt;</span> <span class="function"><span class="title">map</span> -&gt;</span> &lt;<span class="function"><span class="title">k2</span>, v2&gt; -&gt;</span> <span class="function"><span class="title">combine</span> -&gt;</span> &lt;<span class="function"><span class="title">k2</span>, v2&gt; -&gt;</span> <span class="function"><span class="title">reduce</span> -&gt;</span> &lt;k3, v3&gt; (output)</span><br></pre></td></tr></table></figure>

<p>MapReduce 的特点</p>
<ul>
<li>计算跟着数据走</li>
<li>良好的扩展性：计算能力随着节点数增加，近似线性递增</li>
<li>高容错</li>
<li>状态监控</li>
<li>适合海量数据的离线批处理</li>
<li>降低了分布式编程的门槛</li>
</ul>
<h2 id="MapReduce-应用场景"><a href="#MapReduce-应用场景" class="headerlink" title="MapReduce 应用场景"></a>MapReduce 应用场景</h2><p>适用场景：</p>
<ul>
<li>数据统计，如：网站的 PV、UV 统计</li>
<li>搜索引擎构建索引</li>
<li>海量数据查询</li>
</ul>
<p>不适用场景：</p>
<ul>
<li>OLAP - 要求毫秒或秒级返回结果</li>
<li>流计算 - 流计算的输入数据集是动态的，而 MapReduce 是静态的</li>
<li>DAG 计算<ul>
<li>多个作业存在依赖关系，后一个的输入是前一个的输出，构成有向无环图 DAG</li>
<li>每个 MapReduce 作业的输出结果都会落盘，造成大量磁盘 IO，导致性能非常低下</li>
</ul>
</li>
</ul>
<h2 id="MapReduce-工作流"><a href="#MapReduce-工作流" class="headerlink" title="MapReduce 工作流"></a>MapReduce 工作流</h2><p>MapReduce 编程模型：MapReduce 程序被分为 Map（映射）阶段和 Reduce（化简）阶段。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20200601162305.png" alt="img"></p>
<ol>
<li><strong>input</strong> : 读取文本文件；</li>
<li><strong>splitting</strong> : 将文件按照行进行拆分，此时得到的 <code>K1</code> 行数，<code>V1</code> 表示对应行的文本内容；</li>
<li><strong>mapping</strong> : 并行将每一行按照空格进行拆分，拆分得到的 <code>List(K2,V2)</code>，其中 <code>K2</code> 代表每一个单词，由于是做词频统计，所以 <code>V2</code> 的值为 1，代表出现 1 次；</li>
<li><strong>shuffling</strong>：由于 <code>Mapping</code> 操作可能是在不同的机器上并行处理的，所以需要通过 <code>shuffling</code> 将相同 <code>key</code> 值的数据分发到同一个节点上去合并，这样才能统计出最终的结果，此时得到 <code>K2</code> 为每一个单词，<code>List(V2)</code> 为可迭代集合，<code>V2</code> 就是 Mapping 中的 V2；</li>
<li><strong>Reducing</strong> : 这里的案例是统计单词出现的总次数，所以 <code>Reducing</code> 对 <code>List(V2)</code> 进行归约求和操作，最终输出。</li>
</ol>
<p>MapReduce 编程模型中 <code>splitting</code> 和 <code>shuffing</code> 操作都是由框架实现的，需要我们自己编程实现的只有 <code>mapping</code> 和 <code>reducing</code>，这也就是 MapReduce 这个称呼的来源。</p>
<h2 id="MapReduce-组件"><a href="#MapReduce-组件" class="headerlink" title="MapReduce 组件"></a>MapReduce 组件</h2><p>MapReduce 有以下核心组件：</p>
<ul>
<li><strong>Job</strong> - <a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/Job.html">Job</a> 表示 MapReduce 作业配置。<code>Job</code> 通常用于指定 <code>Mapper</code>、combiner（如果有）、<code>Partitioner</code>、<code>Reducer</code>、<code>InputFormat</code>、<code>OutputFormat</code> 实现。</li>
<li><strong>Mapper</strong> - <a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/Mapper.html">Mapper</a> 负责将输入键值对<strong>映射</strong>到一组中间键值对。转换的中间记录不需要与输入记录具有相同的类型。一个给定的输入键值对可能映射到零个或多个输出键值对。</li>
<li><strong>Combiner</strong> - <code>combiner</code> 是 <code>map</code> 运算后的可选操作，它实际上是一个本地化的 <code>reduce</code> 操作。它执行中间输出的本地聚合，这有助于减少从 <code>Mapper</code> 传输到 <code>Reducer</code> 的数据量。</li>
<li><strong>Reducer</strong> - <a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/Reducer.html">Reducer</a> 将共享一个 key 的一组中间值归并为一个小的数值集。Reducer 有 3 个主要子阶段：shuffle，sort 和 reduce。<ul>
<li><strong>shuffle</strong> - Reducer 的输入就是 mapper 的排序输出。在这个阶段，框架通过 HTTP 获取所有 mapper 输出的相关分区。</li>
<li><strong>sort</strong> - 在这个阶段中，框架将按照 key （因为不同 mapper 的输出中可能会有相同的 key) 对 Reducer 的输入进行分组。shuffle 和 sort 两个阶段是同时发生的。</li>
<li><strong>reduce</strong> - 对按键分组的数据进行聚合统计。</li>
</ul>
</li>
<li><strong>Partitioner</strong> - <a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/Partitioner.html">Partitioner</a> 负责控制 map 中间输出结果的键的分区。<ul>
<li>键（或者键的子集）用于产生分区，通常通过一个散列函数。</li>
<li>分区总数与作业的 reduce 任务数是一样的。因此，它控制中间输出结果（也就是这条记录）的键发送给 m 个 reduce 任务中的哪一个来进行 reduce 操作。</li>
</ul>
</li>
<li><strong>InputFormat</strong> - <a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/InputFormat.html">InputFormat</a> 描述 MapReduce 作业的输入规范。MapReduce 框架依赖作业的 InputFormat 来完成以下工作：<ul>
<li>确认作业的输入规范。</li>
<li>把输入文件分割成多个逻辑的 InputSplit 实例，然后将每个实例分配给一个单独的 Mapper。<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/InputSplit.html">InputSplit</a> 表示要由单个 <code>Mapper</code> 处理的数据。</li>
<li>提供 RecordReader 的实现。<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/RecordReader.html">RecordReader</a> 从 <code>InputSplit</code> 中读取 <code>&lt;key， value&gt;</code> 对，并提供给 <code>Mapper</code> 实现进行处理。</li>
</ul>
</li>
<li><strong>OutputFormat</strong> - <a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/OutputFormat.html">OutputFormat</a> 描述 MapReduce 作业的输出规范。MapReduce 框架依赖作业的 OutputFormat 来完成以下工作：<ul>
<li>确认作业的输出规范，例如检查输出路径是否已经存在。</li>
<li>提供 RecordWriter 实现。<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/RecordWriter.html">RecordWriter</a> 将输出 <code>&lt;key， value&gt;</code> 对到文件系统。</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20200601163846.png" alt="img"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/Hadoop-MapReduce.md">分布式计算框架——MapReduce</a></li>
<li><a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">MapReduce 官方文档</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/9c744ef1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/9c744ef1/" class="post-title-link" itemprop="url">大数据学习</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-06-22 00:22:25" itemprop="dateCreated datePublished" datetime="2020-06-22T00:22:25+08:00">2020-06-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%BB%BC%E5%90%88/" itemprop="url" rel="index"><span itemprop="name">综合</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="大数据学习路线"><a href="#大数据学习路线" class="headerlink" title="大数据学习路线"></a>大数据学习路线</h1><h2 id="大数据简介"><a href="#大数据简介" class="headerlink" title="大数据简介"></a>大数据简介</h2><h3 id="移动计算"><a href="#移动计算" class="headerlink" title="移动计算"></a>移动计算</h3><p>传统的软件计算处理模型，都是“输入 -&gt; 计算 -&gt; 输出”模型。</p>
<p>如何解决 PB 级数据进行计算的问题呢？</p>
<p>采用分布式集群的解决方案，用数千台甚至上万台计算机构建一个大数据计算处理集群，利用更多的网络带宽、内存空间、磁盘容量、CPU 核心数去进行计算处理。</p>
<p>大数据计算处理通常针对的是网站的存量数据，网站大数据系统要做的就是将这些统计规律和关联关系计算出来，并由此进一步改善网站的用户体验和运营决策。</p>
<p>将程序分发到数据所在的地方进行计算，也就是所谓的移动计算比移动数据更划算。</p>
<h3 id="大数据存储"><a href="#大数据存储" class="headerlink" title="大数据存储"></a>大数据存储</h3><p>大规模数据存储的核心问题：</p>
<ul>
<li>数据存储容量</li>
<li>数据读写速度</li>
<li>数据可靠性</li>
</ul>
<p>解决方案：水平伸缩</p>
<h2 id="大数据处理流程"><a href="#大数据处理流程" class="headerlink" title="大数据处理流程"></a>大数据处理流程</h2><p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20220217114216.png" alt="img"></p>
<h3 id="1-1-数据采集"><a href="#1-1-数据采集" class="headerlink" title="1.1 数据采集"></a>1.1 数据采集</h3><p>大数据处理的第一步是数据的收集。现在的中大型项目通常采用微服务架构进行分布式部署，所以数据的采集需要在多台服务器上进行，且采集过程不能影响正常业务的开展。基于这种需求，就衍生了多种日志收集工具，如 Flume 、Logstash、Kibana 等，它们都能通过简单的配置完成复杂的数据收集和数据聚合。</p>
<h3 id="1-2-数据存储"><a href="#1-2-数据存储" class="headerlink" title="1.2 数据存储"></a>1.2 数据存储</h3><p>收集到数据后，下一个问题就是：数据该如何进行存储？通常大家最为熟知是 MySQL、Oracle 等传统的关系型数据库，它们的优点是能够快速存储结构化的数据，并支持随机访问。但大数据的数据结构通常是半结构化（如日志数据）、甚至是非结构化的（如视频、音频数据），为了解决海量半结构化和非结构化数据的存储，衍生了 Hadoop HDFS 、KFS、GFS 等分布式文件系统，它们都能够支持结构化、半结构和非结构化数据的存储，并可以通过增加机器进行横向扩展。</p>
<p>分布式文件系统完美地解决了海量数据存储的问题，但是一个优秀的数据存储系统需要同时考虑数据存储和访问两方面的问题，比如你希望能够对数据进行随机访问，这是传统的关系型数据库所擅长的，但却不是分布式文件系统所擅长的，那么有没有一种存储方案能够同时兼具分布式文件系统和关系型数据库的优点，基于这种需求，就产生了 HBase、MongoDB。</p>
<h3 id="1-3-数据分析"><a href="#1-3-数据分析" class="headerlink" title="1.3 数据分析"></a>1.3 数据分析</h3><p>大数据处理最重要的环节就是数据分析，数据分析通常分为两种：批处理和流处理。</p>
<ul>
<li><strong>批处理</strong>：对一段时间内海量的离线数据进行统一的处理，对应的处理框架有 Hadoop MapReduce、Spark、Flink 等；</li>
<li><strong>流处理</strong>：对运动中的数据进行处理，即在接收数据的同时就对其进行处理，对应的处理框架有 Storm、Spark Streaming、Flink Streaming 等。</li>
</ul>
<p>批处理和流处理各有其适用的场景，时间不敏感或者硬件资源有限，可以采用批处理；时间敏感和实时性要求高就可以采用流处理。随着服务器硬件的价格越来越低和大家对及时性的要求越来越高，流处理越来越普遍，如股票价格预测和电商运营数据分析等。</p>
<p>上面的框架都是需要通过编程来进行数据分析，那么如果你不是一个后台工程师，是不是就不能进行数据的分析了？当然不是，大数据是一个非常完善的生态圈，有需求就有解决方案。为了能够让熟悉 SQL 的人员也能够进行数据的分析，查询分析框架应运而生，常用的有 Hive 、Spark SQL 、Flink SQL、 Pig、Phoenix 等。这些框架都能够使用标准的 SQL 或者 类 SQL 语法灵活地进行数据的查询分析。这些 SQL 经过解析优化后转换为对应的作业程序来运行，如 Hive 本质上就是将 SQL 转换为 MapReduce 作业，Spark SQL 将 SQL 转换为一系列的 RDDs 和转换关系（transformations），Phoenix 将 SQL 查询转换为一个或多个 HBase Scan。</p>
<h3 id="1-4-数据应用"><a href="#1-4-数据应用" class="headerlink" title="1.4 数据应用"></a>1.4 数据应用</h3><p>数据分析完成后，接下来就是数据应用的范畴，这取决于你实际的业务需求。比如你可以将数据进行可视化展现，或者将数据用于优化你的推荐算法，这种运用现在很普遍，比如短视频个性化推荐、电商商品推荐、头条新闻推荐等。当然你也可以将数据用于训练你的机器学习模型，这些都属于其他领域的范畴，都有着对应的框架和技术栈进行处理，这里就不一一赘述。</p>
<h3 id="1-5-其他框架"><a href="#1-5-其他框架" class="headerlink" title="1.5 其他框架"></a>1.5 其他框架</h3><p>上面是一个标准的大数据处理流程所用到的技术框架。但是实际的大数据处理流程比上面复杂很多，针对大数据处理中的各种复杂问题分别衍生了各类框架：</p>
<ul>
<li>单机的处理能力都是存在瓶颈的，所以大数据框架都是采用集群模式进行部署，为了更方便的进行集群的部署、监控和管理，衍生了 Ambari、Cloudera Manager 等集群管理工具；</li>
<li>想要保证集群高可用，需要用到 ZooKeeper ，ZooKeeper 是最常用的分布式协调服务，它能够解决大多数集群问题，包括首领选举、失败恢复、元数据存储及其一致性保证。同时针对集群资源管理的需求，又衍生了 Hadoop YARN ;</li>
<li>复杂大数据处理的另外一个显著的问题是，如何调度多个复杂的并且彼此之间存在依赖关系的作业？基于这种需求，产生了 Azkaban 和 Oozie 等工作流调度框架；</li>
<li>大数据流处理中使用的比较多的另外一个框架是 Kafka，它可以用于消峰，避免在秒杀等场景下并发数据对流处理程序造成冲击；</li>
<li>另一个常用的框架是 Sqoop ，主要是解决了数据迁移的问题，它能够通过简单的命令将关系型数据库中的数据导入到 HDFS 、Hive 或 HBase 中，或者从 HDFS 、Hive 导出到关系型数据库上。</li>
</ul>
<h2 id="大数据学习路线-1"><a href="#大数据学习路线-1" class="headerlink" title="大数据学习路线"></a>大数据学习路线</h2><h3 id="框架分类"><a href="#框架分类" class="headerlink" title="框架分类"></a>框架分类</h3><p><strong>日志收集框架</strong>：Flume 、Logstash、Kibana</p>
<p><strong>分布式文件存储系统</strong>：Hadoop HDFS</p>
<p><strong>数据库系统</strong>：Mongodb、HBase</p>
<p><strong>分布式计算框架</strong>：</p>
<ul>
<li>批处理框架：Hadoop MapReduce</li>
<li>流处理框架：Storm</li>
<li>混合处理框架：Spark、Flink</li>
</ul>
<p><strong>查询分析框架</strong>：Hive 、Spark SQL 、Flink SQL、 Pig、Phoenix</p>
<p><strong>集群资源管理器</strong>：Hadoop YARN</p>
<p><strong>分布式协调服务</strong>：Zookeeper</p>
<p><strong>数据迁移工具</strong>：Sqoop</p>
<p><strong>任务调度框架</strong>：Azkaban、Oozie</p>
<p><strong>集群部署和监控</strong>：Ambari、Cloudera Manager</p>
<p>上面列出的都是比较主流的大数据框架，社区都很活跃，学习资源也比较丰富。建议从 Hadoop 开始入门学习，因为它是整个大数据生态圈的基石，其它框架都直接或者间接依赖于 Hadoop 。接着就可以学习计算框架，Spark 和 Flink 都是比较主流的混合处理框架，Spark 出现得较早，所以其应用也比较广泛。 Flink 是当下最火热的新一代的混合处理框架，其凭借众多优异的特性得到了众多公司的青睐。两者可以按照你个人喜好或者实际工作需要进行学习。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20200601160917.png" alt="img"></p>
<h3 id="学习资料"><a href="#学习资料" class="headerlink" title="学习资料"></a>学习资料</h3><p>大数据最权威和最全面的学习资料就是官方文档。热门的大数据框架社区都比较活跃、版本更新迭代也比较快，所以其出版物都明显滞后于其实际版本，基于这个原因采用书本学习不是一个最好的方案。比较庆幸的是，大数据框架的官方文档都写的比较好，内容完善，重点突出，同时都采用了大量配图进行辅助讲解。当然也有一些优秀的书籍历经时间的检验，至今依然很经典，这里列出部分个人阅读过的经典书籍：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://book.douban.com/subject/27115351/">《hadoop 权威指南 (第四版)》</a> 2017 年</li>
<li><a target="_blank" rel="noopener" href="https://item.jd.com/12270295.html">《Kafka 权威指南》</a> 2017 年</li>
<li><a target="_blank" rel="noopener" href="https://item.jd.com/11622772.html">《从 Paxos 到 Zookeeper 分布式一致性原理与实践》</a> 2015 年</li>
<li><a target="_blank" rel="noopener" href="https://book.douban.com/subject/26649141/">《Spark 技术内幕 深入解析 Spark 内核架构设计与实现原理》</a> 2015 年</li>
<li><a target="_blank" rel="noopener" href="https://book.douban.com/subject/27035127/">《Spark.The.Definitive.Guide》</a> 2018 年</li>
<li><a target="_blank" rel="noopener" href="https://book.douban.com/subject/10748460/">《HBase 权威指南》</a> 2012 年</li>
<li><a target="_blank" rel="noopener" href="https://book.douban.com/subject/25791255/">《Hive 编程指南》</a> 2013 年</li>
</ul>
<h3 id="视频学习资料"><a href="#视频学习资料" class="headerlink" title="视频学习资料"></a>视频学习资料</h3><p>上面我推荐的都是书籍学习资料，很少推荐视频学习资料，这里说明一下原因：因为书籍历经时间的考验，能够再版的或者豆瓣等平台评价高的证明都是被大众所认可的，从概率的角度上来说，其必然更加优秀，不容易浪费大家的学习时间和精力，所以我个人更倾向于官方文档或者书本的学习方式，而不是视频。因为视频学习资料，缺少一个公共的评价平台和完善的评价机制，所以其质量良莠不齐。但是视频任然有其不可替代的好处，学习起来更直观、印象也更深刻，所以对于习惯视频学习的小伙伴，这里我各推荐一个免费的和付费的视频学习资源，大家按需选择：</p>
<ul>
<li>免费学习资源：尚硅谷大数据学习路线 —— <a target="_blank" rel="noopener" href="http://www.atguigu.com/bigdata_video.shtml#bigdata">下载链接</a> \ <a target="_blank" rel="noopener" href="https://space.bilibili.com/302417610/">在线观看链接</a></li>
<li>付费学习资源：<a target="_blank" rel="noopener" href="https://www.imooc.com/t/2781843">慕课网 Michael PK 的系列课程</a></li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.md">大数据学习路线</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/c3adffde/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/c3adffde/" class="post-title-link" itemprop="url">Java 虚拟机之类加载</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-06-17 15:06:46" itemprop="dateCreated datePublished" datetime="2020-06-17T15:06:46+08:00">2020-06-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/Java/" itemprop="url" rel="index"><span itemprop="name">Java</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/Java/JavaCore/" itemprop="url" rel="index"><span itemprop="name">JavaCore</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/Java/JavaCore/JVM/" itemprop="url" rel="index"><span itemprop="name">JVM</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>13k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>12 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Java-虚拟机之类加载"><a href="#Java-虚拟机之类加载" class="headerlink" title="Java 虚拟机之类加载"></a>Java 虚拟机之类加载</h1><h2 id="类加载机制"><a href="#类加载机制" class="headerlink" title="类加载机制"></a>类加载机制</h2><blockquote>
<p>类是在运行期间动态加载的。</p>
</blockquote>
<p>类的加载指的是将类的 <code>.class</code> 文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在堆区创建一个<code>java.lang.Class</code>对象，用来封装类在方法区内的数据结构。类的加载的最终产品是位于堆区中的<code>Class</code>对象，<code>Class</code>对象封装了类在方法区内的数据结构，并且向 Java 程序员提供了访问方法区内的数据结构的接口。</p>
<p>类加载器并不需要等到某个类被“首次主动使用”时再加载它，JVM 规范允许类加载器在预料某个类将要被使用时就预先加载它，如果在预先加载的过程中遇到了.class 文件缺失或存在错误，类加载器必须在程序首次主动使用该类时才报告错误（LinkageError 错误）如果这个类一直没有被程序主动使用，那么类加载器就不会报告错误。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202505070638969.png"></p>
<h2 id="类的生命周期"><a href="#类的生命周期" class="headerlink" title="类的生命周期"></a>类的生命周期</h2><p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202505070635024.png"></p>
<p>Java 类的完整生命周期包括以下几个阶段：</p>
<ul>
<li><strong>加载（Loading）</strong></li>
<li><strong>链接（Linking）</strong><ul>
<li><strong>验证（Verification）</strong></li>
<li><strong>准备（Preparation）</strong></li>
<li><strong>解析（Resolution）</strong></li>
</ul>
</li>
<li><strong>初始化（Initialization）</strong></li>
<li><strong>使用（Using）</strong></li>
<li><strong>卸载（Unloading）</strong></li>
</ul>
<p>加载、验证、准备、初始化和卸载这 5 个阶段的顺序是确定的，类的加载过程必须按照这种顺序按部就班地开始。而<strong>解析过程在某些情况下可以在初始化阶段之后再开始，这是为了支持 Java 的动态绑定</strong>。</p>
<p>类加载过程是指加载、验证、准备、解析和初始化这 5 个阶段。</p>
<h3 id="（一）加载"><a href="#（一）加载" class="headerlink" title="（一）加载"></a>（一）加载</h3><p>加载是类加载的一个阶段，注意不要混淆。</p>
<p><strong>加载，是指查找字节流，并且据此创建类的过程</strong>。</p>
<p>加载过程完成以下三件事：</p>
<ul>
<li>通过一个类的全限定名来获取定义此类的二进制字节流。</li>
<li>将这个字节流所代表的静态存储结构转化为方法区的运行时存储结构。</li>
<li>在内存中生成一个代表这个类的 <code>Class</code> 对象，作为方法区这个类的各种数据的访问入口。</li>
</ul>
<p>其中二进制字节流可以从以下方式中获取：</p>
<ul>
<li>从 ZIP 包读取，这很常见，最终成为日后 JAR、EAR、WAR 格式的基础。</li>
<li>从网络中获取，这种场景最典型的应用是 Applet。</li>
<li>运行时计算生成，这种场景使用得最多得就是动态代理技术，在 <code>java.lang.reflect.Proxy</code> 中，就是用了 <code>ProxyGenerator.generateProxyClass</code> 的代理类的二进制字节流。</li>
<li>由其他文件生成，典型场景是 JSP 应用，即由 JSP 文件生成对应的 Class 类。</li>
<li>从数据库读取，这种场景相对少见，例如有些中间件服务器（如 SAP Netweaver）可以选择把程序安装到数据库中来完成程序代码在集群间的分发。<br>…</li>
</ul>
<blockquote>
<p>更详细内容会在 <a href="#3-classloader">3. ClassLoader</a> 介绍。</p>
</blockquote>
<h3 id="（二）验证"><a href="#（二）验证" class="headerlink" title="（二）验证"></a>（二）验证</h3><p>验证是链接阶段的第一步。<strong>验证的目标是确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求</strong>，并且不会危害虚拟机自身的安全。</p>
<p>验证阶段大致会完成 4 个阶段的检验动作：</p>
<ul>
<li><strong>文件格式验证</strong> - 验证字节流是否符合 Class 文件格式的规范，并且能被当前版本的虚拟机处理。</li>
<li><strong>元数据验证</strong> - 对字节码描述的信息进行语义分析，以保证其描述的信息符合 Java 语言规范的要求。</li>
<li><strong>字节码验证</strong> - 通过数据流和控制流分析，确保程序语义是合法、符合逻辑的。</li>
<li><strong>符号引用验证</strong> - 发生在虚拟机将符号引用转换为直接引用的时候，对类自身以外（常量池中的各种符号引用）的信息进行匹配性校验。</li>
</ul>
<p>验证阶段是非常重要的，但不是必须的，它对程序运行期没有影响，如果所引用的类经过反复验证，那么可以考虑采用 <code>-Xverifynone</code> 参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。</p>
<h3 id="（三）准备"><a href="#（三）准备" class="headerlink" title="（三）准备"></a>（三）准备</h3><p><strong>类变量是被 static 修饰的变量，准备阶段为 static 变量在方法区分配内存并初始化为默认值，使用的是方法区的内存。</strong></p>
<p>实例变量不会在这阶段分配内存，它将会在对象实例化时随着对象一起分配在 Java 堆中。（实例化不是类加载的一个过程，类加载发生在所有实例化操作之前，并且类加载只进行一次，实例化可以进行多次）</p>
<p>初始值一般为 0 值，例如下面的类变量 value 被初始化为 0 而不是 123。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span> <span class="variable">value</span> <span class="operator">=</span> <span class="number">123</span>;</span><br></pre></td></tr></table></figure>

<p>如果类变量是常量，那么会按照表达式来进行初始化，而不是赋值为 0。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">value</span> <span class="operator">=</span> <span class="number">123</span>;</span><br></pre></td></tr></table></figure>

<p>准备阶段有以下几点需要注意：</p>
<ul>
<li>这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在 Java 堆中。</li>
<li>这里所设置的初始值通常情况下是数据类型默认的零值（如 <code>0</code>、<code>0L</code>、<code>null</code>、<code>false</code> 等），而不是被在 Java 代码中被显式地赋予的值。</li>
</ul>
<p>假设一个类变量的定义为：<code>public static int value = 3</code>；</p>
<p>那么变量 value 在准备阶段过后的初始值为 0，而不是 3，因为这时候尚未开始执行任何 Java 方法，而把 value 赋值为 3 的<code>public static</code>指令是在程序编译后，存放于类构造器<code>（）</code>方法之中的，所以把 value 赋值为 3 的动作将在初始化阶段才会执行。</p>
<blockquote>
<p>这里还需要注意如下几点：</p>
<ul>
<li>对基本数据类型来说，对于类变量（static）和全局变量，如果不显式地对其赋值而直接使用，则系统会为其赋予默认的零值，而对于局部变量来说，在使用前必须显式地为其赋值，否则编译时不通过。</li>
<li>对于同时被 static 和 final 修饰的常量，必须在声明的时候就为其显式地赋值，否则编译时不通过；而只被 final 修饰的常量则既可以在声明时显式地为其赋值，也可以在类初始化时显式地为其赋值，总之，在使用前必须为其显式地赋值，系统不会为其赋予默认零值。</li>
<li>对于引用数据类型 reference 来说，如数组引用、对象引用等，如果没有对其进行显式地赋值而直接使用，系统都会为其赋予默认的零值，即 null。</li>
<li>如果在数组初始化时没有对数组中的各元素赋值，那么其中的元素将根据对应的数据类型而被赋予默认的零值。</li>
</ul>
</blockquote>
<ul>
<li>如果类字段的字段属性表中存在<code>ConstantValue</code>属性，即同时被 final 和 static 修饰，那么在准备阶段变量 value 就会被初始化为 ConstValue 属性所指定的值。</li>
</ul>
<p>假设上面的类变量 value 被定义为： <code>public static final int value = 3</code>；</p>
<p>编译时 Javac 将会为 value 生成 ConstantValue 属性，在准备阶段虚拟机就会根据<code>ConstantValue</code>的设置将 value 赋值为 3。我们可以理解为 static final 常量在编译期就将其结果放入了调用它的类的常量池中</p>
<h3 id="（四）解析"><a href="#（四）解析" class="headerlink" title="（四）解析"></a>（四）解析</h3><p>在 class 文件被加载至 Java 虚拟机之前，这个类无法知道其他类及其方法、字段所对应的具体地址，甚至不知道自己方法、字段的地址。因此，每当需要引用这些成员时，Java 编译器会生成一个符号引用。在运行阶段，这个符号引用一般都能够无歧义地定位到具体目标上。</p>
<p>举例来说，对于一个方法调用，编译器会生成一个包含目标方法所在类的名字、目标方法的名字、接收参数类型以及返回值类型的符号引用，来指代所要调用的方法。</p>
<p><strong>解析阶段目标是将常量池的符号引用替换为直接引用的过程</strong>。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符 7 类符号引用进行。</p>
<ul>
<li><strong>符号引用（Symbolic References）</strong> - 符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可。</li>
<li><strong>直接引用（Direct Reference）</strong> - 直接引用可以是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。</li>
</ul>
<h3 id="（五）初始化"><a href="#（五）初始化" class="headerlink" title="（五）初始化"></a>（五）初始化</h3><p>在 Java 代码中，如果要初始化一个静态字段，可以在声明时直接赋值，也可以在静态代码块中对其赋值。</p>
<p>如果直接赋值的静态字段被 <code>final</code> 所修饰，并且它的类型是基本类型或字符串时，那么该字段便会被 Java 编译器标记成常量值（ConstantValue），其初始化直接由 Java 虚拟机完成。除此之外的直接赋值操作，以及所有静态代码块中的代码，则会被 Java 编译器置于同一方法中，并把它命名为 <code>&lt; clinit &gt;</code>。</p>
<p>初始化阶段才真正开始执行类中的定义的 Java 程序代码。初始化，<strong>为类的静态变量赋予正确的初始值，JVM 负责对类进行初始化，主要对类变量进行初始化</strong>。</p>
<h4 id="类初始化方式"><a href="#类初始化方式" class="headerlink" title="类初始化方式"></a>类初始化方式</h4><ul>
<li>声明类变量时指定初始值</li>
<li>使用静态代码块为类变量指定初始值</li>
</ul>
<blockquote>
<p>在准备阶段，类变量已经赋过一次系统要求的初始值，而在初始化阶段，根据程序员通过程序制定的主观计划去初始化类变量和其它资源。</p>
</blockquote>
<h4 id="类初始化步骤"><a href="#类初始化步骤" class="headerlink" title="类初始化步骤"></a>类初始化步骤</h4><ol>
<li>如果类还没有被加载和链接，开始加载该类。</li>
<li>如果该类的直接父类还没有被初始化，先初始化其父类。</li>
<li>如果该类有初始化语句，则依次执行这些初始化语句。</li>
</ol>
<h4 id="类初始化时机"><a href="#类初始化时机" class="headerlink" title="类初始化时机"></a>类初始化时机</h4><p>只有主动引用类的时候才会导致类的初始化。</p>
<p><strong>（1）主动引用</strong></p>
<p>类的主动引用包括以下六种：</p>
<ul>
<li><strong>创建类的实例</strong> - 也就是 <code>new</code> 对象</li>
<li><strong>访问静态变量</strong> - 访问某个类或接口的静态变量，或者对该静态变量赋值</li>
<li><strong>访问静态方法</strong></li>
<li><strong>反射</strong> - 如<code>Class.forName(“com.shengsiyuan.Test”)</code></li>
<li><strong>初始化子类</strong> - 初始化某个类的子类，则其父类也会被初始化</li>
<li><strong>启动类</strong> - Java 虚拟机启动时被标明为启动类的类（<code>Java Test</code>），直接使用<code>java.exe</code>命令来运行某个主类</li>
</ul>
<p><strong>（2）被动引用</strong></p>
<p>以上 5 种场景中的行为称为对一个类进行主动引用。除此之外，所有引用类的方式都不会触发初始化，称为被动引用。被动引用的常见例子包括：</p>
<ul>
<li><strong>通过子类引用父类的静态字段，不会导致子类初始化</strong>。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">System.out.println(SubClass.value); <span class="comment">// value 字段在 SuperClass 中定义</span></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>通过数组定义来引用类，不会触发此类的初始化</strong>。该过程会对数组类进行初始化，数组类是一个由虚拟机自动生成的、直接继承自 <code>Object</code> 的子类，其中包含了数组的属性和方法。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SuperClass[] sca = <span class="keyword">new</span> <span class="title class_">SuperClass</span>[<span class="number">10</span>];</span><br></pre></td></tr></table></figure>

<ul>
<li>常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发<strong>定义常量的类的初始化</strong>。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">System.out.println(ConstClass.HELLOWORLD);</span><br></pre></td></tr></table></figure>

<h4 id="类初始化细节"><a href="#类初始化细节" class="headerlink" title="类初始化细节"></a>类初始化细节</h4><p>类初始化 <code>&lt;clinit&gt;()</code> 方法的细节：</p>
<ul>
<li>是由编译器自动收集类中所有类变量的赋值动作和静态语句块（static{} 块）中的语句合并产生的，编译器收集的顺序由语句在源文件中出现的顺序决定。特别注意的是，静态语句块只能访问到定义在它之前的类变量，定义在它之后的类变量只能赋值，不能访问。例如以下代码：</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Test</span> &#123;</span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        i = <span class="number">0</span>;                <span class="comment">// 给变量赋值可以正常编译通过</span></span><br><span class="line">        System.out.print(i);  <span class="comment">// 这句编译器会提示“非法向前引用”</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">static</span> <span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>与类的构造函数（或者说实例构造器 <code>&lt;init&gt;()</code>）不同，不需要显式的调用父类的构造器。虚拟机会自动保证在子类的 <code>&lt;clinit&gt;()</code> 方法运行之前，父类的 <code>&lt;clinit&gt;()</code> 方法已经执行结束。因此虚拟机中第一个执行 <code>&lt;clinit&gt;()</code> 方法的类肯定为 <code>java.lang.Object</code>。</li>
<li>由于父类的 <code>&lt;clinit&gt;()</code> 方法先执行，也就意味着父类中定义的静态语句块要优于子类的变量赋值操作。例如以下代码：</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Parent</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span> <span class="variable">A</span> <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        A = <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Sub</span> <span class="keyword">extends</span> <span class="title class_">Parent</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span> <span class="variable">B</span> <span class="operator">=</span> A;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">     System.out.println(Sub.B);  <span class="comment">// 输出结果是父类中的静态变量 A 的值，也就是 2。</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><code>&lt;clinit&gt;()</code> 方法对于类或接口不是必须的，如果一个类中不包含静态语句块，也没有对类变量的赋值操作，编译器可以不为该类生成 <code>&lt;clinit&gt;()</code> 方法。</li>
<li>接口中不可以使用静态语句块，但仍然有类变量初始化的赋值操作，因此接口与类一样都会生成 <code>&lt;clinit&gt;()</code> 方法。但接口与类不同的是，执行接口的 <code>&lt;clinit&gt;()</code> 方法不需要先执行父接口的 <code>&lt;clinit&gt;()</code> 方法。只有当父接口中定义的变量使用时，父接口才会初始化。另外，接口的实现类在初始化时也一样不会执行接口的 <code>&lt;clinit&gt;()</code> 方法。</li>
<li>虚拟机会保证一个类的 <code>&lt;clinit&gt;()</code> 方法在多线程环境下被正确的加锁和同步，如果多个线程同时初始化一个类，只会有一个线程执行这个类的 <code>&lt;clinit&gt;()</code> 方法，其它线程都会阻塞等待，直到活动线程执行 <code>&lt;clinit&gt;()</code> 方法完毕。如果在一个类的 <code>&lt;clinit&gt;()</code> 方法中有耗时的操作，就可能造成多个线程阻塞，在实际过程中此种阻塞很隐蔽。</li>
</ul>
<h2 id="ClassLoader"><a href="#ClassLoader" class="headerlink" title="ClassLoader"></a>ClassLoader</h2><p><code>ClassLoader</code> 即类加载器，负责将类加载到 JVM。在 Java 虚拟机外部实现，以便让应用程序自己决定如何去获取所需要的类。</p>
<p>JVM 加载 <code>class</code> 文件到内存有两种方式：</p>
<ul>
<li>隐式加载 - JVM 自动加载需要的类到内存中。</li>
<li>显示加载 - 通过使用 <code>ClassLoader</code> 来加载一个类到内存中。</li>
</ul>
<h3 id="类与类加载器"><a href="#类与类加载器" class="headerlink" title="类与类加载器"></a>类与类加载器</h3><p>如何判断两个类是否相等：类本身相等，并且使用同一个类加载器进行加载。这是因为<strong>每一个 <code>ClassLoader</code> 都拥有一个独立的类名称空间</strong>。</p>
<p>这里的相等，包括类的 <code>Class</code> 对象的 <code>equals()</code> 方法、<code>isAssignableFrom()</code> 方法、<code>isInstance()</code> 方法的返回结果为 true，也包括使用 <code>instanceof</code> 关键字做对象所属关系判定结果为 true。</p>
<h3 id="类加载器分类"><a href="#类加载器分类" class="headerlink" title="类加载器分类"></a>类加载器分类</h3><p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20200617115936.png" alt="img"></p>
<h4 id="Bootstrap-ClassLoader"><a href="#Bootstrap-ClassLoader" class="headerlink" title="Bootstrap ClassLoader"></a>Bootstrap ClassLoader</h4><p><code>Bootstrap ClassLoader</code> ，即启动类加载器 ，<strong>负责加载 JVM 自身工作所需要的类</strong>。</p>
<p><strong><code>Bootstrap ClassLoader</code> 会将存放在 <code>&lt;JAVA_HOME&gt;\lib</code> 目录中的，或者被 <code>-Xbootclasspath</code> 参数所指定的路径中的，并且是虚拟机识别的（仅按照文件名识别，如 rt.jar，名字不符合的类库即使放在 lib 目录中也不会被加载）类库加载到虚拟机内存中</strong>。</p>
<p><code>Bootstrap ClassLoader</code> 是由 C++ 实现的，它完全由 JVM 自己控制的，启动类加载器无法被 Java 程序直接引用，用户在编写自定义类加载器时，如果需要把加载请求委派给启动类加载器，直接使用 <code>null</code> 代替即可。</p>
<h4 id="ExtClassLoader"><a href="#ExtClassLoader" class="headerlink" title="ExtClassLoader"></a>ExtClassLoader</h4><p><code>ExtClassLoader</code>，即扩展类加载器，这个类加载器是由 <code>ExtClassLoader(sun.misc.Launcher\$ExtClassLoader)</code>实现的。</p>
<p><strong><code>ExtClassLoader</code> 负责将 <code>&lt;JAVA_HOME&gt;\lib\ext</code> 或者被 <code>java.ext.dir</code> 系统变量所指定路径中的所有类库加载到内存中，开发者可以直接使用扩展类加载器</strong>。</p>
<h4 id="AppClassLoader"><a href="#AppClassLoader" class="headerlink" title="AppClassLoader"></a>AppClassLoader</h4><p><code>AppClassLoader</code>，即应用程序类加载器，这个类加载器是由 <code>AppClassLoader(sun.misc.Launcher\$AppClassLoader)</code> 实现的。由于这个类加载器是 <code>ClassLoader</code> 中的 <code>getSystemClassLoader()</code> 方法的返回值，因此一般称为系统类加载器。</p>
<p><strong><code>AppClassLoader</code> 负责加载用户类路径（即 <code>classpath</code>）上所指定的类库</strong>，开发者可以直接使用这个类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。</p>
<h4 id="自定义类加载器"><a href="#自定义类加载器" class="headerlink" title="自定义类加载器"></a>自定义类加载器</h4><p>自定义类加载器可以做到如下几点：</p>
<ul>
<li>在执行非置信代码之前，自动验证数字签名。</li>
<li>动态地创建符合用户特定需要的定制化构建类。</li>
<li>从特定的场所取得 java class，例如数据库中和网络中。</li>
</ul>
<p>假设，我们需要自定义一个名为 <code>FileSystemClassLoader</code> 的类加载器，继承自 <code>java.lang.ClassLoader</code>，用于加载文件系统上的类。它首先根据类的全名在文件系统上查找类的字节代码文件（<code>.class</code> 文件），然后读取该文件内容，最后通过 <code>defineClass()</code> 方法来把这些字节代码转换成 <code>java.lang.Class</code> 类的实例。</p>
<p><code>java.lang.ClassLoader</code> 类的方法 <code>loadClass()</code> 实现了双亲委派模型的逻辑，因此自定义类加载器一般不去覆写它，而是通过覆写 <code>findClass()</code> 方法。</p>
<p><code>ClassLoader</code> 常用的场景：</p>
<ul>
<li>容器 - 典型应用：Servlet 容器（如：Tomcat、Jetty）、udf （Mysql、Hive）等。加载解压 jar 包或 war 包后，加载其 Class 到指定的类加载器中运行（通常需要考虑空间隔离）。</li>
<li>热部署、热插拔 - 应用启动后，动态获得某个类信息，然后加载到 JVM 中工作。很多著名的容器软件、框架（如：Spring 等），都使用 <code>ClassLoader</code> 来实现自身的热部署。</li>
</ul>
<p>【示例】自定义一个类加载器</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FileSystemClassLoader</span> <span class="keyword">extends</span> <span class="title class_">ClassLoader</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String rootDir;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">FileSystemClassLoader</span><span class="params">(String rootDir)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.rootDir = rootDir;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> Class&lt;?&gt; findClass(String name) <span class="keyword">throws</span> ClassNotFoundException &#123;</span><br><span class="line">        <span class="type">byte</span>[] classData = getClassData(name);</span><br><span class="line">        <span class="keyword">if</span> (classData == <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">ClassNotFoundException</span>();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> defineClass(name, classData, <span class="number">0</span>, classData.length);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">byte</span>[] getClassData(String className) &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">path</span> <span class="operator">=</span> classNameToPath(className);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">InputStream</span> <span class="variable">ins</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(path);</span><br><span class="line">            <span class="type">ByteArrayOutputStream</span> <span class="variable">baos</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ByteArrayOutputStream</span>();</span><br><span class="line">            <span class="type">int</span> <span class="variable">bufferSize</span> <span class="operator">=</span> <span class="number">4096</span>;</span><br><span class="line">            <span class="type">byte</span>[] buffer = <span class="keyword">new</span> <span class="title class_">byte</span>[bufferSize];</span><br><span class="line">            <span class="type">int</span> bytesNumRead;</span><br><span class="line">            <span class="keyword">while</span> ((bytesNumRead = ins.read(buffer)) != -<span class="number">1</span>) &#123;</span><br><span class="line">                baos.write(buffer, <span class="number">0</span>, bytesNumRead);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> baos.toByteArray();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String <span class="title function_">classNameToPath</span><span class="params">(String className)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> rootDir + File.separatorChar</span><br><span class="line">                + className.replace(<span class="string">&#x27;.&#x27;</span>, File.separatorChar) + <span class="string">&quot;.class&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="双亲委派"><a href="#双亲委派" class="headerlink" title="双亲委派"></a>双亲委派</h3><p>理解双亲委派之前，先让我们看一个示例。</p>
<p>【示例】寻找类加载示例</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">    <span class="type">ClassLoader</span> <span class="variable">loader</span> <span class="operator">=</span> Thread.currentThread().getContextClassLoader();</span><br><span class="line">    System.out.println(loader);</span><br><span class="line">    System.out.println(loader.getParent());</span><br><span class="line">    System.out.println(loader.getParent().getParent());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sun.misc.Launcher$AppClassLoader<span class="symbol">@18b4aac2</span></span><br><span class="line">sun.misc.Launcher$ExtClassLoader<span class="symbol">@19e1023e</span></span><br><span class="line"><span class="literal">null</span></span><br></pre></td></tr></table></figure>

<p>从上面的结果可以看出，并没有获取到 <code>ExtClassLoader</code> 的父 Loader，原因是 <code>Bootstrap Loader</code>（引导类加载器）是用 C 语言实现的，找不到一个确定的返回父 Loader 的方式，于是就返回 null。</p>
<p>下图展示的类加载器之间的层次关系，称为类加载器的<strong>双亲委派模型（Parents Delegation Model）</strong>。<strong>该模型要求除了顶层的 Bootstrap ClassLoader 外，其余的类加载器都应有自己的父类加载器</strong>。<strong>这里类加载器之间的父子关系一般通过组合（Composition）关系来实现，而不是通过继承（Inheritance）的关系实现</strong>。</p>
<p><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202505070634474.png"></p>
<p><strong>（1）工作过程</strong></p>
<p><strong>一个类加载器首先将类加载请求传送到父类加载器，只有当父类加载器无法完成类加载请求时才尝试加载</strong>。</p>
<p><strong>（2）好处</strong></p>
<p><strong>使得 Java 类随着它的类加载器一起具有一种带有优先级的层次关系</strong>，从而使得基础类得到统一：</p>
<ul>
<li>系统类防止内存中出现多份同样的字节码</li>
<li>保证 Java 程序安全稳定运行</li>
</ul>
<p>例如： <code>java.lang.Object</code> 存放在 rt.jar 中，如果编写另外一个 <code>java.lang.Object</code> 的类并放到 <code>classpath</code> 中，程序可以编译通过。因为双亲委派模型的存在，所以在 rt.jar 中的 <code>Object</code> 比在 <code>classpath</code> 中的 <code>Object</code> 优先级更高，因为 rt.jar 中的 <code>Object</code> 使用的是启动类加载器，而 <code>classpath</code> 中的 <code>Object</code> 使用的是应用程序类加载器。正因为 rt.jar 中的 <code>Object</code> 优先级更高，因为程序中所有的 <code>Object</code> 都是这个 <code>Object</code>。</p>
<p><strong>（3）实现</strong></p>
<p>以下是抽象类 <code>java.lang.ClassLoader</code> 的代码片段，其中的 <code>loadClass()</code> 方法运行过程如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">class</span> <span class="title class_">ClassLoader</span> &#123;</span><br><span class="line">    <span class="comment">// The parent class loader for delegation</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ClassLoader parent;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Class&lt;?&gt; loadClass(String name) <span class="keyword">throws</span> ClassNotFoundException &#123;</span><br><span class="line">        <span class="keyword">return</span> loadClass(name, <span class="literal">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> Class&lt;?&gt; loadClass(String name, <span class="type">boolean</span> resolve) <span class="keyword">throws</span> ClassNotFoundException &#123;</span><br><span class="line">        <span class="keyword">synchronized</span> (getClassLoadingLock(name)) &#123;</span><br><span class="line">            <span class="comment">// 首先判断该类型是否已经被加载</span></span><br><span class="line">            Class&lt;?&gt; c = findLoadedClass(name);</span><br><span class="line">            <span class="keyword">if</span> (c == <span class="literal">null</span>) &#123;</span><br><span class="line">                <span class="comment">// 如果没有被加载，就委托给父类加载或者委派给启动类加载器加载</span></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (parent != <span class="literal">null</span>) &#123;</span><br><span class="line">                        <span class="comment">// 如果存在父类加载器，就委派给父类加载器加载</span></span><br><span class="line">                        c = parent.loadClass(name, <span class="literal">false</span>);</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">// 如果不存在父类加载器，就检查是否是由启动类加载器加载的类，通过调用本地方法native Class findBootstrapClass(String name)</span></span><br><span class="line">                        c = findBootstrapClassOrNull(name);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">catch</span> (ClassNotFoundException e) &#123;</span><br><span class="line">                    <span class="comment">// 如果父类加载器加载失败，会抛出 ClassNotFoundException</span></span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (c == <span class="literal">null</span>) &#123;</span><br><span class="line">                    <span class="comment">// 如果父类加载器和启动类加载器都不能完成加载任务，才调用自身的加载功能</span></span><br><span class="line">                    c = findClass(name);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (resolve) &#123;</span><br><span class="line">                resolveClass(c);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> c;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> Class&lt;?&gt; findClass(String name) <span class="keyword">throws</span> ClassNotFoundException &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">ClassNotFoundException</span>(name);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>【说明】</p>
<ul>
<li>先检查类是否已经加载过，如果没有则让父类加载器去加载。</li>
<li>当父类加载器加载失败时抛出 <code>ClassNotFoundException</code>，此时尝试自己去加载。</li>
</ul>
<h3 id="ClassLoader-参数"><a href="#ClassLoader-参数" class="headerlink" title="ClassLoader 参数"></a>ClassLoader 参数</h3><p>在生产环境上启动 java 应用时，通常会指定一些 <code>ClassLoader</code> 参数，以加载应用所需要的 lib：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -jar xxx.jar -classpath lib/*</span><br></pre></td></tr></table></figure>

<p><code>ClassLoader</code> 相关参数选项：</p>
<table>
<thead>
<tr>
<th>参数选项</th>
<th>ClassLoader 类型</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>-Xbootclasspath</code></td>
<td><code>Bootstrap ClassLoader</code></td>
<td>设置 <code>Bootstrap ClassLoader</code> 搜索路径。【不常用】</td>
</tr>
<tr>
<td><code>-Xbootclasspath/a</code></td>
<td><code>Bootstrap ClassLoader</code></td>
<td>把路径添加到已存在的 <code>Bootstrap ClassLoader</code> 搜索路径后面。【常用】</td>
</tr>
<tr>
<td><code>-Xbootclasspath/p</code></td>
<td><code>Bootstrap ClassLoader</code></td>
<td>把路径添加到已存在的 <code>Bootstrap ClassLoader</code> 搜索路径前面。【不常用】</td>
</tr>
<tr>
<td><code>-Djava.ext.dirs</code></td>
<td><code>ExtClassLoader</code></td>
<td>设置 <code>ExtClassLoader</code> 搜索路径。</td>
</tr>
<tr>
<td><code>-Djava.class.path</code> 或 <code>-cp</code> 或 <code>-classpath</code></td>
<td><code>AppClassLoader</code></td>
<td>设置 <code>AppClassLoader</code> 搜索路径。</td>
</tr>
</tbody></table>
<h2 id="类的加载"><a href="#类的加载" class="headerlink" title="类的加载"></a>类的加载</h2><h3 id="类加载方式"><a href="#类加载方式" class="headerlink" title="类加载方式"></a>类加载方式</h3><p>类加载有三种方式：</p>
<ul>
<li>命令行启动应用时候由 JVM 初始化加载</li>
<li>通过 <code>Class.forName()</code> 方法动态加载</li>
<li>通过 <code>ClassLoader.loadClass()</code> 方法动态加载</li>
</ul>
<p><strong><code>Class.forName()</code> 和 <code>ClassLoader.loadClass()</code> 区别</strong></p>
<ul>
<li><code>Class.forName()</code> 将类的 <code>.class</code> 文件加载到 jvm 中之外，还会对类进行解释，执行类中的 <code>static</code> 块；</li>
<li><code>ClassLoader.loadClass()</code> 只干一件事情，就是将 <code>.class</code> 文件加载到 jvm 中，不会执行 <code>static</code> 中的内容，只有在 <code>newInstance</code> 才会去执行 <code>static</code> 块。</li>
<li><code>Class.forName(name, initialize, loader)</code> 带参函数也可控制是否加载 <code>static</code> 块。并且只有调用了 <code>newInstance()</code> 方法采用调用构造函数，创建类的对象 。</li>
</ul>
<h3 id="加载类错误"><a href="#加载类错误" class="headerlink" title="加载类错误"></a>加载类错误</h3><h4 id="ClassNotFoundException"><a href="#ClassNotFoundException" class="headerlink" title="ClassNotFoundException"></a>ClassNotFoundException</h4><p><code>ClassNotFoundException</code> 异常出镜率极高。**<code>ClassNotFoundException</code> 表示当前 <code>classpath</code> 下找不到指定类**。</p>
<p>常见问题原因：</p>
<ul>
<li>调用 <code>Class</code> 的 <code>forName()</code> 方法，未找到类。</li>
<li>调用 <code>ClassLoader</code> 中的 <code>loadClass()</code> 方法，未找到类。</li>
<li>调用 <code>ClassLoader</code> 中的 <code>findSystemClass()</code> 方法，未找到类。</li>
</ul>
<p>【示例】执行以下代码，会抛出 <code>ClassNotFoundException</code> 异常：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ClassNotFoundExceptionDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Class.forName(<span class="string">&quot;NotFound&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ClassNotFoundException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>解决方法：检查 <code>classpath</code> 下有没有相应的 class 文件。</p>
<h4 id="NoClassDefFoundError"><a href="#NoClassDefFoundError" class="headerlink" title="NoClassDefFoundError"></a>NoClassDefFoundError</h4><p>常见问题原因：</p>
<ul>
<li>类依赖的 Class 或者 jar 不存在。</li>
<li>类文件存在，但是存在不同的域中。</li>
</ul>
<p>解决方法：现代 Java 项目，一般使用 <code>maven</code>、<code>gradle</code> 等构建工具管理项目，仔细检查找不到的类所在的 jar 包是否已添加为依赖。</p>
<h4 id="UnsatisfiedLinkError"><a href="#UnsatisfiedLinkError" class="headerlink" title="UnsatisfiedLinkError"></a>UnsatisfiedLinkError</h4><p>这个异常倒不是很常见，但是出错的话，通常是在 JVM 启动的时候如果一不小心将在 JVM 中的某个 lib 删除了，可能就会报这个错误了。</p>
<p>【示例】执行以下代码，会抛出 <code>UnsatisfiedLinkError</code> 错误。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">UnsatisfiedLinkErrorDemo</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title function_">nativeMethod</span><span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        System.loadLibrary(<span class="string">&quot;NoLib&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">UnsatisfiedLinkErrorDemo</span>().nativeMethod();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>【输出】</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">java.lang.UnsatisfiedLinkError: no NoLib in java.library.path</span><br><span class="line">	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:<span class="number">1867</span>)</span><br><span class="line">	at java.lang.Runtime.loadLibrary0(Runtime.java:<span class="number">870</span>)</span><br><span class="line">	at java.lang.System.loadLibrary(System.java:<span class="number">1122</span>)</span><br><span class="line">	at io.github.dunwu.javacore.jvm.classloader.exception.UnsatisfiedLinkErrorDemo.&lt;clinit&gt;(UnsatisfiedLinkErrorDemo.java:<span class="number">12</span>)</span><br></pre></td></tr></table></figure>

<h4 id="ClassCastException"><a href="#ClassCastException" class="headerlink" title="ClassCastException"></a>ClassCastException</h4><p><code>ClassCastException</code> 异常通常是在程序中强制类型转换失败时出现。</p>
<p>【示例】执行以下代码，会抛出 <code>ClassCastException</code> 异常。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ClassCastExceptionDemo</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Object</span> <span class="variable">obj</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Object</span>();</span><br><span class="line">        <span class="type">EmptyClass</span> <span class="variable">newObj</span> <span class="operator">=</span> (EmptyClass) obj;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">EmptyClass</span> &#123;&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>【输出】</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">&quot;main&quot;</span> java.lang.ClassCastException: java.lang.Object cannot be cast to io.github.dunwu.javacore.jvm.classloader.exception.ClassCastExceptionDemo$EmptyClass</span><br><span class="line">	at io.github.dunwu.javacore.jvm.classloader.exception.ClassCastExceptionDemo.main(ClassCastExceptionDemo.java:<span class="number">11</span>)</span><br></pre></td></tr></table></figure>

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://book.douban.com/subject/34907497/">《深入理解 Java 虚拟机》</a></li>
<li><a target="_blank" rel="noopener" href="https://time.geekbang.org/column/intro/100010301">极客时间教程 - 深入拆解 Java 虚拟机</a></li>
<li><a target="_blank" rel="noopener" href="https://juejin.im/post/5e479c2cf265da575f4e65e4">一篇图文彻底弄懂类加载器与双亲委派机制</a></li>
<li><a target="_blank" rel="noopener" href="http://www.ityouknow.com/jvm/2017/08/19/class-loading-principle.html">Jvm 系列(一):Java 类的加载机制</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dunwu.github.io/blog/pages/bc341aee/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/avatar.gif">
      <meta itemprop="name" content="钝悟 ◾ Dunwu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dunwu Blog">
      <meta itemprop="description" content="钝悟的个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Dunwu Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/pages/bc341aee/" class="post-title-link" itemprop="url">Elastic 快速入门</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-06-16 07:10:44" itemprop="dateCreated datePublished" datetime="2020-06-16T07:10:44+08:00">2020-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-13 17:56:53" itemprop="dateModified" datetime="2025-09-13T17:56:53+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">数据库</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">搜索引擎数据库</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/elastic/" itemprop="url" rel="index"><span itemprop="name">elastic</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Elastic-快速入门"><a href="#Elastic-快速入门" class="headerlink" title="Elastic 快速入门"></a>Elastic 快速入门</h1><blockquote>
<p>开源协议：<a target="_blank" rel="noopener" href="https://github.com/elastic/elasticsearch/tree/7.4/licenses/APACHE-LICENSE-2.0.txt">Apache 2.0</a></p>
</blockquote>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="Elastic-Stack-是什么"><a href="#Elastic-Stack-是什么" class="headerlink" title="Elastic Stack 是什么"></a>Elastic Stack 是什么</h3><p><strong>Elastic Stack</strong> 即 <strong>ELK Stack</strong>。</p>
<p>ELK 是指 Elastic 公司旗下三款产品 <a target="_blank" rel="noopener" href="https://www.elastic.co/cn/products/elasticsearch">ElasticSearch</a> 、<a target="_blank" rel="noopener" href="https://www.elastic.co/cn/products/logstash">Logstash</a> 、<a target="_blank" rel="noopener" href="https://www.elastic.co/cn/products/kibana">Kibana</a> 的首字母组合。</p>
<ul>
<li>Elasticsearch 是一个搜索和分析引擎。</li>
<li>Logstash 是服务器端数据处理管道，能够同时从多个来源采集数据，转换数据，然后将数据发送到诸如 Elasticsearch 等“存储库”中。</li>
<li>Kibana 则可以让用户在 Elasticsearch 中使用图形和图表对数据进行可视化。</li>
</ul>
<p>而 Elastic Stack 是 ELK Stack 的更新换代产品，最新产品引入了轻量型的单一功能数据采集器，并把它们叫做 <a target="_blank" rel="noopener" href="https://www.elastic.co/cn/products/beats">Beats</a>。</p>
<h3 id="为什么使用-Elastic-Stack"><a href="#为什么使用-Elastic-Stack" class="headerlink" title="为什么使用 Elastic Stack"></a>为什么使用 Elastic Stack</h3><p>对于有一定规模的公司来说，通常会很多个应用，并部署在大量的服务器上。运维和开发人员常常需要通过查看日志来定位问题。如果应用是集群化部署，试想如果登录一台台服务器去查看日志，是多么费时费力。</p>
<p>而通过 ELK 这套解决方案，可以同时实现日志收集、日志搜索和日志分析的功能。</p>
<h3 id="Elastic-Stack-架构"><a href="#Elastic-Stack-架构" class="headerlink" title="Elastic Stack 架构"></a>Elastic Stack 架构</h3><p><img src="https://www.elastic.co/guide/en/logstash/current/static/images/deploy3.png" alt="img"></p>
<blockquote>
<p><strong>说明</strong></p>
<p>以上是 Elastic Stack 的一个架构图。从图中可以清楚的看到数据流向。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.elastic.co/products/beats">Beats</a> 是单一用途的数据传输平台，它可以将多台机器的数据发送到 Logstash 或 ElasticSearch。但 Beats 并不是不可或缺的一环，所以本文中暂不介绍。</li>
<li><a target="_blank" rel="noopener" href="https://www.elastic.co/products/logstash">Logstash</a> 是一个动态数据收集管道。支持以 TCP&#x2F;UDP&#x2F;HTTP 多种方式收集数据（也可以接受 Beats 传输来的数据），并对数据做进一步丰富或提取字段处理。</li>
<li><a target="_blank" rel="noopener" href="https://www.elastic.co/elasticsearch">ElasticSearch</a> 是一个基于 JSON 的分布式的搜索和分析引擎。作为 ELK 的核心，它集中存储数据。</li>
<li><a target="_blank" rel="noopener" href="https://www.elastic.co/kibana">Kibana</a> 是 ELK 的用户界面。它将收集的数据进行可视化展示（各种报表、图形化数据），并提供配置、管理 ELK 的界面。</li>
</ul>
</blockquote>
<h2 id="ElasticSearch"><a href="#ElasticSearch" class="headerlink" title="ElasticSearch"></a>ElasticSearch</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/elastic/elasticsearch">Elasticsearch</a> 是一个分布式、RESTful 风格的搜索和数据分析引擎，能够解决不断涌现出的各种用例。 作为 Elastic Stack 的核心，它集中存储您的数据，帮助您发现意料之中以及意料之外的情况。</p>
</blockquote>
<h3 id="ElasticSearch-简介"><a href="#ElasticSearch-简介" class="headerlink" title="ElasticSearch 简介"></a>ElasticSearch 简介</h3><p><a target="_blank" rel="noopener" href="https://github.com/elastic/elasticsearch">Elasticsearch</a> 基于搜索库 <a target="_blank" rel="noopener" href="https://github.com/apache/lucene-solr">Lucene</a> 开发。ElasticSearch 隐藏了 Lucene 的复杂性，提供了简单易用的 REST API &#x2F; Java API 接口（另外还有其他语言的 API 接口）。</p>
<p>ElasticSearch 可以视为一个文档存储，它<strong>将复杂数据结构序列化为 JSON 存储</strong>。</p>
<p><strong>ElasticSearch 是近乎于实时的全文搜素</strong>，这是指：</p>
<ul>
<li>从写入数据到数据可以被搜索，存在较小的延迟（大概是 1s）</li>
<li>基于 ES 执行搜索和分析可以达到秒级</li>
</ul>
<h4 id="2-1-1-核心概念"><a href="#2-1-1-核心概念" class="headerlink" title="2.1.1. 核心概念"></a>2.1.1. 核心概念</h4><ul>
<li><strong><code>索引（Index）</code></strong> 可以认为是文档（document）的优化集合。</li>
<li>每个 <strong><code>文档（document）</code></strong> 都是字段（field）的集合。</li>
<li><strong><code>字段（field）</code></strong> 是包含数据的键值对。</li>
<li>默认情况下，Elasticsearch 对每个字段中的所有数据建立索引，并且每个索引字段都具有专用的优化数据结构。</li>
<li>每个索引里可以有一个或者多个类型（type）。<code>类型（type）</code> 是 index 的一个逻辑分类，</li>
<li>当单台机器不足以存储大量数据时，Elasticsearch 可以将一个索引中的数据切分为多个 <strong><code>分片（shard）</code></strong> 。 <strong><code>分片（shard）</code></strong> 分布在多台服务器上存储。有了 shard 就可以横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升吞吐量和性能。每个 shard 都是一个 lucene index。</li>
<li>任何一个服务器随时可能故障或宕机，此时 shard 可能就会丢失，因此可以为每个 shard 创建多个 **<code>副本（replica）</code>**。replica 可以在 shard 故障时提供备用服务，保证数据不丢失，多个 replica 还可以提升搜索操作的吞吐量和性能。primary shard（建立索引时一次设置，不能修改，默认 5 个），replica shard（随时修改数量，默认 1 个），默认每个索引 10 个 shard，5 个 primary shard，5 个 replica shard，最小的高可用配置，是 2 台服务器。</li>
</ul>
<h3 id="ElasticSearch-原理"><a href="#ElasticSearch-原理" class="headerlink" title="ElasticSearch 原理"></a>ElasticSearch 原理</h3><h4 id="2-2-1-ES-写数据过程"><a href="#2-2-1-ES-写数据过程" class="headerlink" title="2.2.1. ES 写数据过程"></a>2.2.1. ES 写数据过程</h4><ul>
<li>客户端选择一个 node 发送请求过去，这个 node 就是 <code>coordinating node</code>（协调节点）。</li>
<li><code>coordinating node</code> 对 document 进行<strong>路由</strong>，将请求转发给对应的 node（有 primary shard）。</li>
<li>实际的 node 上的 <code>primary shard</code> 处理请求，然后将数据同步到 <code>replica node</code>。</li>
<li><code>coordinating node</code> 如果发现 <code>primary node</code> 和所有 <code>replica node</code> 都搞定之后，就返回响应结果给客户端。</li>
</ul>
<p><img src="https://github.com/doocs/advanced-java/raw/master/images/es-write.png" alt="es-write"></p>
<h4 id="2-2-2-es-读数据过程"><a href="#2-2-2-es-读数据过程" class="headerlink" title="2.2.2. es 读数据过程"></a>2.2.2. es 读数据过程</h4><p>可以通过 <code>doc id</code> 来查询，会根据 <code>doc id</code> 进行 hash，判断出来当时把 <code>doc id</code> 分配到了哪个 shard 上面去，从那个 shard 去查询。</p>
<ul>
<li>客户端发送请求到<strong>任意</strong>一个 node，成为 <code>coordinate node</code>。</li>
<li><code>coordinate node</code> 对 <code>doc id</code> 进行哈希路由，将请求转发到对应的 node，此时会使用 <code>round-robin</code> <strong>随机轮询算法</strong>，在 <code>primary shard</code> 以及其所有 replica 中随机选择一个，让读请求负载均衡。</li>
<li>接收请求的 node 返回 document 给 <code>coordinate node</code>。</li>
<li><code>coordinate node</code> 返回 document 给客户端。</li>
</ul>
<h4 id="2-2-3-写数据底层原理"><a href="#2-2-3-写数据底层原理" class="headerlink" title="2.2.3. 写数据底层原理"></a>2.2.3. 写数据底层原理</h4><p><img src="https://github.com/doocs/advanced-java/raw/master/images/es-write-detail.png" alt="es-write-detail"></p>
<p>先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；同时将数据写入 translog 日志文件。</p>
<p>如果 buffer 快满了，或者到一定时间，就会将内存 buffer 数据 <code>refresh</code> 到一个新的 <code>segment file</code> 中，但是此时数据不是直接进入 <code>segment file</code> 磁盘文件，而是先进入 <code>os cache</code> 。这个过程就是 <code>refresh</code>。</p>
<p>每隔 1 秒钟，es 将 buffer 中的数据写入一个<strong>新的</strong> <code>segment file</code>，每秒钟会产生一个<strong>新的磁盘文件</strong> <code>segment file</code>，这个 <code>segment file</code> 中就存储最近 1 秒内 buffer 中写入的数据。</p>
<p>但是如果 buffer 里面此时没有数据，那当然不会执行 refresh 操作，如果 buffer 里面有数据，默认 1 秒钟执行一次 refresh 操作，刷入一个新的 segment file 中。</p>
<p>操作系统里面，磁盘文件其实都有一个东西，叫做 <code>os cache</code>，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入 <code>os cache</code>，先进入操作系统级别的一个内存缓存中去。只要 <code>buffer</code> 中的数据被 refresh 操作刷入 <code>os cache</code>中，这个数据就可以被搜索到了。</p>
<p>为什么叫 es 是<strong>准实时</strong>的？ <code>NRT</code>，全称 <code>near real-time</code>。默认是每隔 1 秒 refresh 一次的，所以 es 是准实时的，因为写入的数据 1 秒之后才能被看到。可以通过 es 的 <code>restful api</code> 或者 <code>java api</code>，<strong>手动</strong>执行一次 refresh 操作，就是手动将 buffer 中的数据刷入 <code>os cache</code>中，让数据立马就可以被搜索到。只要数据被输入 <code>os cache</code> 中，buffer 就会被清空了，因为不需要保留 buffer 了，数据在 translog 里面已经持久化到磁盘去一份了。</p>
<p>重复上面的步骤，新的数据不断进入 buffer 和 translog，不断将 <code>buffer</code> 数据写入一个又一个新的 <code>segment file</code> 中去，每次 <code>refresh</code> 完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发 <code>commit</code> 操作。</p>
<p>commit 操作发生第一步，就是将 buffer 中现有数据 <code>refresh</code> 到 <code>os cache</code> 中去，清空 buffer。然后，将一个 <code>commit point</code> 写入磁盘文件，里面标识着这个 <code>commit point</code> 对应的所有 <code>segment file</code>，同时强行将 <code>os cache</code> 中目前所有的数据都 <code>fsync</code> 到磁盘文件中去。最后<strong>清空</strong> 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。</p>
<p>这个 commit 操作叫做 <code>flush</code>。默认 30 分钟自动执行一次 <code>flush</code>，但如果 translog 过大，也会触发 <code>flush</code>。flush 操作就对应着 commit 的全过程，我们可以通过 es api，手动执行 flush 操作，手动将 os cache 中的数据 fsync 强刷到磁盘上去。</p>
<p>translog 日志文件的作用是什么？你执行 commit 操作之前，数据要么是停留在 buffer 中，要么是停留在 os cache 中，无论是 buffer 还是 os cache 都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件 <code>translog</code> 中，一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。</p>
<p>translog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以默认情况下，可能有 5 秒的数据会仅仅停留在 buffer 或者 translog 文件的 os cache 中，如果此时机器挂了，会<strong>丢失</strong> 5 秒钟的数据。但是这样性能比较好，最多丢 5 秒的数据。也可以将 translog 设置成每次写操作必须是直接 <code>fsync</code> 到磁盘，但是性能会差很多。</p>
<p>实际上你在这里，如果面试官没有问你 es 丢数据的问题，你可以在这里给面试官炫一把，你说，其实 es 第一是准实时的，数据写入 1 秒后可以搜索到；可能会丢失数据的。有 5 秒的数据，停留在 buffer、translog os cache、segment file os cache 中，而不在磁盘上，此时如果宕机，会导致 5 秒的<strong>数据丢失</strong>。</p>
<p><strong>总结一下</strong>，数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）。每隔 5s，将数据写入 translog 文件（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。</p>
<blockquote>
<p>数据写入 segment file 之后，同时就建立好了倒排索引。</p>
</blockquote>
<h4 id="2-2-4-删除-更新数据底层原理"><a href="#2-2-4-删除-更新数据底层原理" class="headerlink" title="2.2.4. 删除&#x2F;更新数据底层原理"></a>2.2.4. 删除&#x2F;更新数据底层原理</h4><p>如果是删除操作，commit 的时候会生成一个 <code>.del</code> 文件，里面将某个 doc 标识为 <code>deleted</code> 状态，那么搜索的时候根据 <code>.del</code> 文件就知道这个 doc 是否被删除了。</p>
<p>如果是更新操作，就是将原来的 doc 标识为 <code>deleted</code> 状态，然后新写入一条数据。</p>
<p>buffer 每 refresh 一次，就会产生一个 <code>segment file</code>，所以默认情况下是 1 秒钟一个 <code>segment file</code>，这样下来 <code>segment file</code> 会越来越多，此时会定期执行 merge。每次 merge 的时候，会将多个 <code>segment file</code> 合并成一个，同时这里会将标识为 <code>deleted</code> 的 doc 给<strong>物理删除掉</strong>，然后将新的 <code>segment file</code> 写入磁盘，这里会写一个 <code>commit point</code>，标识所有新的 <code>segment file</code>，然后打开 <code>segment file</code> 供搜索使用，同时删除旧的 <code>segment file</code>。</p>
<h4 id="2-2-5-底层-lucene"><a href="#2-2-5-底层-lucene" class="headerlink" title="2.2.5. 底层 lucene"></a>2.2.5. 底层 lucene</h4><p>简单来说，lucene 就是一个 jar 包，里面包含了封装好的各种建立倒排索引的算法代码。我们用 Java 开发的时候，引入 lucene jar，然后基于 lucene 的 api 去开发就可以了。</p>
<p>通过 lucene，我们可以将已有的数据建立索引，lucene 会在本地磁盘上面，给我们组织索引的数据结构。</p>
<h4 id="2-2-6-倒排索引"><a href="#2-2-6-倒排索引" class="headerlink" title="2.2.6. 倒排索引"></a>2.2.6. 倒排索引</h4><p>在搜索引擎中，每个文档都有一个对应的文档 ID，文档内容被表示为一系列关键词的集合。例如，文档 1 经过分词，提取了 20 个关键词，每个关键词都会记录它在文档中出现的次数和出现位置。</p>
<p>那么，倒排索引就是<strong>关键词到文档</strong> ID 的映射，每个关键词都对应着一系列的文件，这些文件中都出现了关键词。</p>
<p>举个栗子。</p>
<p>有以下文档：</p>
<table>
<thead>
<tr>
<th>DocId</th>
<th>Doc</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>谷歌地图之父跳槽 Facebook</td>
</tr>
<tr>
<td>2</td>
<td>谷歌地图之父加盟 Facebook</td>
</tr>
<tr>
<td>3</td>
<td>谷歌地图创始人拉斯离开谷歌加盟 Facebook</td>
</tr>
<tr>
<td>4</td>
<td>谷歌地图之父跳槽 Facebook 与 Wave 项目取消有关</td>
</tr>
<tr>
<td>5</td>
<td>谷歌地图之父拉斯加盟社交网站 Facebook</td>
</tr>
</tbody></table>
<p>对文档进行分词之后，得到以下<strong>倒排索引</strong>。</p>
<table>
<thead>
<tr>
<th>WordId</th>
<th>Word</th>
<th>DocIds</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>谷歌</td>
<td>1,2,3,4,5</td>
</tr>
<tr>
<td>2</td>
<td>地图</td>
<td>1,2,3,4,5</td>
</tr>
<tr>
<td>3</td>
<td>之父</td>
<td>1,2,4,5</td>
</tr>
<tr>
<td>4</td>
<td>跳槽</td>
<td>1,4</td>
</tr>
<tr>
<td>5</td>
<td>Facebook</td>
<td>1,2,3,4,5</td>
</tr>
<tr>
<td>6</td>
<td>加盟</td>
<td>2,3,5</td>
</tr>
<tr>
<td>7</td>
<td>创始人</td>
<td>3</td>
</tr>
<tr>
<td>8</td>
<td>拉斯</td>
<td>3,5</td>
</tr>
<tr>
<td>9</td>
<td>离开</td>
<td>3</td>
</tr>
<tr>
<td>10</td>
<td>与</td>
<td>4</td>
</tr>
<tr>
<td>..</td>
<td>..</td>
<td>..</td>
</tr>
</tbody></table>
<p>另外，实用的倒排索引还可以记录更多的信息，比如文档频率信息，表示在文档集合中有多少个文档包含某个单词。</p>
<p>那么，有了倒排索引，搜索引擎可以很方便地响应用户的查询。比如用户输入查询 <code>Facebook</code>，搜索系统查找倒排索引，从中读出包含这个单词的文档，这些文档就是提供给用户的搜索结果。</p>
<p>要注意倒排索引的两个重要细节：</p>
<ul>
<li>倒排索引中的所有词项对应一个或多个文档；</li>
<li>倒排索引中的词项<strong>根据字典顺序升序排列</strong></li>
</ul>
<blockquote>
<p>上面只是一个简单的栗子，并没有严格按照字典顺序升序排列。</p>
</blockquote>
<h2 id="Logstash"><a href="#Logstash" class="headerlink" title="Logstash"></a>Logstash</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/elastic/logstash">Logstash</a> 是开源的服务器端数据处理管道，能够同时从多个来源采集数据，转换数据，然后将数据发送到您最喜欢的“存储库”中。</p>
</blockquote>
<h3 id="Logstash-简介"><a href="#Logstash-简介" class="headerlink" title="Logstash 简介"></a>Logstash 简介</h3><p>Logstash 可以传输和处理你的日志、事务或其他数据。</p>
<p>Logstash 是 Elasticsearch 的最佳数据管道。</p>
<p>Logstash 是插件式管理模式，在输入、过滤、输出以及编码过程中都可以使用插件进行定制。Logstash 社区有超过 200 种可用插件。</p>
<h3 id="Logstash-原理"><a href="#Logstash-原理" class="headerlink" title="Logstash 原理"></a>Logstash 原理</h3><p>Logstash 有两个必要元素：<code>input</code> 和 <code>output</code> ，一个可选元素：<code>filter</code>。</p>
<p>这三个元素，分别代表 Logstash 事件处理的三个阶段：输入 &gt; 过滤器 &gt; 输出。</p>
<p><img src="https://www.elastic.co/guide/en/logstash/current/static/images/basic_logstash_pipeline.png" alt="img"></p>
<ul>
<li><strong>input</strong> - 负责从数据源采集数据。</li>
<li><strong><code>filter</code></strong> - 将数据修改为你指定的格式或内容。</li>
<li><strong><code>output</code></strong> - 将数据传输到目的地。</li>
</ul>
<p>在实际应用场景中，通常输入、输出、过滤器不止一个。Logstash 的这三个元素都使用插件式管理方式，用户可以根据应用需要，灵活的选用各阶段需要的插件，并组合使用。</p>
<h2 id="Beats"><a href="#Beats" class="headerlink" title="Beats"></a>Beats</h2><blockquote>
<p><strong><a target="_blank" rel="noopener" href="https://github.com/elastic/beats">Beats</a> 是安装在服务器上的数据中转代理</strong>。</p>
<p>Beats 可以将数据直接传输到 Elasticsearch 或传输到 Logstash 。</p>
</blockquote>
<p><img src="https://www.elastic.co/guide/en/beats/libbeat/current/images/beats-platform.png" alt="img"></p>
<p>Beats 有多种类型，可以根据实际应用需要选择合适的类型。</p>
<p>常用的类型有：</p>
<ul>
<li><strong>Packetbeat：</strong>网络数据包分析器，提供有关您的应用程序服务器之间交换的事务的信息。</li>
<li><strong>Filebeat：</strong>从您的服务器发送日志文件。</li>
<li><strong>Metricbeat：</strong>是一个服务器监视代理程序，它定期从服务器上运行的操作系统和服务收集指标。</li>
<li><strong>Winlogbeat：</strong>提供 Windows 事件日志。</li>
</ul>
<h3 id="Filebeat-简介"><a href="#Filebeat-简介" class="headerlink" title="Filebeat 简介"></a>Filebeat 简介</h3><blockquote>
<p>_由于本人仅接触过 Filebeat，所以本文只介绍 Beats 组件中的 Filebeat_。</p>
</blockquote>
<p>相比 Logstash，FileBeat 更加轻量化。</p>
<p>在任何环境下，应用程序都有停机的可能性。 Filebeat 读取并转发日志行，如果中断，则会记住所有事件恢复联机状态时所在位置。</p>
<p>Filebeat 带有内部模块（auditd，Apache，Nginx，System 和 MySQL），可通过一个指定命令来简化通用日志格式的收集，解析和可视化。</p>
<p>FileBeat 不会让你的管道超负荷。FileBeat 如果是向 Logstash 传输数据，当 Logstash 忙于处理数据，会通知 FileBeat 放慢读取速度。一旦拥塞得到解决，FileBeat 将恢复到原来的速度并继续传播。</p>
<p><img src="https://www.elastic.co/guide/en/beats/filebeat/current/images/filebeat.png" alt="img"></p>
<h3 id="Filebeat-原理"><a href="#Filebeat-原理" class="headerlink" title="Filebeat 原理"></a>Filebeat 原理</h3><p>Filebeat 有两个主要组件：</p>
<ul>
<li><code>harvester</code>：负责读取一个文件的内容。它会逐行读取文件内容，并将内容发送到输出目的地。</li>
<li><code>prospector</code>：负责管理 harvester 并找到所有需要读取的文件源。比如类型是日志，prospector 就会遍历制定路径下的所有匹配要求的文件。</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">filebeat.prospectors:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">log</span></span><br><span class="line">    <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/var/log/*.log</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/var/path2/*.log</span></span><br></pre></td></tr></table></figure>

<p>Filebeat 保持每个文件的状态，并经常刷新注册表文件中的磁盘状态。状态用于记住 harvester 正在读取的最后偏移量，并确保发送所有日志行。</p>
<p>Filebeat 将每个事件的传递状态存储在注册表文件中。所以它能保证事件至少传递一次到配置的输出，没有数据丢失。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><strong>官方资源</strong><ul>
<li><a target="_blank" rel="noopener" href="https://www.elastic.co/cn/products/elasticsearch">Elasticsearch 官网</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/elastic/elasticsearch">Elasticsearch Github</a></li>
<li><a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html">Elasticsearch 官方文档</a></li>
<li><a target="_blank" rel="noopener" href="https://www.elastic.co/cn/products/logstash">Logstash 官网</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/elastic/logstash">Logstash Github</a></li>
<li><a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/logstash/current/index.html">Logstash 官方文档</a></li>
<li><a target="_blank" rel="noopener" href="https://www.elastic.co/cn/products/kibana">Kibana 官网</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/elastic/kibana">Kibana Github</a></li>
<li><a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/kibana/current/index.html">Kibana 官方文档</a></li>
<li><a target="_blank" rel="noopener" href="https://www.elastic.co/cn/products/beats">Beats 官网</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/elastic/beats">Beats Github</a></li>
<li><a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/beats/libbeat/current/index.html">Beats 官方文档</a></li>
</ul>
</li>
<li><strong>文章</strong><ul>
<li><a target="_blank" rel="noopener" href="https://www.elastic.co/cn/what-is/elk-stack">什么是 ELK Stack？</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/es-introduction.md">https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/es-introduction.md</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/es-write-query-search.md">es-write-query-search</a></li>
</ul>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/blog/page/29/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/blog/">1</a><span class="space">&hellip;</span><a class="page-number" href="/blog/page/29/">29</a><span class="page-number current">30</span><a class="page-number" href="/blog/page/31/">31</a><span class="space">&hellip;</span><a class="page-number" href="/blog/page/51/">51</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/blog/page/31/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2015 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">钝悟 ◾ Dunwu</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">4.5m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">68:08</span>
  </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/dunwu/blog" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '64px',
  right: 'unset',
  left: '32px',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.8.0/dist/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"dunwu","repo":"blog","client_id":"c45bc13ca1d3d3aa4836","client_secret":"1907a9f0c22087badad3938e1d7dcba9078f88ac","admin_user":"dunwu","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdn.jsdelivr.net/npm/gitalk@1.8.0/dist/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"f2d1304cc5f4fffd1ce253f759e45841"}</script>
<script src="/blog/js/third-party/comments/gitalk.js" defer></script>

</body>
</html>
