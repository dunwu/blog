---
icon: devicon:hadoop
title: YARN
date: 2019-05-07 20:19:25
categories:
  - 大数据
  - hadoop
tags:
  - 大数据
  - hadoop
  - yarn
permalink: /pages/11a2c936/
---

# YARN

## YARN 简介

**Apache YARN** (Yet Another Resource Negotiator) 是 hadoop 2.0 引入的集群资源管理系统。用户可以将各种服务框架部署在 YARN 上，由 YARN 进行统一地管理和资源分配。

![](https://raw.githubusercontent.com/dunwu/images/master/snap/202502192251433.png)

## YARN 架构

![](https://raw.githubusercontent.com/dunwu/images/master/snap/202502192252145.png)

### ResourceManager

`ResourceManager` 通常在独立的机器上以后台进程的形式运行，它是整个集群资源的主要协调者和管理者。`ResourceManager` 负责给用户提交的所有应用程序分配资源，它根据应用程序优先级、队列容量、ACLs、数据位置等信息，做出决策，然后以共享的、安全的、多租户的方式制定分配策略，调度集群资源。

### NodeManager

`NodeManager` 是 YARN 集群中的每个具体节点的管理者。主要负责该节点内所有容器的生命周期的管理，监视资源和跟踪节点健康。具体如下：

- 启动时向 `ResourceManager` 注册并定时发送心跳消息，等待 `ResourceManager` 的指令；
- 维护 `Container` 的生命周期，监控 `Container` 的资源使用情况；
- 管理任务运行时的相关依赖，根据 `ApplicationMaster` 的需要，在启动 `Container` 之前将需要的程序及其依赖拷贝到本地。

### ApplicationMaster

在用户提交一个应用程序时，YARN 会启动一个轻量级的进程 `ApplicationMaster`。`ApplicationMaster` 负责协调来自 `ResourceManager` 的资源，并通过 `NodeManager` 监视容器内资源的使用情况，同时还负责任务的监控与容错。具体如下：

- 根据应用的运行状态来决定动态计算资源需求；
- 向 `ResourceManager` 申请资源，监控申请的资源的使用情况；
- 跟踪任务状态和进度，报告资源的使用情况和应用的进度信息；
- 负责任务的容错。

### Container

`Container` 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。当 AM 向 RM 申请资源时，RM 为 AM 返回的资源是用 `Container` 表示的。YARN 会为每个任务分配一个 `Container`，该任务只能使用该 `Container` 中描述的资源。`ApplicationMaster` 可在 `Container` 内运行任何类型的任务。例如，`MapReduce ApplicationMaster` 请求一个容器来启动 map 或 reduce 任务，而 `Giraph ApplicationMaster` 请求一个容器来运行 Giraph 任务。

## YARN 工作原理

![](https://raw.githubusercontent.com/dunwu/images/master/snap/202502192253437.png)

1. `Client` 提交作业到 YARN 上；

2. `Resource Manager` 选择一个 `Node Manager`，启动一个 `Container` 并运行 `Application Master` 实例；

3. `Application Master` 根据实际需要向 `Resource Manager` 请求更多的 `Container` 资源（如果作业很小，应用管理器会选择在其自己的 JVM 中运行任务）；

4. `Application Master` 通过获取到的 `Container` 资源执行分布式计算。

![](https://raw.githubusercontent.com/dunwu/images/master/snap/202502192255544.png)

#### 作业提交

client 调用 job.waitForCompletion 方法，向整个集群提交 MapReduce 作业 （第 1 步） 。新的作业 ID（应用 ID) 由资源管理器分配 （第 2 步）。作业的 client 核实作业的输出，计算输入的 split, 将作业的资源 （包括 Jar 包，配置文件，split 信息） 拷贝给 HDFS（第 3 步）。 最后，通过调用资源管理器的 submitApplication() 来提交作业 （第 4 步）。

#### 作业初始化

当资源管理器收到 submitApplciation() 的请求时，就将该请求发给调度器 (scheduler), 调度器分配 container, 然后资源管理器在该 container 内启动应用管理器进程，由节点管理器监控 （第 5 步）。

MapReduce 作业的应用管理器是一个主类为 MRAppMaster 的 Java 应用，其通过创造一些 bookkeeping 对象来监控作业的进度，得到任务的进度和完成报告 （第 6 步）。然后其通过分布式文件系统得到由客户端计算好的输入 split（第 7 步），然后为每个输入 split 创建一个 map 任务，根据 mapreduce.job.reduces 创建 reduce 任务对象。

#### 任务分配

如果作业很小，应用管理器会选择在其自己的 JVM 中运行任务。

如果不是小作业，那么应用管理器向资源管理器请求 container 来运行所有的 map 和 reduce 任务 （第 8 步）。这些请求是通过心跳来传输的，包括每个 map 任务的数据位置，比如存放输入 split 的主机名和机架 (rack)，调度器利用这些信息来调度任务，尽量将任务分配给存储数据的节点，或者分配给和存放输入 split 的节点相同机架的节点。

#### 任务运行

当一个任务由资源管理器的调度器分配给一个 container 后，应用管理器通过联系节点管理器来启动 container（第 9 步）。任务由一个主类为 YarnChild 的 Java 应用执行， 在运行任务之前首先本地化任务需要的资源，比如作业配置，JAR 文件，以及分布式缓存的所有文件 （第 10 步。 最后，运行 map 或 reduce 任务 （第 11 步）。

YarnChild 运行在一个专用的 JVM 中，但是 YARN 不支持 JVM 重用。

#### 进度和状态更新

YARN 中的任务将其进度和状态 （包括 counter) 返回给应用管理器，客户端每秒 （通 mapreduce.client.progressmonitor.pollinterval 设置） 向应用管理器请求进度更新，展示给用户。

#### 作业完成

除了向应用管理器请求作业进度外，客户端每 5 分钟都会通过调用 waitForCompletion() 来检查作业是否完成，时间间隔可以通过 mapreduce.client.completion.pollinterval 来设置。作业完成之后，应用管理器和 container 会清理工作状态， OutputCommiter 的作业清理方法也会被调用。作业的信息会被作业历史服务器存储以备之后用户核查。

## 提交作业到 YARN 上运行

这里以提交 Hadoop Examples 中计算 Pi 的 MApReduce 程序为例，相关 Jar 包在 Hadoop 安装目录的 `share/hadoop/mapreduce` 目录下：

```shell
# 提交格式：hadoop jar jar 包路径 主类名称 主类参数
# hadoop jar hadoop-mapreduce-examples-2.6.0-cdh5.15.2.jar pi 3 3
```

## 参考资料

- [初步掌握 Yarn 的架构及原理](https://www.cnblogs.com/codeOfLife/p/5492740.html)
- [Apache Hadoop 2.9.2 > Apache Hadoop YARN](http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html)
- [深入浅出 Hadoop YARN](https://zhuanlan.zhihu.com/p/54192454)